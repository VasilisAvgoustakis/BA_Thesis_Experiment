{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cap to 1000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"train\", \"test\", \"valid\"]\n",
    "for name in data:\n",
    "    with open(\"data/hd/raw/\" + name + \".wp_target\") as f:\n",
    "        stories = f.readlines()\n",
    "    stories = [\" \".join(i.split()[0:1000]) for i in stories]\n",
    "    with open(\"data/hd/prepro/\" + name + \".wp_target\", \"w\") as o:\n",
    "        for line in stories:\n",
    "            o.write(line.strip() + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine each prompt with its corresponding story to a new txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define regex pattern for impurities\n",
    "pattern = r\"(<newline>)\"\n",
    "\n",
    "names = [\"train\", \"test\", \"valid\"]\n",
    "\n",
    "for name in names:\n",
    "    # Python script to concatenate prompts and stories\n",
    "    with open('data/hd/prepro/' + name + '.wp_source', 'r', encoding='utf-8') as sources, \\\n",
    "         open('data/hd/prepro/' + name + '.wp_target', 'r', encoding='utf-8') as targets, \\\n",
    "         open('data/hd/prepro/combined0/' + name + '_combined.txt', 'w', encoding='utf-8') as outfile:\n",
    "        for prompt, story in zip(sources, targets):\n",
    "            cleaned_prompt = re.sub(r\"\\<[^\\>]*\\>|\\[ WP \\]|\\-\\-\", \"\", prompt[6:])\n",
    "            cleaned_story = re.sub(pattern, \"\", story)\n",
    "            outfile.write(cleaned_prompt.strip() + \"\\n\" + cleaned_story.strip() + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve this structure for your model training across different generations, you'll need to partition your human-generated data (from train_combined.txt) into decreasing portions for each subsequent generation, ensuring that each generation sees a unique subset of the human data for the first time. Hereâ€™s how you can split the data and prepare the training files for each generation:\n",
    "Step 1: Calculate Split Sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries per file: [109040, 81780, 61335, 46001, 34500, 25875, 19406, 14555, 10916]\n"
     ]
    }
   ],
   "source": [
    "def count_entries(filepath):\n",
    "    \"\"\"Counts the number of double-newline-separated entries in a file.\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        content = file.read().strip()\n",
    "    return len(content.split('\\n\\n'))\n",
    "\n",
    "def calculate_portions(total_entries, initial_portion=0.40):\n",
    "    \"\"\"Calculate the number of entries for each dataset based on reducing portions.\"\"\"\n",
    "    portions = [initial_portion]\n",
    "    current_portion = initial_portion\n",
    "\n",
    "    # Assuming a reduction of 25% relatively per generation\n",
    "    while current_portion > 0.05:  # Continue until the portion is very small\n",
    "        current_portion *= 0.75\n",
    "        portions.append(current_portion)\n",
    "\n",
    "    # Calculate the number of entries per portion\n",
    "    entries_per_portion = [int(total_entries * p) for p in portions]\n",
    "    return entries_per_portion\n",
    "\n",
    "# Example usage\n",
    "total_entries = count_entries('./data/hd/initial_combined/train_combined.txt')\n",
    "entries_distribution = calculate_portions(total_entries)\n",
    "print(\"Entries per file:\", entries_distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "First, calculate the size of each portion based on the total number of lines in train_combined.txt. You'll want to divide the data so that each subsequent file has 25% less human data than the previous one.\n",
    "Step 2: Split the Data\n",
    "\n",
    "You can use Python to handle the reading, splitting, and writing of the data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(filepath, portions):\n",
    "    import random\n",
    "    \n",
    "    # Read the entire file and split by entries\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        content = file.read().strip().split('\\n\\n')\n",
    "    \n",
    "    random.shuffle(content)  # Shuffle to randomize the entries distribution\n",
    "\n",
    "    # Calculate the starting index for each portion\n",
    "    total_len = len(content)\n",
    "    portions_indices = [0]\n",
    "    cumulative_sum = 0\n",
    "\n",
    "    for portion in portions:\n",
    "        cumulative_sum += int(total_len * portion)\n",
    "        portions_indices.append(cumulative_sum)\n",
    "\n",
    "    # Write out each portion to a different file\n",
    "    for i in range(len(portions_indices) - 1):\n",
    "        with open(f'data/hd/combined{i}/train_combined{i}.txt', 'w', encoding='utf-8') as f:\n",
    "            for entry in content[portions_indices[i]:portions_indices[i+1]]:\n",
    "                f.write(entry + \"\\n\\n\")\n",
    "\n",
    "# Example usage\n",
    "portions = [0.40, 0.30, 0.20, 0.10]  # Adjust based on total data and requirements\n",
    "split_data('./data/hd/initial_combined/train_combined.txt', portions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109040\n",
      "81780\n",
      "54520\n",
      "27260\n"
     ]
    }
   ],
   "source": [
    "print(count_entries('./data/hd/combined0/train_combined0.txt'))\n",
    "print(count_entries('./data/hd/combined1/train_combined1.txt'))\n",
    "print(count_entries('./data/hd/combined2/train_combined2.txt'))\n",
    "print(count_entries('./data/hd/combined3/train_combined3.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that there is no data leakage between the different train_combined#.txt files you've created, you'll want to verify that each dataset is unique and that entries from one dataset do not appear in another. Here are a few strategies you could employ to check for data leakage:\n",
    "1. Hash Checking\n",
    "\n",
    "You can calculate a unique hash for each entry in every dataset and ensure that no hash appears in more than one dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leakage detected! Entry in ./data/hd/combined0/train_combined0.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined0/train_combined0.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined0/train_combined0.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined0/train_combined0.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined0/train_combined0.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined0/train_combined0.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined0/train_combined0.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined0/train_combined0.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined0/train_combined0.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined0/train_combined0.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined0/train_combined0.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined0/train_combined0.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined0/train_combined0.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined0/train_combined0.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined0/train_combined0.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined0/train_combined0.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined1/train_combined1.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined1/train_combined1.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined1/train_combined1.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined1/train_combined1.txt already appears in ./data/hd/combined1/train_combined1.txt\n",
      "Leakage detected! Entry in ./data/hd/combined1/train_combined1.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined1/train_combined1.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined1/train_combined1.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined1/train_combined1.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined1/train_combined1.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined1/train_combined1.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined1/train_combined1.txt already appears in ./data/hd/combined1/train_combined1.txt\n",
      "Leakage detected! Entry in ./data/hd/combined1/train_combined1.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined1/train_combined1.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined1/train_combined1.txt already appears in ./data/hd/combined1/train_combined1.txt\n",
      "Leakage detected! Entry in ./data/hd/combined1/train_combined1.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined1/train_combined1.txt already appears in ./data/hd/combined1/train_combined1.txt\n",
      "Leakage detected! Entry in ./data/hd/combined1/train_combined1.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined1/train_combined1.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined1/train_combined1.txt already appears in ./data/hd/combined1/train_combined1.txt\n",
      "Leakage detected! Entry in ./data/hd/combined1/train_combined1.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined1/train_combined1.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined1/train_combined1.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined1/train_combined1.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined1/train_combined1.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined1/train_combined1.txt already appears in ./data/hd/combined1/train_combined1.txt\n",
      "Leakage detected! Entry in ./data/hd/combined1/train_combined1.txt already appears in ./data/hd/combined1/train_combined1.txt\n",
      "Leakage detected! Entry in ./data/hd/combined1/train_combined1.txt already appears in ./data/hd/combined1/train_combined1.txt\n",
      "Leakage detected! Entry in ./data/hd/combined1/train_combined1.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined1/train_combined1.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined1/train_combined1.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined1/train_combined1.txt already appears in ./data/hd/combined1/train_combined1.txt\n",
      "Leakage detected! Entry in ./data/hd/combined1/train_combined1.txt already appears in ./data/hd/combined1/train_combined1.txt\n",
      "Leakage detected! Entry in ./data/hd/combined1/train_combined1.txt already appears in ./data/hd/combined1/train_combined1.txt\n",
      "Leakage detected! Entry in ./data/hd/combined1/train_combined1.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined1/train_combined1.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined1/train_combined1.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined1/train_combined1.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined1/train_combined1.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined1/train_combined1.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined1/train_combined1.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined1/train_combined1.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined1/train_combined1.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined2/train_combined2.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined2/train_combined2.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined1/train_combined1.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined1/train_combined1.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined1/train_combined1.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined2/train_combined2.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined1/train_combined1.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined2/train_combined2.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined1/train_combined1.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined1/train_combined1.txt\n",
      "Leakage detected! Entry in ./data/hd/combined2/train_combined2.txt already appears in ./data/hd/combined2/train_combined2.txt\n",
      "Leakage detected! Entry in ./data/hd/combined3/train_combined3.txt already appears in ./data/hd/combined2/train_combined2.txt\n",
      "Leakage detected! Entry in ./data/hd/combined3/train_combined3.txt already appears in ./data/hd/combined2/train_combined2.txt\n",
      "Leakage detected! Entry in ./data/hd/combined3/train_combined3.txt already appears in ./data/hd/combined2/train_combined2.txt\n",
      "Leakage detected! Entry in ./data/hd/combined3/train_combined3.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined3/train_combined3.txt already appears in ./data/hd/combined2/train_combined2.txt\n",
      "Leakage detected! Entry in ./data/hd/combined3/train_combined3.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined3/train_combined3.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined3/train_combined3.txt already appears in ./data/hd/combined1/train_combined1.txt\n",
      "Leakage detected! Entry in ./data/hd/combined3/train_combined3.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined3/train_combined3.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined3/train_combined3.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined3/train_combined3.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined3/train_combined3.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined3/train_combined3.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Leakage detected! Entry in ./data/hd/combined3/train_combined3.txt already appears in ./data/hd/combined0/train_combined0.txt\n",
      "Total Leakages: 111\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "def compute_hash(text):\n",
    "    \"\"\"Computes an MD5 hash for the given text.\"\"\"\n",
    "    return hashlib.md5(text.encode('utf-8')).hexdigest()\n",
    "\n",
    "def check_leakage(files):\n",
    "    seen_hashes = {}\n",
    "    leakage = False\n",
    "    leakage_count = 0\n",
    "\n",
    "    for filename in files:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            content = file.read().strip()\n",
    "        entries = content.split('\\n\\n')\n",
    "\n",
    "        for entry in entries:\n",
    "            entry_hash = compute_hash(entry)\n",
    "            if entry_hash in seen_hashes:\n",
    "                leakage_count += 1\n",
    "                print(f\"Leakage detected! Entry in {filename} already appears in {seen_hashes[entry_hash]}\")\n",
    "                #print(entry)\n",
    "                #break\n",
    "                leakage = True\n",
    "            else:\n",
    "                seen_hashes[entry_hash] = filename\n",
    "\n",
    "    if not leakage:\n",
    "        print(\"No data leakage detected among the files.\")\n",
    "    \n",
    "    print(\"Total Leakages:\",leakage_count)\n",
    "\n",
    "# List your files\n",
    "files = ['./data/hd/combined0/train_combined0.txt', './data/hd/combined1/train_combined1.txt', \n",
    "         './data/hd/combined2/train_combined2.txt', './data/hd/combined3/train_combined3.txt']\n",
    "check_leakage(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leakage detected between ./data/hd/combined0/train_combined0.txt and ./data/hd/combined1/train_combined1.txt!\n",
      "Leakage detected between ./data/hd/combined0/train_combined0.txt and ./data/hd/combined2/train_combined2.txt!\n",
      "Leakage detected between ./data/hd/combined0/train_combined0.txt and ./data/hd/combined3/train_combined3.txt!\n",
      "Leakage detected between ./data/hd/combined1/train_combined1.txt and ./data/hd/combined2/train_combined2.txt!\n",
      "Leakage detected between ./data/hd/combined1/train_combined1.txt and ./data/hd/combined3/train_combined3.txt!\n",
      "Leakage detected between ./data/hd/combined2/train_combined2.txt and ./data/hd/combined3/train_combined3.txt!\n"
     ]
    }
   ],
   "source": [
    "def read_entries(filename):\n",
    "    \"\"\"Reads entries from a file and returns them as a set.\"\"\"\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        content = file.read().strip()\n",
    "    return set(content.split('\\n\\n'))\n",
    "\n",
    "def check_intersection(files):\n",
    "    entries_sets = {filename: read_entries(filename) for filename in files}\n",
    "    all_files = list(entries_sets.keys())\n",
    "    for i in range(len(all_files)):\n",
    "        for j in range(i + 1, len(all_files)):\n",
    "            intersection = entries_sets[all_files[i]].intersection(entries_sets[all_files[j]])\n",
    "            if intersection:\n",
    "                print(f\"Leakage detected between {all_files[i]} and {all_files[j]}!\")\n",
    "            else:\n",
    "                print(f\"No leakage between {all_files[i]} and {all_files[j]}.\")\n",
    "\n",
    "# List your files\n",
    "files = ['./data/hd/combined0/train_combined0.txt', './data/hd/combined1/train_combined1.txt', \n",
    "         './data/hd/combined2/train_combined2.txt', './data/hd/combined3/train_combined3.txt']\n",
    "check_intersection(files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate Tokenizer (irrelevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vasi/Documents/BA_Thesis_Experiment/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2TokenizerFast(name_or_path='facebook/opt-350m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from transformers import OPTForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"facebook/opt-350m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def tokenize_and_save(file_prefix, batch_size=100):\n",
    "    prompts_file = f\"data/hd/prepro/{file_prefix}.wp_source\"\n",
    "    stories_file = f\"data/hd/prepro/{file_prefix}.wp_target\"\n",
    "    output_file = f'data/hd/prepro/tokenized/tokenized_{file_prefix}_data.pkl'\n",
    "    \n",
    "    with open(prompts_file, encoding=\"utf-8\") as p, open(stories_file, encoding=\"utf-8\") as f, open(output_file, 'wb') as d:\n",
    "        while True:\n",
    "            prompts = [next(p, None) for _ in range(batch_size)]\n",
    "            stories = [next(f, None) for _ in range(batch_size)]\n",
    "            # Break if the first item is None, indicating end of file\n",
    "            if prompts[0] is None or stories[0] is None:\n",
    "                break\n",
    "            \n",
    "            # Filter out None values in case of mismatched lengths (shouldn't happen with well-formed data)\n",
    "            prompts = [prompt for prompt in prompts if prompt]\n",
    "            stories = [story for story in stories if story]\n",
    "            \n",
    "            # Tokenize batch\n",
    "            tokenized_batch = tokenizer(prompts, stories, padding=True, max_length=1024, truncation=True, return_tensors=\"pt\")\n",
    "            \n",
    "            # Save tokenized batch immediately to reduce memory usage\n",
    "            pickle.dump(tokenized_batch, d)\n",
    "\n",
    "# Example usage\n",
    "for name in [\"train\", \"test\", \"valid\"]:\n",
    "    tokenize_and_save(name, batch_size=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
