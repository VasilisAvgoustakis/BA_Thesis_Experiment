{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Traning Data for GEN2 Finetuning \n",
    "We create a new txt file \"data/md/mixed_gen2/mix2_train_combined.txt\" approx the same size as \"data/hd/combined0/combined0/train_combined0.txt\" \n",
    "of which 1/2 or 50% is human data from \"data/hd/combined2/train_combined2.txt\" and 1/2 or 50% is synthetic data from GEN1's, \"data/sd/gen1/gen1_sd.txt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully combined and written to mix1_train_combined.txt.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "def read_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = file.read().strip().split('\\n\\n')\n",
    "    return data\n",
    "\n",
    "def write_data(data, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write('\\n\\n'.join(data))\n",
    "\n",
    "def get_file_size(filename):\n",
    "    \"\"\"Returns the size of the file in bytes.\"\"\"\n",
    "    return os.path.getsize(filename)\n",
    "\n",
    "# Load data\n",
    "human_data = read_data('./data/hd/combined2/train_combined2.txt')\n",
    "synthetic_data = read_data('./data/sd/gen1/gen1_sd.txt')\n",
    "\n",
    "# Combine and shuffle\n",
    "combined_data = human_data + synthetic_data\n",
    "random.shuffle(combined_data)\n",
    "\n",
    "# Write combined data to a new file\n",
    "write_data(combined_data, './data/md/mixed_gen2/mix2_train_combined.txt')\n",
    "print(\"Data has been successfully combined and written to mix1_train_combined.txt.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the file size of mix2_train_combined.txt is about the same as in the original dataset for gen0 \"data/hd/combined0/train_combined0.txt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original GEN0 HD Size:  307558317\n",
      "Training Dataset for GEN1 Size:  305696540\n"
     ]
    }
   ],
   "source": [
    "print(\"Original GEN0 HD Size: \", get_file_size(\"data/hd/combined0/train_combined0.txt\"))\n",
    "print(\"Training Dataset for GEN1 Size: \", get_file_size(\"data/md/mixed_gen2/mix2_train_combined.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning\n",
    "\n",
    "Finetune the model GEN2 for 5 epochs with the combined txt file containing 1/2 real data and 1/2 synthetic data \"data/md/mixed_gen2/mix2_train_combined.txt\".  \n",
    "The global_step parameter in \"models/distilgpt2-finetuned_gen0_75/trainer_state.json\" and \"models/distilgpt2-finetuned_gen0_75/checkpoint-10755/trainer_state.json\" is originally \"global_step\": 10755, but in order to force run_clm.py to  \n",
    "train the model from the last checkpoint with the new dataset we set this parameter to 0 to force it to learn from the entirety of the new dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-21 13:38:03,032] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-21 13:38:03,656] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2024-04-21 13:38:03,669] [INFO] [runner.py:568:main] cmd = /home/vasi/Documents/BA_Thesis_Experiment/.venv/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None run_clm.py --model_name_or_path distilgpt2 --train_file data/md/mixed_gen2/mix2_train_combined.txt --validation_file data/hd/initial_combined/valid_combined.txt --do_train --do_eval --output_dir ./models/distilgpt2-finetuned_gen2_50 --num_train_epochs 5 --save_strategy epoch --learning_rate 5e-5 --per_device_train_batch_size 4 --gradient_accumulation_steps 4 --deepspeed ds_config.json --resume_from_checkpoint ./models/distilgpt2-finetuned_gen1_75/checkpoint-10755\n",
      "[2024-04-21 13:38:05,494] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-21 13:38:06,035] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}\n",
      "[2024-04-21 13:38:06,035] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0\n",
      "[2024-04-21 13:38:06,035] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})\n",
      "[2024-04-21 13:38:06,035] [INFO] [launch.py:163:main] dist_world_size=2\n",
      "[2024-04-21 13:38:06,035] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1\n",
      "[2024-04-21 13:38:06,036] [INFO] [launch.py:253:main] process 10304 spawned with command: ['/home/vasi/Documents/BA_Thesis_Experiment/.venv/bin/python', '-u', 'run_clm.py', '--local_rank=0', '--model_name_or_path', 'distilgpt2', '--train_file', 'data/md/mixed_gen2/mix2_train_combined.txt', '--validation_file', 'data/hd/initial_combined/valid_combined.txt', '--do_train', '--do_eval', '--output_dir', './models/distilgpt2-finetuned_gen2_50', '--num_train_epochs', '5', '--save_strategy', 'epoch', '--learning_rate', '5e-5', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--deepspeed', 'ds_config.json', '--resume_from_checkpoint', './models/distilgpt2-finetuned_gen1_75/checkpoint-10755']\n",
      "[2024-04-21 13:38:06,037] [INFO] [launch.py:253:main] process 10305 spawned with command: ['/home/vasi/Documents/BA_Thesis_Experiment/.venv/bin/python', '-u', 'run_clm.py', '--local_rank=1', '--model_name_or_path', 'distilgpt2', '--train_file', 'data/md/mixed_gen2/mix2_train_combined.txt', '--validation_file', 'data/hd/initial_combined/valid_combined.txt', '--do_train', '--do_eval', '--output_dir', './models/distilgpt2-finetuned_gen2_50', '--num_train_epochs', '5', '--save_strategy', 'epoch', '--learning_rate', '5e-5', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--deepspeed', 'ds_config.json', '--resume_from_checkpoint', './models/distilgpt2-finetuned_gen1_75/checkpoint-10755']\n",
      "[2024-04-21 13:38:09,568] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-21 13:38:09,569] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-21 13:38:09,734] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-04-21 13:38:09,735] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-04-21 13:38:09,735] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "04/21/2024 13:38:09 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False\n",
      "04/21/2024 13:38:09 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=ds_config.json,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./models/distilgpt2-finetuned_gen2_50/runs/Apr21_13-38-09_U-Bizon,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=5.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./models/distilgpt2-finetuned_gen2_50,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=4,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=./models/distilgpt2-finetuned_gen1_75/checkpoint-10755,\n",
      "run_name=./models/distilgpt2-finetuned_gen2_50,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "04/21/2024 13:38:09 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False\n",
      "Using custom data configuration default-e5737d211651809b\n",
      "04/21/2024 13:38:10 - INFO - datasets.builder - Using custom data configuration default-e5737d211651809b\n",
      "Loading Dataset Infos from /home/vasi/Documents/BA_Thesis_Experiment/.venv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "04/21/2024 13:38:10 - INFO - datasets.info - Loading Dataset Infos from /home/vasi/Documents/BA_Thesis_Experiment/.venv/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "Generating dataset text (/home/vasi/.cache/huggingface/datasets/text/default-e5737d211651809b/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34)\n",
      "04/21/2024 13:38:10 - INFO - datasets.builder - Generating dataset text (/home/vasi/.cache/huggingface/datasets/text/default-e5737d211651809b/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34)\n",
      "Downloading and preparing dataset text/default to /home/vasi/.cache/huggingface/datasets/text/default-e5737d211651809b/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34...\n",
      "04/21/2024 13:38:10 - INFO - datasets.builder - Downloading and preparing dataset text/default to /home/vasi/.cache/huggingface/datasets/text/default-e5737d211651809b/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34...\n",
      "Downloading took 0.0 min\n",
      "04/21/2024 13:38:10 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "Checksum Computation took 0.0 min\n",
      "04/21/2024 13:38:10 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Generating train split\n",
      "04/21/2024 13:38:10 - INFO - datasets.builder - Generating train split\n",
      "Generating train split: 431785 examples [00:03, 120810.61 examples/s]\n",
      "Generating validation split\n",
      "04/21/2024 13:38:13 - INFO - datasets.builder - Generating validation split\n",
      "Generating validation split: 46860 examples [00:00, 77793.36 examples/s]\n",
      "Unable to verify splits sizes.\n",
      "04/21/2024 13:38:14 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /home/vasi/.cache/huggingface/datasets/text/default-e5737d211651809b/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34. Subsequent calls will reuse this data.\n",
      "04/21/2024 13:38:14 - INFO - datasets.builder - Dataset text downloaded and prepared to /home/vasi/.cache/huggingface/datasets/text/default-e5737d211651809b/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34. Subsequent calls will reuse this data.\n",
      "[INFO|configuration_utils.py:726] 2024-04-21 13:38:14,754 >> loading configuration file config.json from cache at /home/vasi/.cache/huggingface/hub/models--distilgpt2/snapshots/2290a62682d06624634c1f46a6ad5be0f47f38aa/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-04-21 13:38:14,756 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"distilgpt2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 6,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.40.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:726] 2024-04-21 13:38:15,102 >> loading configuration file config.json from cache at /home/vasi/.cache/huggingface/hub/models--distilgpt2/snapshots/2290a62682d06624634c1f46a6ad5be0f47f38aa/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-04-21 13:38:15,103 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"distilgpt2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 6,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.40.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-04-21 13:38:15,118 >> loading file vocab.json from cache at /home/vasi/.cache/huggingface/hub/models--distilgpt2/snapshots/2290a62682d06624634c1f46a6ad5be0f47f38aa/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-04-21 13:38:15,118 >> loading file merges.txt from cache at /home/vasi/.cache/huggingface/hub/models--distilgpt2/snapshots/2290a62682d06624634c1f46a6ad5be0f47f38aa/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-04-21 13:38:15,118 >> loading file tokenizer.json from cache at /home/vasi/.cache/huggingface/hub/models--distilgpt2/snapshots/2290a62682d06624634c1f46a6ad5be0f47f38aa/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-04-21 13:38:15,118 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-04-21 13:38:15,118 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-04-21 13:38:15,118 >> loading file tokenizer_config.json from cache at /home/vasi/.cache/huggingface/hub/models--distilgpt2/snapshots/2290a62682d06624634c1f46a6ad5be0f47f38aa/tokenizer_config.json\n",
      "[INFO|configuration_utils.py:726] 2024-04-21 13:38:15,118 >> loading configuration file config.json from cache at /home/vasi/.cache/huggingface/hub/models--distilgpt2/snapshots/2290a62682d06624634c1f46a6ad5be0f47f38aa/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-04-21 13:38:15,119 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"distilgpt2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 6,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.40.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3423] 2024-04-21 13:38:15,224 >> loading weights file model.safetensors from cache at /home/vasi/.cache/huggingface/hub/models--distilgpt2/snapshots/2290a62682d06624634c1f46a6ad5be0f47f38aa/model.safetensors\n",
      "[INFO|configuration_utils.py:928] 2024-04-21 13:38:15,242 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4164] 2024-04-21 13:38:15,545 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:4172] 2024-04-21 13:38:15,545 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:883] 2024-04-21 13:38:15,676 >> loading configuration file generation_config.json from cache at /home/vasi/.cache/huggingface/hub/models--distilgpt2/snapshots/2290a62682d06624634c1f46a6ad5be0f47f38aa/generation_config.json\n",
      "[INFO|configuration_utils.py:928] 2024-04-21 13:38:15,677 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|           | 0/431785 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:3896] 2024-04-21 13:38:15,777 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1129 > 1024). Running this sequence through the model will result in indexing errors\n",
      "[WARNING|run_clm.py:478] 2024-04-21 13:38:15,777 >> ^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits before being passed to the model.\n",
      "Caching processed dataset at /home/vasi/.cache/huggingface/datasets/text/default-e5737d211651809b/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-d3167b5b0639a8a3.arrow\n",
      "04/21/2024 13:38:15 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/vasi/.cache/huggingface/datasets/text/default-e5737d211651809b/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-d3167b5b0639a8a3.arrow\n",
      "Running tokenizer on dataset: 100%|█| 431785/431785 [00:40<00:00, 10630.86 examp\n",
      "Running tokenizer on dataset:   0%|            | 0/46860 [00:00<?, ? examples/s]Caching processed dataset at /home/vasi/.cache/huggingface/datasets/text/default-e5737d211651809b/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-e572d542cd94e401.arrow\n",
      "04/21/2024 13:38:56 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/vasi/.cache/huggingface/datasets/text/default-e5737d211651809b/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-e572d542cd94e401.arrow\n",
      "Running tokenizer on dataset: 100%|█| 46860/46860 [00:05<00:00, 7871.50 examples\n",
      "Running tokenizer on dataset:   0%|            | 0/46860 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:3896] 2024-04-21 13:39:02,698 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1068 > 1024). Running this sequence through the model will result in indexing errors\n",
      "[WARNING|run_clm.py:478] 2024-04-21 13:39:02,698 >> ^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits before being passed to the model.\n",
      "Caching processed dataset at /home/vasi/.cache/huggingface/datasets/text/default-e5737d211651809b/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-24feed55198a96a4.arrow\n",
      "04/21/2024 13:39:02 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/vasi/.cache/huggingface/datasets/text/default-e5737d211651809b/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-24feed55198a96a4.arrow\n",
      "Running tokenizer on dataset: 100%|█| 46860/46860 [00:06<00:00, 7220.86 examples\n",
      "Grouping texts in chunks of 1024: 100%|█| 431785/431785 [01:01<00:00, 7034.04 ex\n",
      "Grouping texts in chunks of 1024:   0%|        | 0/46860 [00:00<?, ? examples/s]Caching processed dataset at /home/vasi/.cache/huggingface/datasets/text/default-e5737d211651809b/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-3da64819324d76bb.arrow\n",
      "04/21/2024 13:40:04 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/vasi/.cache/huggingface/datasets/text/default-e5737d211651809b/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-3da64819324d76bb.arrow\n",
      "Grouping texts in chunks of 1024: 100%|█| 46860/46860 [00:10<00:00, 4667.22 exam\n",
      "Grouping texts in chunks of 1024:  13%|▏| 6000/46860 [00:01<00:07, 5324.57 examp[2024-04-21 13:40:15,250] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "Grouping texts in chunks of 1024: 100%|█| 46860/46860 [00:10<00:00, 4394.24 exam\n",
      "[2024-04-21 13:40:26,195] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-04-21 13:40:26,195] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-04-21 13:40:26,195] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-04-21 13:40:26,198] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n",
      "[2024-04-21 13:40:26,198] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\n",
      "[2024-04-21 13:40:26,198] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer\n",
      "[2024-04-21 13:40:26,198] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-04-21 13:40:26,198] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-04-21 13:40:26,198] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-04-21 13:40:26,198] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-04-21 13:40:26,707] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-04-21 13:40:26,708] [INFO] [utils.py:801:see_memory_usage] MA 0.46 GB         Max_MA 0.46 GB         CA 0.47 GB         Max_CA 0 GB \n",
      "[2024-04-21 13:40:26,708] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 7.78 GB, percent = 25.1%\n",
      "[2024-04-21 13:40:26,871] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-04-21 13:40:26,872] [INFO] [utils.py:801:see_memory_usage] MA 0.46 GB         Max_MA 0.62 GB         CA 0.62 GB         Max_CA 1 GB \n",
      "[2024-04-21 13:40:26,872] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 7.7 GB, percent = 24.8%\n",
      "[2024-04-21 13:40:26,872] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-04-21 13:40:27,033] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-04-21 13:40:27,034] [INFO] [utils.py:801:see_memory_usage] MA 0.46 GB         Max_MA 0.46 GB         CA 0.62 GB         Max_CA 1 GB \n",
      "[2024-04-21 13:40:27,034] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 7.92 GB, percent = 25.5%\n",
      "[2024-04-21 13:40:27,035] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW\n",
      "[2024-04-21 13:40:27,035] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-04-21 13:40:27,035] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-04-21 13:40:27,035] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05, 5e-05], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2024-04-21 13:40:27,035] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-04-21 13:40:27,035] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-04-21 13:40:27,035] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-04-21 13:40:27,035] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-04-21 13:40:27,035] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   bfloat16_enabled ............. False\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x72a3a8b108b0>\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 4\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   gradient_clipping ............ 1.0\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 65536\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   loss_scale ................... 0\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-04-21 13:40:27,036] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-04-21 13:40:27,037] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-04-21 13:40:27,037] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-04-21 13:40:27,037] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-04-21 13:40:27,037] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-04-21 13:40:27,037] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-04-21 13:40:27,037] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-04-21 13:40:27,037] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-04-21 13:40:27,037] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-04-21 13:40:27,037] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-04-21 13:40:27,037] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-04-21 13:40:27,037] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-04-21 13:40:27,037] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-04-21 13:40:27,037] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-04-21 13:40:27,037] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n",
      "[2024-04-21 13:40:27,037] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  4\n",
      "[2024-04-21 13:40:27,037] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-04-21 13:40:27,037] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-04-21 13:40:27,037] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-04-21 13:40:27,037] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-04-21 13:40:27,037] [INFO] [config.py:1000:print]   world_size ................... 2\n",
      "[2024-04-21 13:40:27,037] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-04-21 13:40:27,037] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-04-21 13:40:27,037] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-04-21 13:40:27,037] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-04-21 13:40:27,037] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-04-21 13:40:27,037] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true\n",
      "    }, \n",
      "    \"train_micro_batch_size_per_gpu\": 4, \n",
      "    \"gradient_accumulation_steps\": 4, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "[INFO|deepspeed.py:430] 2024-04-21 13:40:27,038 >> Attempting to resume from ./models/distilgpt2-finetuned_gen1_75/checkpoint-10755\n",
      "[2024-04-21 13:40:27,038] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from ./models/distilgpt2-finetuned_gen1_75/checkpoint-10755/global_step21860/mp_rank_00_model_states.pt...\n",
      "[2024-04-21 13:40:27,176] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from ./models/distilgpt2-finetuned_gen1_75/checkpoint-10755/global_step21860/mp_rank_00_model_states.pt.\n",
      "[2024-04-21 13:40:27,192] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from ./models/distilgpt2-finetuned_gen1_75/checkpoint-10755/global_step21860/mp_rank_00_model_states.pt...\n",
      "[2024-04-21 13:40:27,331] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from ./models/distilgpt2-finetuned_gen1_75/checkpoint-10755/global_step21860/mp_rank_00_model_states.pt.\n",
      "[2024-04-21 13:40:27,433] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from ./models/distilgpt2-finetuned_gen1_75/checkpoint-10755/global_step21860/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-04-21 13:40:27,794] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from ./models/distilgpt2-finetuned_gen1_75/checkpoint-10755/global_step21860/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-04-21 13:40:27,795] [INFO] [engine.py:3031:_get_all_zero_checkpoint_state_dicts] successfully read 2 ZeRO state_dicts for rank 0\n",
      "[2024-04-21 13:40:27,876] [INFO] [engine.py:2963:_load_zero_checkpoint] loading 2 zero partition checkpoints for rank 0\n",
      "[INFO|trainer.py:2031] 2024-04-21 13:40:27,900 >> ***** Running training *****\n",
      "[INFO|trainer.py:2032] 2024-04-21 13:40:27,901 >>   Num examples = 66,073\n",
      "[INFO|trainer.py:2033] 2024-04-21 13:40:27,901 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:2034] 2024-04-21 13:40:27,901 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:2037] 2024-04-21 13:40:27,901 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:2038] 2024-04-21 13:40:27,901 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:2039] 2024-04-21 13:40:27,901 >>   Total optimization steps = 10,325\n",
      "[INFO|trainer.py:2040] 2024-04-21 13:40:27,901 >>   Number of trainable parameters = 81,912,576\n",
      "[INFO|trainer.py:2061] 2024-04-21 13:40:27,901 >>   Continuing training from checkpoint, will skip to saved global_step\n",
      "[INFO|trainer.py:2062] 2024-04-21 13:40:27,901 >>   Continuing training from epoch 0\n",
      "[INFO|trainer.py:2063] 2024-04-21 13:40:27,902 >>   Continuing training from global step 0\n",
      "[INFO|trainer.py:2065] 2024-04-21 13:40:27,902 >>   Will skip the first 0 epochs then the first 0 batches in the first epoch.\n",
      "{'loss': 3.4123, 'grad_norm': 3.705000162124634, 'learning_rate': 0.0, 'epoch': 0.24}\n",
      "{'loss': 3.4148, 'grad_norm': 4.163922309875488, 'learning_rate': 0.0, 'epoch': 0.48}\n",
      "{'loss': 3.4189, 'grad_norm': 3.9985251426696777, 'learning_rate': 0.0, 'epoch': 0.73}\n",
      "{'loss': 3.4142, 'grad_norm': 2.893251895904541, 'learning_rate': 0.0, 'epoch': 0.97}\n",
      " 20%|██████▊                           | 2065/10325 [1:21:54<5:27:27,  2.38s/it][INFO|trainer.py:3288] 2024-04-21 15:02:22,422 >> Saving model checkpoint to ./models/distilgpt2-finetuned_gen2_50/checkpoint-2065\n",
      "[INFO|configuration_utils.py:471] 2024-04-21 15:02:22,423 >> Configuration saved in ./models/distilgpt2-finetuned_gen2_50/checkpoint-2065/config.json\n",
      "[INFO|configuration_utils.py:697] 2024-04-21 15:02:22,424 >> Configuration saved in ./models/distilgpt2-finetuned_gen2_50/checkpoint-2065/generation_config.json\n",
      "[INFO|modeling_utils.py:2584] 2024-04-21 15:02:23,026 >> Model weights saved in ./models/distilgpt2-finetuned_gen2_50/checkpoint-2065/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2488] 2024-04-21 15:02:23,026 >> tokenizer config file saved in ./models/distilgpt2-finetuned_gen2_50/checkpoint-2065/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2497] 2024-04-21 15:02:23,026 >> Special tokens file saved in ./models/distilgpt2-finetuned_gen2_50/checkpoint-2065/special_tokens_map.json\n",
      "[2024-04-21 15:02:23,086] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step23925 is about to be saved!\n",
      "/home/vasi/Documents/BA_Thesis_Experiment/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/vasi/Documents/BA_Thesis_Experiment/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "[2024-04-21 15:02:23,088] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./models/distilgpt2-finetuned_gen2_50/checkpoint-2065/global_step23925/mp_rank_00_model_states.pt\n",
      "[2024-04-21 15:02:23,089] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./models/distilgpt2-finetuned_gen2_50/checkpoint-2065/global_step23925/mp_rank_00_model_states.pt...\n",
      "[2024-04-21 15:02:23,598] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./models/distilgpt2-finetuned_gen2_50/checkpoint-2065/global_step23925/mp_rank_00_model_states.pt.\n",
      "[2024-04-21 15:02:23,599] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./models/distilgpt2-finetuned_gen2_50/checkpoint-2065/global_step23925/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-04-21 15:02:24,403] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./models/distilgpt2-finetuned_gen2_50/checkpoint-2065/global_step23925/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-04-21 15:02:24,409] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ./models/distilgpt2-finetuned_gen2_50/checkpoint-2065/global_step23925/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-04-21 15:02:24,409] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23925 is ready now!\n",
      "{'loss': 3.4166, 'grad_norm': 4.085331439971924, 'learning_rate': 0.0, 'epoch': 1.21}\n",
      "{'loss': 3.4146, 'grad_norm': 3.9519505500793457, 'learning_rate': 0.0, 'epoch': 1.45}\n",
      "{'loss': 3.4156, 'grad_norm': 4.1266045570373535, 'learning_rate': 0.0, 'epoch': 1.69}\n",
      "{'loss': 3.4151, 'grad_norm': 3.942365884780884, 'learning_rate': 0.0, 'epoch': 1.94}\n",
      " 40%|█████████████▌                    | 4130/10325 [2:43:47<4:05:36,  2.38s/it][INFO|trainer.py:3288] 2024-04-21 16:24:15,566 >> Saving model checkpoint to ./models/distilgpt2-finetuned_gen2_50/checkpoint-4130\n",
      "[INFO|configuration_utils.py:471] 2024-04-21 16:24:15,567 >> Configuration saved in ./models/distilgpt2-finetuned_gen2_50/checkpoint-4130/config.json\n",
      "[INFO|configuration_utils.py:697] 2024-04-21 16:24:15,568 >> Configuration saved in ./models/distilgpt2-finetuned_gen2_50/checkpoint-4130/generation_config.json\n",
      "[INFO|modeling_utils.py:2584] 2024-04-21 16:24:16,161 >> Model weights saved in ./models/distilgpt2-finetuned_gen2_50/checkpoint-4130/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2488] 2024-04-21 16:24:16,161 >> tokenizer config file saved in ./models/distilgpt2-finetuned_gen2_50/checkpoint-4130/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2497] 2024-04-21 16:24:16,162 >> Special tokens file saved in ./models/distilgpt2-finetuned_gen2_50/checkpoint-4130/special_tokens_map.json\n",
      "[2024-04-21 16:24:16,222] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step25990 is about to be saved!\n",
      "/home/vasi/Documents/BA_Thesis_Experiment/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "[2024-04-21 16:24:16,224] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./models/distilgpt2-finetuned_gen2_50/checkpoint-4130/global_step25990/mp_rank_00_model_states.pt\n",
      "[2024-04-21 16:24:16,224] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./models/distilgpt2-finetuned_gen2_50/checkpoint-4130/global_step25990/mp_rank_00_model_states.pt...\n",
      "[2024-04-21 16:24:16,717] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./models/distilgpt2-finetuned_gen2_50/checkpoint-4130/global_step25990/mp_rank_00_model_states.pt.\n",
      "[2024-04-21 16:24:16,718] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./models/distilgpt2-finetuned_gen2_50/checkpoint-4130/global_step25990/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-04-21 16:24:17,518] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./models/distilgpt2-finetuned_gen2_50/checkpoint-4130/global_step25990/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-04-21 16:24:17,518] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ./models/distilgpt2-finetuned_gen2_50/checkpoint-4130/global_step25990/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-04-21 16:24:17,518] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25990 is ready now!\n",
      "{'loss': 3.413, 'grad_norm': 4.39361047744751, 'learning_rate': 0.0, 'epoch': 2.18}\n",
      "{'loss': 3.4164, 'grad_norm': 4.100351333618164, 'learning_rate': 0.0, 'epoch': 2.42}\n",
      "{'loss': 3.4111, 'grad_norm': 2.7874391078948975, 'learning_rate': 0.0, 'epoch': 2.66}\n",
      "{'loss': 3.417, 'grad_norm': 2.9752635955810547, 'learning_rate': 0.0, 'epoch': 2.91}\n",
      " 60%|████████████████████▍             | 6195/10325 [4:05:37<2:42:37,  2.36s/it][INFO|trainer.py:3288] 2024-04-21 17:46:06,072 >> Saving model checkpoint to ./models/distilgpt2-finetuned_gen2_50/checkpoint-6195\n",
      "[INFO|configuration_utils.py:471] 2024-04-21 17:46:06,073 >> Configuration saved in ./models/distilgpt2-finetuned_gen2_50/checkpoint-6195/config.json\n",
      "[INFO|configuration_utils.py:697] 2024-04-21 17:46:06,073 >> Configuration saved in ./models/distilgpt2-finetuned_gen2_50/checkpoint-6195/generation_config.json\n",
      "[INFO|modeling_utils.py:2584] 2024-04-21 17:46:06,668 >> Model weights saved in ./models/distilgpt2-finetuned_gen2_50/checkpoint-6195/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2488] 2024-04-21 17:46:06,668 >> tokenizer config file saved in ./models/distilgpt2-finetuned_gen2_50/checkpoint-6195/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2497] 2024-04-21 17:46:06,669 >> Special tokens file saved in ./models/distilgpt2-finetuned_gen2_50/checkpoint-6195/special_tokens_map.json\n",
      "[2024-04-21 17:46:06,723] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step28055 is about to be saved!\n",
      "/home/vasi/Documents/BA_Thesis_Experiment/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "[2024-04-21 17:46:06,725] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./models/distilgpt2-finetuned_gen2_50/checkpoint-6195/global_step28055/mp_rank_00_model_states.pt\n",
      "[2024-04-21 17:46:06,725] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./models/distilgpt2-finetuned_gen2_50/checkpoint-6195/global_step28055/mp_rank_00_model_states.pt...\n",
      "[2024-04-21 17:46:07,221] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./models/distilgpt2-finetuned_gen2_50/checkpoint-6195/global_step28055/mp_rank_00_model_states.pt.\n",
      "[2024-04-21 17:46:07,222] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./models/distilgpt2-finetuned_gen2_50/checkpoint-6195/global_step28055/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-04-21 17:46:07,909] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./models/distilgpt2-finetuned_gen2_50/checkpoint-6195/global_step28055/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-04-21 17:46:07,909] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ./models/distilgpt2-finetuned_gen2_50/checkpoint-6195/global_step28055/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-04-21 17:46:07,909] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28055 is ready now!\n",
      "{'loss': 3.4189, 'grad_norm': 3.982415199279785, 'learning_rate': 0.0, 'epoch': 3.15}\n",
      "{'loss': 3.4149, 'grad_norm': 2.8400442600250244, 'learning_rate': 0.0, 'epoch': 3.39}\n",
      "{'loss': 3.4139, 'grad_norm': 3.7069101333618164, 'learning_rate': 0.0, 'epoch': 3.63}\n",
      "{'loss': 3.4141, 'grad_norm': 3.8730154037475586, 'learning_rate': 0.0, 'epoch': 3.87}\n",
      " 80%|███████████████████████████▏      | 8260/10325 [5:27:28<1:22:12,  2.39s/it][INFO|trainer.py:3288] 2024-04-21 19:07:56,650 >> Saving model checkpoint to ./models/distilgpt2-finetuned_gen2_50/checkpoint-8260\n",
      "[INFO|configuration_utils.py:471] 2024-04-21 19:07:56,651 >> Configuration saved in ./models/distilgpt2-finetuned_gen2_50/checkpoint-8260/config.json\n",
      "[INFO|configuration_utils.py:697] 2024-04-21 19:07:56,651 >> Configuration saved in ./models/distilgpt2-finetuned_gen2_50/checkpoint-8260/generation_config.json\n",
      "[INFO|modeling_utils.py:2584] 2024-04-21 19:07:57,250 >> Model weights saved in ./models/distilgpt2-finetuned_gen2_50/checkpoint-8260/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2488] 2024-04-21 19:07:57,250 >> tokenizer config file saved in ./models/distilgpt2-finetuned_gen2_50/checkpoint-8260/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2497] 2024-04-21 19:07:57,250 >> Special tokens file saved in ./models/distilgpt2-finetuned_gen2_50/checkpoint-8260/special_tokens_map.json\n",
      "[2024-04-21 19:07:57,306] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step30120 is about to be saved!\n",
      "/home/vasi/Documents/BA_Thesis_Experiment/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "[2024-04-21 19:07:57,308] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./models/distilgpt2-finetuned_gen2_50/checkpoint-8260/global_step30120/mp_rank_00_model_states.pt\n",
      "[2024-04-21 19:07:57,308] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./models/distilgpt2-finetuned_gen2_50/checkpoint-8260/global_step30120/mp_rank_00_model_states.pt...\n",
      "[2024-04-21 19:07:57,653] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./models/distilgpt2-finetuned_gen2_50/checkpoint-8260/global_step30120/mp_rank_00_model_states.pt.\n",
      "[2024-04-21 19:07:57,655] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./models/distilgpt2-finetuned_gen2_50/checkpoint-8260/global_step30120/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-04-21 19:07:58,344] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./models/distilgpt2-finetuned_gen2_50/checkpoint-8260/global_step30120/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-04-21 19:07:58,345] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ./models/distilgpt2-finetuned_gen2_50/checkpoint-8260/global_step30120/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-04-21 19:07:58,345] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30120 is ready now!\n",
      "{'loss': 3.4168, 'grad_norm': 3.013005018234253, 'learning_rate': 0.0, 'epoch': 4.12}\n",
      "{'loss': 3.413, 'grad_norm': 3.2520437240600586, 'learning_rate': 0.0, 'epoch': 4.36}\n",
      "{'loss': 3.4167, 'grad_norm': 3.480463743209839, 'learning_rate': 0.0, 'epoch': 4.6}\n",
      "{'loss': 3.4145, 'grad_norm': 4.18679666519165, 'learning_rate': 0.0, 'epoch': 4.84}\n",
      "100%|███████████████████████████████████| 10325/10325 [6:49:16<00:00,  2.38s/it][INFO|trainer.py:3288] 2024-04-21 20:29:44,800 >> Saving model checkpoint to ./models/distilgpt2-finetuned_gen2_50/checkpoint-10325\n",
      "[INFO|configuration_utils.py:471] 2024-04-21 20:29:44,801 >> Configuration saved in ./models/distilgpt2-finetuned_gen2_50/checkpoint-10325/config.json\n",
      "[INFO|configuration_utils.py:697] 2024-04-21 20:29:44,801 >> Configuration saved in ./models/distilgpt2-finetuned_gen2_50/checkpoint-10325/generation_config.json\n",
      "[INFO|modeling_utils.py:2584] 2024-04-21 20:29:45,403 >> Model weights saved in ./models/distilgpt2-finetuned_gen2_50/checkpoint-10325/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2488] 2024-04-21 20:29:45,403 >> tokenizer config file saved in ./models/distilgpt2-finetuned_gen2_50/checkpoint-10325/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2497] 2024-04-21 20:29:45,403 >> Special tokens file saved in ./models/distilgpt2-finetuned_gen2_50/checkpoint-10325/special_tokens_map.json\n",
      "[2024-04-21 20:29:45,456] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step32185 is about to be saved!\n",
      "/home/vasi/Documents/BA_Thesis_Experiment/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "[2024-04-21 20:29:45,459] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./models/distilgpt2-finetuned_gen2_50/checkpoint-10325/global_step32185/mp_rank_00_model_states.pt\n",
      "[2024-04-21 20:29:45,459] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./models/distilgpt2-finetuned_gen2_50/checkpoint-10325/global_step32185/mp_rank_00_model_states.pt...\n",
      "[2024-04-21 20:29:45,964] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./models/distilgpt2-finetuned_gen2_50/checkpoint-10325/global_step32185/mp_rank_00_model_states.pt.\n",
      "[2024-04-21 20:29:45,966] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./models/distilgpt2-finetuned_gen2_50/checkpoint-10325/global_step32185/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-04-21 20:29:46,766] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./models/distilgpt2-finetuned_gen2_50/checkpoint-10325/global_step32185/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-04-21 20:29:46,767] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ./models/distilgpt2-finetuned_gen2_50/checkpoint-10325/global_step32185/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-04-21 20:29:46,767] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32185 is ready now!\n",
      "[INFO|trainer.py:2299] 2024-04-21 20:29:46,769 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 24558.8683, 'train_samples_per_second': 13.452, 'train_steps_per_second': 0.42, 'train_loss': 3.4151411522964588, 'epoch': 5.0}\n",
      "100%|███████████████████████████████████| 10325/10325 [6:49:18<00:00,  2.38s/it]\n",
      "[INFO|trainer.py:3288] 2024-04-21 20:29:47,079 >> Saving model checkpoint to ./models/distilgpt2-finetuned_gen2_50\n",
      "[INFO|configuration_utils.py:471] 2024-04-21 20:29:47,080 >> Configuration saved in ./models/distilgpt2-finetuned_gen2_50/config.json\n",
      "[INFO|configuration_utils.py:697] 2024-04-21 20:29:47,081 >> Configuration saved in ./models/distilgpt2-finetuned_gen2_50/generation_config.json\n",
      "[INFO|modeling_utils.py:2584] 2024-04-21 20:29:47,702 >> Model weights saved in ./models/distilgpt2-finetuned_gen2_50/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2488] 2024-04-21 20:29:47,702 >> tokenizer config file saved in ./models/distilgpt2-finetuned_gen2_50/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2497] 2024-04-21 20:29:47,702 >> Special tokens file saved in ./models/distilgpt2-finetuned_gen2_50/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        5.0\n",
      "  train_loss               =     3.4151\n",
      "  train_runtime            = 6:49:18.86\n",
      "  train_samples            =      66073\n",
      "  train_samples_per_second =     13.452\n",
      "  train_steps_per_second   =       0.42\n",
      "04/21/2024 20:29:47 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3597] 2024-04-21 20:29:47,787 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3599] 2024-04-21 20:29:47,787 >>   Num examples = 10105\n",
      "[INFO|trainer.py:3602] 2024-04-21 20:29:47,787 >>   Batch size = 8\n",
      "100%|█████████████████████████████████████████| 632/632 [04:19<00:00,  2.43it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        5.0\n",
      "  eval_accuracy           =     0.3637\n",
      "  eval_loss               =     3.3447\n",
      "  eval_runtime            = 0:04:20.33\n",
      "  eval_samples            =      10105\n",
      "  eval_samples_per_second =     38.815\n",
      "  eval_steps_per_second   =      2.428\n",
      "  perplexity              =    28.3524\n",
      "[INFO|modelcard.py:450] 2024-04-21 20:34:08,523 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.3637166545021168}]}\n",
      "[2024-04-21 20:34:09,914] [INFO] [launch.py:348:main] Process 10305 exits successfully.\n",
      "[2024-04-21 20:34:10,915] [INFO] [launch.py:348:main] Process 10304 exits successfully.\n"
     ]
    }
   ],
   "source": [
    "!deepspeed run_clm.py \\\n",
    "    --model_name_or_path distilgpt2 \\\n",
    "    --train_file data/md/mixed_gen2/mix2_train_combined.txt \\\n",
    "    --validation_file data/hd/initial_combined/valid_combined.txt \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --output_dir ./models/distilgpt2-finetuned_gen2_50 \\\n",
    "    --num_train_epochs 5 \\\n",
    "    --save_strategy epoch \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --deepspeed ds_config.json \\\n",
    "    --resume_from_checkpoint ./models/distilgpt2-finetuned_gen1_75/checkpoint-10755\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "### Step 3\n",
    "Let the model generate 100 stories from a ChatGPT4-generated prompts and write them to an output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep in the Amazon rainforest, a team of scientists discovers a hidden tribe with a secret that could change the course of history.\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/sd/eval_prompts/eval_prompts.txt\") as pfile:\n",
    "    prompts = pfile.readlines()\n",
    "\n",
    "print(prompts[3].split('.')[1][1:]+\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vasi/Documents/BA_Thesis_Experiment/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a post-apocalyptic world where nature has reclaimed the cities, a group of survivors seeks refuge in an abandoned library.\n",
      "`` There's nothing to do. ''   `` It won't work, you know? What if I get caught and killed by someone else here on earth instead of just being there for me like that... but then again, they would have no idea how long it took before anyone noticed them or even did anything about it anyway! How could something be so stupid as to make people forget what happened last time we were born.. And who knows when things will end up different than this one thing is left behind with us forever forgotten - The only difference between our two lives are those we both love now because each other loves her more every night all alone ; she was always my favorite girl after school -- ever since i grew older he had gotten old fast enough to notice him constantly coming out of bed while crying over his mommy's little pink toy toys which eventually became mine too. So many memories from previous times together making sure everything went according to plan until today morning ( 4th Tuesday ) right at midnight without bothering to finish another quick checkup day off tomorrow afternoon etc. That sounds great though : At least everyone keeps having fun doing whatever their life wants whenever possible due to some weird coincidence involving *your* friends/relatives leaving your own house during the week later & still getting back to sleep once per year haha   After finishing first checking up yesterday evening however, well done man.. Ok im going to go ahead and start reading through these prompts soon lol Im gon na read any part 2 thats ok lets see why humanity does not survive anymore given its been destroyed 3 years ago most of the humans seem to want to live happily ever after except maybe eons into adulthood anyways whats kind of funny dude hes talking about human extinction cause u dont need to worry guys mate ami kinda bored huh oh yea god dammit yeah come around next stophope listen wtf shit fucking sound good alright fuck yes wait bro let me tell ya goodbye john please shut down mrppbsshhh sorry buddyshow highway bye hey look hello okay hmm hi John satin outside listening to music everyday almost immediately started hearing footsteps approaching inside doorways doors leading downstairs slowly opening wide open doorway towards the front hallway room hall facing the main entrance gate near the front window opposite the main exit gates turn the knob upwards slightly turning downwards causing several loud knocks\n",
      "On a remote space station orbiting a distant planet, tensions rise among the crew as mysterious occurrences threaten their mission.\n",
      "It had been twenty years since we first discovered that life forms existed. The world was so far away from us and all other living things were just floating around in our vast, yet beautiful light.   `` So what happened? '' I asked with a grimace.   `` We've got no idea how it started out but when you see people like you on your ship or something go off... Then they start to come back for some reason because of *this* coincidence! It turns into an epidemic somewhere within ourselves which can destroy anything at any moment without damaging anyone else.. And then there is this new phenomenon known as 'The Revenant'where every single person goes home one day until death happens again ( not really ) - everything lasts forever after someone dies ; only once does somebody realize why he/she has survived long enough to know who she could be if her actions would cause such terrible damage over time -- even though everyone knows exactly whats happening now- except me : my friends have decided to make fun of them by making sure nobody gets hurt too much while still being alive whenever possible before going insane anyway. My girlfriend told me about these situations pretty quickly afterward saying nothing more than `` You can't possibly survive anymore unless you die soon either way thanks to God blessing Satan himself AND give him eternal damnation regardless of whether He wishes to kill yourself instead of killing myself rather than letting his own thoughts wander through humanity altogether** She said absolutely nothing ever quite right next to me telling me otherwise already thinking very carefully “ Well yes sir… well thats certainly true considering how many times humans died due to unforeseen events leading up to Earth becoming extinct etc ” Now here comes another question entirely : did i get sick early last night knowing full well THAT **HUMANDA** IS DEAD BY GOD NOT HAPPENING TO ME BEFORE YOU FEEL LIKE THIS MOTHER FUCKERS EVERYONE IN THE WORLD HAS BEEN KILLED AFTER ALL THESE YEARS BUT WHEN THEY CAME OUT OF IT THEN WHY ARE SOME PEOPLE GOING TO FIND THEIR OWN SEXUALITY WITHOUT ANY REAL WITNESSES AGAINST MY LIVES ASKING FOR A LAST LIFE OR SO JUST THOUGH WE HAVE ALWAYS REQUIRED ONE WAY ONLY IF HE DOES N'T KNOW WHAT DID HIS NAME SAY ABOUT HER – HOW CAN SHE GET EVEN MORE TALKIN\n",
      "In a futuristic city where memories can be bought and sold, a detective investigates a series of memory-related crimes.\n",
      "`` I'm here to tell you, '' said the man standing behind me. He was wearing a suit that looked like it had been made from old leather bound paper with no logo on either side. `` This is your last chance at saving my life... or perhaps dying in this moment! You know what?   `` Yes sir, but don't worry about anything else except for how long we wait until he gets there first.. The whole world will take care of him soon enough as his soulmate comes back alive so please let us go home alone too quickly anyway if anyone needs help getting out then give them some time off before they get into trouble again eventually because i ca n ’ t risk losing someone ; ) ''   `` Wait… just look around guys now who are still going through these trials really well right? If only everyone could see all those people being put down by themselves without any repercussions when one of them dies suddenly after their initial thought process has finished doing nothing wrong huh? Maybe not everybody knows exactly why even though every single person whose body died instantly would have lived another day longer than normal* That does sound weird - maybe thats true since nobody ever actually experienced such things anymore ( unless death itself happened faster sometimes anyways haha ) Anyway : ) *I hope everything goes smoothly alright tomorrow morning**   ***Chapter One***   **Chapter Two***   -- -   Thanks /u/The_Donalds232 for reading an excerpt [ 3 ] while also having fun playing Fallout 4 online which should make sense given its title & characters may need more details regarding dialogue etc. Any feedback seems very welcome ^^Thanks @ # # # # # # # # # [ ] ( https: //www2.wikiawiki4com//Diary_of_Nuclear_Matter )\n",
      "In order to keep yourself sane once per year – save up money later during school holidays instead of spending extra years practicing writing prompts using Redditors' accounts > They wo n´t stop caring today especially considering reddit exists due to user complaints claiming users accidentally wrote memes incorrectly under Google+ posts rather than trying to figure out whether other subreddits post comments were meant to improve our lives better. However, upon further inspection, two new threads posted yesterday found links suggesting otherwise similar sites existed between different stories shared via comment boards across various social\n",
      "Deep in the Amazon rainforest, a team of scientists discovers a hidden tribe with a secret that could change the course of history.\n",
      "`` I'm just not sure what it is, but this whole thing has to be discovered. ''   `` Why are you telling me about my wife?! My entire life and her family have been murdered by something so different than ours... The way she does things without our knowledge or consent.. Her name was Diana, we all know who killed them before they were born out here on Earth because there had to be some kind-of magic for us to don't get caught up in their lives forever like those people did after being cut down from a tree as an infant at birth through the use of magic - how can someone still remember where your wife was when you left behind while cleaning these forests instead of having to take care of everyone else around you every day trying to figure out why everything changed overnight if one of us found any clues right now then anyways. You mean no one should ever die again either ; only knowing death would cause pain too much though eh? So maybe Death wanted to bring back his memories more often since he needed to cleanse himself off once another time huh? And honestly i ca n ’ t imagine anyone going to tell him anymore anyway : ) They said nothing really except for myself thinking twice already… Well well guess its fine thanks manly menial jobs sometimes come along pretty quick guys – especially working long shifts such as mine ( which usually brings even less stress due to work schedules etc ).. But thats ok thank god God dammit go ahead let me finish writing tomorrow morning haha….I hope others enjoy reading~~ ^^^^ [ https ] ( http: //www2.redditimg.com/user/CnL6t7lP9f0QqwYgRdAJ3XjKzNrTxEcO1D8oHkZFUvM5iSb_pVuBhGsWaBlWWyNGOUeId=UTF8 )   **EDIT** *Warning* If anything went wrong please leave constructive criticism aside~ Sorry OP /u/_awkwardwrites lol ***\n",
      "You find yourself stranded somewhere completely alone between two strangers waiting for each other inside of a gas station across the country during lunchtime until 3pm EST -- Your destination seems awfully strange considering today marks 6:30\n",
      "At an exclusive resort nestled in the Swiss Alps, guests find themselves trapped during a snowstorm with a killer on the loose.\n",
      "The man who lived next door to me is sitting across from me. `` I am not going to die here, '' he says as he takes off his hood and pulls out a gun at me. He looks around for any weapons or gadgets that could be useful against him though.   `` What do you want? You can come down tonight! Please join us if you like what we have today. We're ready now right there so tell me how it goes without waiting until nightfall... ''   `` That sounds good sir. If you are interested in joining us then let me know by emailing your name please send my phone number before closing up again.. ''   `` Yes Sir '' I say excitedly while rubbing my eyes trying desperately to remember where this was coming from.   `` The most beautiful thing about life has always been our love of adventure - freedom never ceases to be known but when something happens- everything seems normal too fast sometimes people start asking questions instead of answers which leads them to insanity why shouldn't they just ask one question more time already after being asked another person would simply keep saying nothing because everyone else probably knows all these things over their heads anyway anyways haha well thats ok i got ta go get some coffee sometime soon alright lets head back home okay calm fuckhead no shit im gon na take care of myself dammit its fine dont worry dude whats done won't stop till midnight oh god damn yeah hes sorry wont wait til dawn tomorrow morning Im afraid maybe even sleep better than usual since every fucking day doesnt feel much different anymore babe hmm hahahahaha bro guys Its almost 8am later still dreaming*what did she dream girl *did she forget* her dreams were real except none of those nightmares made sense either way only 3/10pm**THUNDERLY***OH FUCKHAAHAHAHAHAHAHAHAWAHHHH HA HUH UGHH ughhhhhhhmmm ”   ***   **THE END OF DAY : A TUESDAY NIGHT ( 5:00AM ) ****   As per protocol stated above ; 1st January 2015 6:15AM   No sound came through windows inside the building due to lack of noise outside. There had also been no lights anywhere near the main entrance yet although luckily nobody noticed anything unusual whatsoever despite the fact that everybody knew\n",
      "In a small coastal town, rumors of a haunted lighthouse lead a curious teenager to uncover dark secrets buried in the past.\n",
      "`` What are you doing here? ''   `` I just got another visitor, one that was very strange. He's dressed like a ghost... and he didn't have any shoes or anything else on him yet! You can see his face right now too. It looks kinda creepy but there is something about it though : it's probably part of his mind being scared for some reason so i guess this might be why we need to do something at all today..I think if they try to find out what happened then maybe their little girl will finally wake up from her nightmare she could go home with them again tomorrow morning anyway --it would mean nothing when someone tries to get away without me having to leave our house because sometimes my mom can't even make sure things were ok until school breaks down after recesses last year-and tonight means everything really bad*   My parents always say these days since Christmas Eve as an adult who has never been able look into anyone other than myself before ( not necessarily ). They also said every Sunday evening should bring special presents ; yes Mommy gets off work early next week instead of going outside everyday due to holidays day - which happens pretty often anyways during those times people want to stay indoors/outside looking around while others live near the same time whenever possible through social media sites such Asperger & Friends tell stories where everyone lives within 5 minutes of eachother saying how lonely *this* makes sense especially considering your mother wo n ’ t come back later anymore either way…but hey dad says its good enough cause hes late otherwise no one ever knows exactly whats happening** So yeah Dad tells us stuff sounds weird haha actually lets check reddit under /r/_the_spooky_tales @ r/doriarty # 2 ^^^ [ ] ( https: //www._redditusercontent^/comments/*m3l0u8c4gjfqh9v5p7zxb1nywY2kWQoUghRaD % 3B > ) ** [ ] ( http=//i.imgur.com/PZEeLXNVJF./gif/CydTODGshKA < 1 > ) ** [ ] ( https = 0.0.0/png/gif/113388\n",
      "In a world where magic is outlawed, a young sorcerer must navigate a dangerous underground network to harness her powers.\n",
      "`` This will be my last time. ''  `` I'm sorry, but we were all just trying our best not to get in trouble like this... We had so much fun! And you know what? It was hard work and some of the worst things happened during that one-hundredth year anniversary party with everyone at the same time as us! That whole thing didn't help though anyway because it turned out to be pretty good for everybody here today even after they stopped coming back from the parties or whatever. So instead of going into your house every night while drinking wine on the weekends there would be only three people who could take part right now if you wanted to go home alive any minute - no matter how many times someone tried to try and kill them without fail ; well when somebody tries to attack you don't expect anything else except silence until their eyes are on you screaming profanities about everything possible happening around you too. You should really have known better than to let anyone think up something stupid such an idea unless you thought twice before doing these kinds of actions by yourself can make sense : )   The first problem came naturally given the fact that most sorcerers got themselves killed early enough ( usually killing dragons quickly getting close to being destroyed afterwards due to lack of safety precautions etc ) and then continued living longer years together knowing exactly why others acted differently based off of me thinking those two ideas became *normal* anyways since both of them seemed to be different between us rather than having sex everyday again which meant neither of us actually knew each other either way besides maybe accidentally using magical spells against eachother somehow causing more problems depending on whether we ended up hurting ourselves anymore. As far as the second issue goes, however, I am still feeling fairly normal nowadays thanks to the sheer amount of money involved compared favourably to spending half my life alone giving away gifts whilst drunk driving outside public places apparently keeping track of whoever took his place/sickeningly long enough to avoid accidents resulting in deaths caused by accident itself sometimes making sure to keep running whenever he went over him already made sure to never break ground nearby buildings otherwise nobody noticed yet leaving behind cars despite finding himself stuck under traffic lights preventing anybody from noticing pedestrians passing through parked cars simply letting loose onto the street below parking lot near the end of the road directly opposite the exitway towards the entrance of the\n",
      "Amidst the ruins of a once-great civilization, a lone wanderer searches for the legendary artifact that could restore balance to the world.\n",
      "The first words I remember hearing in my head were those of an old friend who lost his wife and her family.   `` She's gone! '' I yelled, shaking as if it was trying to break free from the shackles around me.   `` Oh how she went? It sure looked like there had been something wrong with her... '' He stammered, holding up a piece of paper he held on a table nearby.   `` What did you say last night about this woman? You think we should go over here now or else maybe one day find out what happened next.. '' His voice sounded even more desperate than before but nothing seemed to come back strong enough to make sense at all so he said nothing ; just silence again until everything calmed down quickly.   `` No sir, no way don't look scared anymore - your mother has disappeared too long ago by the time we get home after school is done yet someone needs to help us move away while we wait outside during class when everyone leaves their parents house alone without saying anything because they can hear them doing things which are not important either. That does n´t matter though : We have left our homes intact anyway since then i never felt any need to disturb anyone except myself ( sorry ) and Mommy. But hey mommy always loves me anyways haha..I really wish dad would take care of him right off the bat ok? Anyway whats going on huh? Is mumma dead alright okay? Its fine Mum won't be able stay alive forever lol   The door opened behind me instantly closing slowly giving way to the sound of footsteps coming through the doorway towards me. I saw my stepfather standing above me staring at me waiting patiently awaiting another answer where he only took few steps forward into the room between us. A short pause followed shortly thereafter looking directly ahead toward the small window leading into the hallway below. There stood a young man sitting beside him walking past me silently enjoying himself whilst watching other people walk along the path avoiding the sight of him entering the room instead of following closely behind. My heart racing slightly however soon I realized why today seems different being closer still makes sense considering these two men sat opposite eachother awkwardly attempting to keep eye contact against each other “ Hey boy am I supposed show ya some love please don ’ t\n",
      "On a distant colony planet, colonists must band together to survive when their settlement is threatened by an alien invasion.\n",
      "I'm not sure why I felt so bad for the first time. But then again, that wasn't exactly what happened at all - this year it took me three days to get up and walk down one of the narrow tunnels into the mountains.   `` Hey! '' he shouted from behind me, `` What? You could go back there if you wanted but here we are... We can move through another day with nothing left in us just like any other human being would do now. And still no-one will see my face because they did n ’ t have enough money or food to eat anymore anyway. Maybe some people think maybe our technology has gone too far though ; perhaps even humanity wants more than anything else… ''   The man looked around him, his eyes widening as he turned away slowly towards the tunnel entrance. He heard voices calling out over the radio :  *Dang! Dang! Dang! *   Silence followed after. A voice on the speaker said : **Dang! Dang! Dang! Dang! Dang! Dang! Dang! DANG! Dang! Dang! DANG! DANG!   It started slow inside – everyone had stopped talking since last week*, which gave me pause before looking directly at the walls below. There were also several large windows visible above them, making it impossible to see who got ahead without seeing anyone outside. After every few minutes, someone walked past the small windowless door between them. Another person stood next to him, wearing a black suit made of white cloth wrapped in a leatherbound shirt.   `` Hello Mr. President, '' the man said quietly.   `` Do you understand your situation? Is everything alright yet? Please tell me something about how things went wrong earlier today.. ''   `` Oh yeah, sorry sir, please stop speaking soon Sir. This means very little trouble right now either way -- wait until tomorrow morning where am I supposed be able talk to myself while sitting under the covers doing laundry during the night shift later tonight ( 8:30 pm ).. well yes thank you. Now hurry along nicely okay? Let me guess let me know whenever possible once i feel comfortable getting dressed early afternoon**   As quickly as possible, I stepped onto the wall opposite the door leading to the\n",
      "In a society where emotions are forbidden, two rebels risk everything to experience love for the first time.\n",
      "`` Oh god! ''   `` No. I'm not here, you know what? You can't go anywhere without me and then my people will hate each other forever unless they can do something about it... ''   `` What if we were all alone in our own world or someone else would come down with this shit right now as well? It makes no sense how hard that is when everyone just wants nothing more than to be loved by anyone who has ever been there before but still does n´t want anything less than their last meal of theirs because even though she was gone long ago her life had changed drastically since birthdays.. She looked so young again after years of trying and failing at every opportunity he could find himself facing his own problems instead of being able to keep up with them anymore. He didn't understand why nobody wanted him back either ; until today morning on Saturday afternoon another man from the city went out into the street carrying groceries outside saying hello to whoever lived next door neighbor told him hi - which made little difference anyway considering the fact that police found him dead while walking home everyday only minutes later one of the men got hurt whilst running towards the car parked next to him claiming he heard footsteps coming around town like crazy tourists following them through some alleyway looking for food ( except apparently an old friend said whatever ).. So very soon these thoughts started to fill in thinking that maybe someday those dreams might finally end together eventually enough i guess thats impossible haha : ) But really let me tell you story nonetheless eh huh ya wo na have to wait till tomorrow anyways… Okay alright ok okay Ok guys welcome fellow adventurers everywhere check your stuffy buddy please give us feedbacks too lol Alright thanks folks its going to take care of things pretty much anyways make sure you dont fall asleep over yourself cause hey whats happened already oh yea sorry uhhh yeah ill wake up tired sleepy awake fast forward 2 hours early tonight 5th day yup get ready wtf happend 7th day eve 1st of May 3rd 1847 thursday 4th 19th 20th 21st 22nd 23rd 23rd 24th 25th 26th 27th 28th 29th 30th 31st 32nd 33rd 34th 35th 36th 37th 38th 39th 40th 41st 42nd 43rd 44th 45th 46th 47th 48th 49th\n",
      "In a dystopian future where technology controls every aspect of life, a hacker discovers a hidden message that could change everything.\n",
      "`` Hello, my name is Sam. ''   `` Hi, your voice? ''   `` What do you mean by it's an idea... we have the ability to communicate with each other through radio waves and light sources - but what does this means for us? How did I become such a famous scientist in 2036 when he became known as the inventor of artificial intelligence? We are now at war against one another on our own terms so let's just talk about something else! ''   `` Well yeah..that was me first time writing anything before then i guess not too much fun since everyone has their stories or whatever they want from them though haha So lets see whats happening eh ok okay maybe some more questions come up here please tell me how many times can people get involved if possible anyway im getting bored like always bein right out there dude got ta help him because his whole family wants to know why these guys weren't even close enough today lol ) This man had been called into office after all things being said etc But most importantly didnt seem to care anymore oh well thats really kinda cool guy already seems to think its over pretty soon ummm wait no problem buddy lookheh dont worry ya wont die everyday nevermind mr lmao still remember seeing her dad face while she played video games huh Ok alright boy again yea thanks god dammit hm sure hope someone doesnt feel scared back home babe looks nothing weird cause daddy says hello hey wan na go watch tv show kiddos mommy dame crybaby crying baby son who knows hi goodnight mate : )   The door opened slowly open revealing a small room filled only with two chairs sitting directly in front of the screen which seemed to be covered in a large circular table staring at the ceiling above which stood a huge group of men dressed in white carrying sticks around their necks. One of the men looked like he knew exactly whose question would lead him to believe himself ; John Hensley ( aka `` Master Brainiac '' ) sat next to the man looking at a picture book reading “ Oh yes sir don ’ t forget to check reddit r/writingprompting any further…just read [ story ] https: //www-redditimg.com/media/default_image/v1nYHd2q0jKz4L\n",
      "In the midst of a zombie apocalypse, a group of survivors takes refuge in an abandoned shopping mall, only to discover they're not alone.\n",
      "“ It ’ s okay, I'm fine. You can go see my house on this side of town if you want, but it seems like no one has been there for too long and that is what keeps me here tonight. We need to get out now before we lose count… ''   The man stared at the screen blankly as he slowly turned his head around towards the exit door behind him. He was standing about 20 feet away from the front door with a large wooden door hanging loosely over his chest so far down where he could barely make eye contact while looking back up into the night sky when suddenly something slammed against the wall outside.   “ Do you have any idea why? What happened yesterday... *I* didn't know! No-one knows how or who came inside last time.. But then again - just keep going through these motions every day until tomorrow morning does n´t seem like such a good thing….we got ta find someone else today because nobody sees us anymore…..And even though everyone says all our lives are pointless without anything important happening**why do you always believe everything anyway? And since your mother died two years ago – oh dear god she never saw anyone except yourself after her death 2 years later ( which made sense ) …because somebody should be dead already ; She said nothing ever happens during those days anyways……and besides, donny think people would stop by their store everyday haha : ) Well yeah right next to the door looks exactly alike everywhere i am im gon na look straight ahead ok Ok maybe its probably alright guys dont feel weird walking along stairs instead of getting madman thats kinda scary really fast eh hey let me talk to ya bro dude u cant wait till 8:00 pm lets face it huh Anyway whats wrong buddy please come forward OK sorry bud bye friend ]   Edit 1/2 /r/WritingPrompts # 10 [ WP ] Writing Prompt > Write prompt < 3rd person writing prompts will fail lol ^_^Aww~ Thanks^^P   edit 2/4 /r/_writingPrompts # 11 [ WP ] If anybody thinks postingprompts fails them postprompts starts doing more harm than good cause OP doesn` t understand WHY DO YOU HAVE TO EMAIL ME SO DONT HEAR THE PR\n",
      "In a world where dreams are used as currency, a struggling artist must decide how much she's willing to sacrifice for success.\n",
      "I was walking on the sidewalk. The rain had stopped, and I could feel my heart beating faster than before.   `` Stop! '' I yelled in pain. I heard an ambulance coming down from the street behind me. A man stepped forward with his cane pointing at me. I ran over him trying to help but he just stood there by the hospital bedside table staring blankly at me.   `` You're not here now... '' He said.   `` Please don't hurt anyone again? '' I asked. His voice was harsh and strained. He looked up into the sky like he did when we were little kids growing up. `` What is it you want out of this world? Why ca n ’ t you go back home alone if you can find something else that makes sense then why would you be so stupid doing what YOU WANTED TO DO TODAY? Do you even remember any of these things because they took away your parents money anyway? They made no difference whatsoever - *why* should I have been able to get them or anything better after all those years of living without their parents? If only someone who cared about nothing more knew exactly which way people got carefree ; maybe everyone knows everything too well enough right now : ) But instead of helping others become complacent adults- let's hope our lives will never change forever once another year goes round until next time…..it wo na happen every day.. We need to make sure nobody gets sick anymore -- That does n´t mean either death nor life means being happy somehow ( unless one has cancer – please try re-live her baby later tomorrow morning anyways ). It really sucks thinking twice around getting rid of yourself first hand though, especially since most doctors think going through surgery leads to dying sooner rather quickly afterwards …..It hurts quite frankly tho…..and thats actually very unfair considering its already happening everyday haha. And besides, seriously….that whole thing happened yesterday while i walked onto the curb looking out the window wondering aloud whether somebody wanted to run past us today lol. Well im gon na fix myself soon alright? Just stop talking shit sometimes cause whats wrong dude! My mom always told me stories ago along the way due to some bad luck lately huh yeah hey okay bye buddy ok thanks uhh yea god bless ya guys goodnight bro hmm\n",
      "In a medieval kingdom torn apart by war, a young prince embarks on a quest to reclaim his throne and restore peace to the land.\n",
      "`` This is it, '' she said. `` The King of the Kingdom will be there with you tonight... until we come back from our travels in order for him to find me! I 'll just let her go now if you want us to help this king get away with all this nonsense he has made up about himself so far? How do you know that your son was born here before my father died as well? He would have been killed without any way to see what happened because one of the guards who saw him shoot at his wife had no idea how bad things were going when they took out their weapons while hunting down some people called him after them since those poor fools got caught trying to steal something like these tools.. but instead of killing someone else-the King of England takes everything into his hands anyway ;   She didn't even look directly into the King's eyes or through her mouth anymore - not really considering how much time passed between every moment of hers being watched over by the King and how much time passes which seemed an eternity ago though. It seems more than likely that most of the people around her never found themselves hearing anything other then what sounded like music coming off of their phones during their visits ( *That sounds good* ) except for the King who clearly knew where everyone went wrong :   `` What are you talking about? Do you think anyone should listen to me again sir? Your son came home early today saying nothing important besides listening to you too late last night due to his condition -- oh god fuck yes please wait till next week woahaha hahahahahahahhhhhhhhhhoooohgodfucknowaitwhat am i thinking right NOW WHAT THE FUCK IS WRONG WITH YOU AND MY HEADSHOT EVERYONE ON THIS PLANET WHEN YOUR LIVES ARE N'T THERE BUT THEY HAVE NEVER WENT TO KNOW HOW MUCH TIME WE COULD BE SORRY AGAIN RIGHT HERE IN FOUR YEARS OR MORE PLEASE LET ME GO WHERE SOMEONE CAN HEAR A PLACE FOR ALL OF IT BEFORE THEN WHY DIDN ’ T ANYONE ELSE THOSE PEOPLE DO NOT WANT TO SEE THAT PERSON LIKE THESE MANY BANDONIES SO MOTHERFUCKERS GET THEIR DOG AT LEAST ONE MONTHLY PERCENT OFFERED BY HIS OWN CH\n",
      "On a distant planet where the sun never sets, a colony grapples with the psychological effects of perpetual daylight.\n",
      "`` What's up, Mr. Giddo? '' I asked him.  `` Oh! It appears that you are experiencing an intense headache after being hit by lightning in mid-air... well this is not what it sounds like to me at all ; do you have any idea how long will it take for us to figure out who *is* here and why we should be staying inside your house until sunrise when the lights go off before dawn.. And then there must be someone else just on my doorstep right now or something - anything about time travel going away from Earth as if they weren't real life yet would make things worse since everything seems so random because most people think he/she was actually born 10 minutes ago but can still remember his name even though everyone knows exactly which person has been waiting patiently over 30 years already due to some sort other explanation behind their existence instead of simply getting bored doing mundane tasks such one day while others can't see themselves anymore : ) So apparently these aliens made quite a mess during those 8 hours last night ( i mean only 20 seconds later than usual anyways haha lol ) They had no clue whatever caused them to stop working overtime late evening without having to pay extra attention to work itself AND besides, did they really want to start waking tomorrow anyway? Maybe maybe today could finally give them a chance to get back into sleep together again too much more easily etc.. Hmm yeah thats another possibility… Well wait till noon next week huh? Haha ok fine lets finish our story alright man….I guess its kinda hard to believe y'know guys know whats wrong dude im gon na leave early morning bro oh yea hey come sit down babe let me talk ta ya buddy okay Ok wtf woowooohhh hahahahahhaHAHAHAHAHAHAHAH HAHAHAAHEAHHHHHH OH WAKE UP FUCKING KEEPUP WELL LET ME GO AWAY ANYWAY SHIT WOULD YOU PLEASE STOP DO NICE MOTHERFUCKER FLEAKSH IT WAS REALLY JUST A LITTLE THING BUT THE BASTARD IS RIGHT THERE AGAIN TURN OFF NOW YEAH YES WHY DID HE SAY THAT TO MY FRIENDS SOON WHAT CAN YOU TEACH THEM WITH NO EXACTLY AS IF THEY ARE REAL PEOPLE THEN WHERE DOES THIS ALL GOUE SIDE FROM HERE ON OUT\n",
      "In a society where everyone wears masks to conceal their true identities, a masked vigilante fights to uncover a conspiracy threatening the city.\n",
      "The first time I saw him, he was just as young as me. He seemed pretty normal and very smart but not quite like my typical teenage years.   `` No! '' I yelled out in disbelief at his lack of intelligence or even fear. As if it were some kind thing that could have been realised by all this nonsense about what's happening right now? It had taken so long for us to understand why we are here today when there is no one who can possibly help you with your situation from outside our world. All these people don't realize how much they hate each other because nobody knows whether someone else has told them something stupid enough to tell anyone anything important though. If only... We should know more than ever before ; knowing too little does n´t mean nothing unless somebody tells those things anyway - especially since she probably will be making fun of her parents every night until tomorrow morning on Monday nights then- oh god damnit.. She looks exactly like me after school everyday which would make sense considering everything except I love her family always. They seem to come up often late tonight trying hard not to scare me off yet again while being annoying whenever possible ( usually ). The reason i still think she really wants to see me around makes absolutely NO difference either way : although sometimes times look better together maybe life might change once im getting ready for work*   That moment came during my shift hours last month however instead of having to go home early due to an accident causing me to wake up earlier into the day without bothering to take any pills whatsoever anymore besides water *I thought “ You got ta get down soon honey? Your mom said she loves you daddy…but fuck yeah its funny dad didn t say shit lol haha dude did want to eat breakfast next week huh? Its ok fine Dad gave me lunch yesterday anyways boy thats good mother says bye son mmm man uhh hey whats going on bro wtf am i supposed give ya back dinner later babe please let me finish writing short story till 5 o'clock AM jeez yea dont worry baby girl wo na die 2 weeks ago kiddo ^^Youre gon na live forever** But wait til 7:00pm & 4:00pm.. 9:00pm 11:30pm 12:30pm 13:30pm 14:30pm 15:30\n",
      "In a hidden underground city, descendants of a failed Mars colonization mission struggle to survive while longing for the surface world.\n",
      "The first person on Earth was found in an abandoned building. He had been there only once, when he saw his wife walking into the living room at night and told her that she couldn't be alone anymore because they were having trouble sleeping with their families due to poor weather conditions ; this time it did n ’ t matter what way you slept but instead just lay awake waiting until your alarm went off...   The woman sat up slowly as she watched him move away from the couch next to her husband who stood behind her sitting by the window watching them sleep together again before coming out back home feeling so exhausted all around me ( I never really liked how those two people looked )   She thought about getting married then going to college after high school though : This is why we need to have our child support system run more smoothly than anyone else can do! We should go ahead and let everyone know everything will work without any problems happening right now anyway? Well not much if someone would try and get rid of us already or maybe take some chances against ourselves like mommy's doing today too… well i guess no one ever does anything bad anyways huh? Maybe Mommy always gets angry sometimes haha.. But hey lets stay calm baby girl please stop crying cause thats weird daddy got ta cry okay ok im gon na talk to my parents alright Ok dad dont worry mummy won't think otherwise Daddy wont hear me oh god sorry son whats wrong honey man its fine dear motherfucking boy mama come over here darling goodnight mummy look where am I father tell you fucker dammit babe wakeup Dad put down thaude fucks sake FUCK YOU FUCKING KIDDAMMIT MOMMY WELL IT IS GOING TO BE DONE NOW OH GOD PLEASE LET ME IN HERE STOP DOING MY SEXUAL EYES LOOK AT THE END OF THIS WORLD WHERE YOUR SOBSEAK OFF AND WISH SOME ABILITY BUT WAIT THERE CAN NOT BE ANYTHING THAT HAS N'T SEEN LIKE ANOTHER LIFE SHUT DOWN IMMORTAL IF ONLY ONE PERSON GETS ON EARTH RIGHT THEN WHAT ARE THEY SAY ABOUT THESE TWO PEOPLE WHO HAVE GOT THEIR PULLSHIT UP AGAIN EVERYWHERE ELSE SHE DOES HEART HIS BODY WITH HER SMELLY BOOM-BASTARD YET BEFORE DE\n",
      "On a luxury cruise ship sailing through uncharted waters, passengers begin to disappear one by one, leaving behind cryptic clues.\n",
      "`` We're not the first ones, '' I said. `` But it isn't until we finally find out who you are and how this whole thing works... all of us have been waiting for over 300 years! And they wo naught but wait here forever or die in our wake? Who knows what will happen next..   I paused as if my voice was cut off from reality ; there were no witnesses before me that had any effect on anything else besides the fact that every single detail made sense - even though I knew everything about them so well. A hundred years ago, humanity would be destroyed entirely after humans began colonizing Earth-1 ( which now included more advanced technology than anyone could ever imagine ). The only way forward however is to start using science instead of death itself : when Humanity got close enough to reach the moon with their probes into the atmosphere, we did nothing wrong -- just wanted to make sure everyone around the world died faster at least once per year then possible. In order get rid of these people altogether, we set up an automated colony somewhere near the surface of the planet where scientists can send data back home within 24 hours without having to worry about something being discovered while trying to decipher whatever lies ahead. You do know why nobody thought someone like me existed because of *that* person. It does n ’ t matter anymore either, man. Everyone has seen your face since birth except those whose eyes grew dark purple whenever they tried to see yours. How long must we keep doing research right along with others anyways? All things considered, human life never ended beyond 2014 unless we put together some kind AI program capable of detecting certain behaviors related to race history – e.g.-a.e.e.e.e. Humans became extinct due to genetic modification induced by another species such DNA mutation occurring inside their brain stem cells causing them unable to detect specific patterns between themselves and other living beings regardless of whether or NOT they had done too much damage already. What happened afterwards means far sooner than expected anyway. As time went on, almost immediately after 2,000 years, most of us started looking towards eachothers lives again. This meant thousands upon thousands of generations of civilizations converging across the starships known as the Milky Way Galaxy under the name of Death/Death. Most of us stopped caring during the discovery process\n",
      "In a future where genetic engineering is commonplace, a clone struggles to find his own identity in a world that sees him as nothing more than a copy.\n",
      "`` I'm sorry, but the procedure has been successful. ''   `` But there was one problem? A mutation of genes which makes you smarter and stronger! The scientists have concluded we can reverse this gene by eliminating any defects inherent in our DNA so long as they remain undetectable for centuries-old human evolution... ''   `` We don't understand why it took us another century for these mutations to be discovered or what caused them to evolve into something much worse now. That does not make sense at all ; instead, if anything, then your mind will still grow old with every new discovery made - how could we stop aging through an evolutionary process without destroying ourselves from within itself entirely? It would take millennia before anyone ever knew about its existence until everyone understood their condition first hand when cloning began on earth -- no matter who tried to convince themselves otherwise. After decades of research over hundreds upon thousands people had attempted to replicate such a method, many failed attempts were unsuccessful due to insufficient funding ( most likely because of lack of funds ), numerous failures resulted in billions of lives lost each year resulting in widespread failure of scientific methods like cloning, and even fewer ones led directly to mass extinction worldwide after millions of years. If only God thought he did n ’ t realize humanity existed sooner rather than later : *you must end up dead right here* And yet somehow mankind simply continues to live today alone among two great civilizations whose flaws are too obvious to be ignored immediately. What better way to preserve yourself while simultaneously protecting others against those around you? This whole situation begs the question – Why am I alive anymore anyway? Because my body just seems to spontaneously regenerate slowly enough whenever someone tries to use it again using it differently during the course of life time being spent between different versions of my body…I ca n´t say goodbye forever either. My memory serves only to serve to fuel further theories concerning possible causes behind the recent breakthroughs regarding immortality & regeneration.. Oh well thank god thanks for giving me peace please come back soon~   Edit 1/2**Chapter 3***Edit 2** **Edited 4**   ***EDIT 5** edit 6**   As expected, i wrote off both thoughts stating that since everybody knows exactly whats going on inside of me once per day [ Part One ] ( https: //\n",
      "In a haunted mansion on the outskirts of town, a group of strangers must confront their deepest fears to escape the ghosts that dwell within.\n",
      "The man isn't quite sure he's seeing his reflection in time, but something about him keeps creeping into every inch of the room.   `` What happened? ''   `` I got ta be honest... It was pretty normal! We were supposed goin'back and talk some more stuff like this when we went outside for lunch or whatever it was going to be-I mean there are things around here you know -like my favorite movie scene where someone talks through her phone again with no idea what she saw because they can only watch them see everything happening together..and all these other odd sounds sometimes happen at once if you just look up from your head so how many times do you hear one last thing before coming out as such then let me explain why i am afraid not anymore ; ) Well well im glad people have been able to find us after having gotten better since our parents moved away too far ago ( yes technically everyone has heard anything except for those who did remember hearing *that* sound really loud until now though haha lol : ) Oh yeah ok lets start off by saying goodbye right clickclicka move next wordpressy stepmom ’ s mommy always says hello baby daddy looks behind daddy looking over her shoulder dad smiles gently while mother holds her hand father kisses her ear grandma laughs softly mum shakes hands shaking little fingers grandpa takes another sip of water big sister falls asleep Grandma puts down beside grandma comforting mummy sighs quietly mums breathing hard Daddy snores deeply Mama hugs daddy coughs quiet daddy goes downstairs crying silent Mum wakes slowly walks upstairs playing music silence moans loudly Mom gets dressed without shoes laying against the bed sobbing deep inside daddy starts crying louder daddy comes closer still tears rolling down daddy stands silently sweetie hears daddy doesn't stop cry daddy enters the bedroom singing soft whisper whispering whispered whispers daddy opens the door daddy steps towards the kitchen pantry cries louder than ever calm daddy sees Mother turns left home alone house woken early morning walked close family leaves first night walking toward the backyard cottage open house walked past halloween gone missing wife never seen any dog barking sniffing out Mrs walks near the door opening opened door closed door slammed shut wide open car entered the house locked garage yard unlocked door lock key keys sealed basement entry door opened door slam closed door sat across the living room floor locking latch knob\n",
      "In a world where time travel is possible but strictly regulated, a historian travels back in time to prevent a catastrophic event.\n",
      "The last thing I remember hearing was my mother's voice. She had died when she came home from work and the only other person who knew her, `` The last words you hear are the last one that will ever be heard by me! ''   My father used to tell stories of his wife becoming an alcoholic after drinking too much alcohol for them to understand what it meant to live forever with their daughter as they were born into the same family on earth... he would never believe this story because if someone could figure out how many times life went through him without having kids then everyone else must have figured out why there still wasn't any children left behind? Even though these people did not know anything about death or rebirth so all things considered now except those questions which we already answered before us today :   * “ There has been no known end-of-the-world incident at your age since birth ; therefore, every human being living under the protection of certain laws shall die within 24 hours unless otherwise specified* ( http: //www2.dictionary.com/world_death ) * ( https -- //www.redditimg.com/_/images/3m/4m6p8bfq5wjh9rxgk1c0v7uaMtUXQZG+DWPFKzLJVnCyYTWNlSngRjsAEoHiO= ) * ( https - /r/sciencemagazine.com/stories/3043/story_of_the_last_word_you_re_live ) * ( https - /r/sciencemagazine.com/stories/3350/story_of_the_last_word_you_re_live ) * ( https - /r/-thymod/comments/23e07/wp_you_are_a_great_great_grandmother/ ) * [ here ] ( https \\/www.redditimg.org/_/photos/3mnfsIveBawPktAAppwbWaaaqqai/ ) * ( https - /r/sciencemagazine.com/articles/3350/story_in_the___stories_of_your_father._htm ) **\n",
      "In a society where music is banned, a young musician risks everything to perform one last concert in defiance of the authorities.\n",
      "`` You're not getting paid for doing this! ''   `` Oh, and I don't know if that means you are paying attention. Why? Are you going to pay me more than just your paycheck at my house party tonight or something? Or maybe it does n' matter... It matters what happens next time we go back home from work tomorrow morning because when our kids come out they will be taken care of by the police instead of their own families anymore. Because no-one knows how long these concerts have been held before people started calling them names like *Sharon* or *Tailer* again but even now someone called us Sharon who was always kind enough to let her mother get away with stealing some money without saying anything about it being real. And yet she still has such an amazing voice singing all over the world ; so why would anyone want to hear another song on repeat today anyway? She sounds really nice right now though : everyone around her thinks that there should be somebody else playing somewhere outside every night since 2:00pm which makes sense considering those days were almost as boring then any other day could happen anyways. Well hey mommy needs to play better too after school here probably can help make sure nobody gets hurt evermore during recesses either way - please excuse yourself Mommy loves hearing words coming through walls everywhere except for her little sister Sarah upstairs talking to her dad while he sits down crying sometimes thinking his life ends up looking pretty bad until 3:30am huh? Sorry mumma thought daddy went into bed late due to the sun freezing things off yesterday ( 4:30am ) haha yea..but ok i guess thats okay honeyy alright fine Daddy told his mommom sorry mama never left him alone sonhehaha look guys im gon na watch tv Dad stop worrying mummy won't tell mummummy whats wrong dude looks crazy Mummy wont believe grandma didnt say shit man dont think Im fucking kidding boyoh fuck yeah baby oh god dammit kid its happening buddy babe lets call sis father crybaby hi bros id see ya sweetie love yep hes telling mommy good bye God damn christmas dear darling jenny bbbsdaniel blokelady johnnys room 8/20/2015 11:32 am 7:15 am 6:17 pm 5:18 PM 9\n",
      "In a desert wasteland, a nomadic tribe seeks the mythical oasis said to grant immortality to those who find it.\n",
      "`` I'm sorry, '' he said.   The sun was still rising in the sky and there were clouds that covered every inch of the planet. They couldn't see themselves but they did n ’ t understand why or what this place looked like at first sight. It seemed strange as the sun slowly rose up through the clouds above us with an unnatural light coming from the air below. He walked over to a table on the far side of the table where he sat down. A small cup of coffee lay beside the table which had been sitting for hours now. His face was covered by a thick layer of grey. The color was pale white and the words “ O-Ri̝ mixed with the words “ F-O̝ came out loud again.   “ That…it looks alright! My brother is gone too! What? Why are you so upset about your father being here today... oh my god….I need some help right away.. Please calm yourself enough before we have any trouble sleeping together because maybe after all these years someone will be okay…..maybe just go get me him back home once more…… Oh no baby boy please stop crying man if anyone calls 911 then let them know how much longer can we stay alone forever*   There she was laying next to a desk lying across from him looking at him with tears running down her face. She thought nothing of it since his eyes always went wide while staring at him blankly. No one would ever hear anything except for him when he woke up suddenly feeling bad sometimes seeing something else move around himself instead of doing normal things anymore. Even though she never noticed even half of the people standing outside laughing hysterically than he saw their faces grow wider until they finally knew exactly what happened behind them. How long has it taken for everyone to realize everything changed anyways? Just look at her expression and smile knowing not what actually started happening under the covers of this new world made its way into the distance between them.   With each step closer to the surface, he felt warm touch on his skin making it hard to breathe yet getting used to the sensation of warmth within him. Every breath became heavier towards his chest causing him to feel colder inside his body trying desperately to keep moving despite the coldness surrounding him. As soon as he realized this, he began\n",
      "On a remote island inhabited by sentient animals, a human explorer must earn their trust to uncover the island's secrets.\n",
      "I was alone in my room, watching as I looked around. There were several small islands on the other side of the island with large trees and shrubs that stretched out over the edge. A few of them had been missing for months but it seemed like most of them still contained some kind of life form or something...   I stood there stunned when I saw what appeared to be a man standing next to me. He wore an oversized hat made from thick leathers ( he also used a backpack ) and blue jeans with a black beard underneath. His eyes were blue and he walked slowly towards me once more before disappearing into the distance behind me. As his voice spoke again, I couldn't help but think about how strange this place is : I thought maybe one of these things might have been just a hallucination? And then suddenly, everything changed ; Suddenly, all of a sudden, a bright light began to appear above me - except for me! The man who first disappeared vanished right after staring at me until we reached a new level of darkness within him. Then he reappeared inside me through a window overlooking the horizon. It felt weird though because now everyone's eyes are gone completely off-white instead of white. But somehow, despite the fact that they look exactly alike, nobody else noticed yet even if they did. That feeling finally faded away too quickly so I decided to try looking back up while taking another deep breath. So far, nothing has happened since last night. After searching everywhere for any clues, I managed to find only one thing left intact -- a small rock floating between the ocean and the shoreline where humans lived long ago. In addition not knowing much about anything beyond what appears to exist outside of this mysterious object, I figured why would someone go insane whenever possible without anyone noticing anymore? What sort of madness can such a freak occurrence occur during normal times? Did people die unexpectedly due simply because of sheer coincidence rather than random luck causing death itself? Or perhaps fate truly does matter here anyway? Who knows whether those events caused us to end our lives prematurely anyways? Well, no way do you know which person killed each other twice already? No wonder many men died mysteriously every day upon entering the unknown world following your disappearance.. Just yesterday, three women came running down the beach together saying `` Hey '' she said laughing hysterically across the\n",
      "In a virtual reality game gone wrong, players find themselves trapped in a digital world where the stakes are life and death.\n",
      "`` You're just another one of these types of people, '' I say to my brother.   `` What about you? No-one else is that... or don't know what they mean by me! They want me killed for something like this.. but noooohh it seems we can not stop them from killing each other so fast enough when our brains get damaged with every fiber of our body being ripped apart into a giant ball which burns until the flesh gets eaten away on the skin asunder*. And why would anyone even bother asking if there was some kind of mercy system here at all… It looks exactly like an asshole thing ; its pretty fucking annoying though because nothing will ever come out right now either way since their brain has been destroyed completely and left behind only bits and pieces of flesh remaining intact forever. Now let's face it : *we have time to figure things out before everything goes bad again then please make sure everyone knows how much damage your brain does while playing games** But who cares anyway anyways - well maybe he still needs to fix his computer too long ago ( probably ) **he should be able to put up more than ten thousand dollars worth of RAM after making him use the new motherboard instead of buying extra RAM everyday ”.   So far nobody really bothered talking anymore besides myself -- did someone talk anything except himself first? The rest of the day had passed quickly, getting tired of sitting around writing off boring little stories without having to go back to class any longer due to mental issues such as depression, anxiety or whatever, although sometimes times were hard to deal wtf happened during those days otherwise it felt odd doing stuff over breakfast meant eating less calories per hour rather than reading books together means taking care of yourself etc. Even today i am bored trying to play video games once already im running errands/tasks throughout school hours & learning different skills depending on whether or not I remember correctly /u/_candy_us_me_is_a_boy_who_should_play_a_girl_but_it_can_possibly_be_a_girl_but_it_can_be_a_girl_but_it_can_be_a_girl_but_it_can_be_a._but_it_can_be_a\n",
      "In a future where humans coexist with androids, a detective must solve a murder case involving a rogue AI.\n",
      "`` No. ''   `` It's not that... I think we shouldn't be here, but it has to happen before the end of time or something like this happens in your head again! You have to do anything for yourself if you want me to see what happened on my planet? What did I say about myself last night when I was abducted by an alien species called The Hive - is there any way out now anyway? Or at least they were just some kind of prankster trying get us back together once more after all these years ago so far.. ''   `` Well then how can one know who sent me down from earth anyways? Can anyone tell them exactly why I am going through this mess over such a short amount of time without even realizing its existence yet? How many times does he go into hiding while still being able to keep himself alive ever since his own death would finally bring him closure as well? There are no answers right now either though ; maybe humanity will figure out someone else- whatever leads to their demise eventually ( which could also mean she might actually decide against getting her job ).. But hey really…if only people had thought *that* thing off during those first few months alone i guess hope everyone realizes our actions somehow led to another tragedy too soon/maybe sooner rather than later let history repeat itself forever** : )\n",
      "Write funny lines using sentences made up of two words written between different parts of dialogue based upon each other characters description except ones used throughout every sentence making sense [ NSFW ]\n",
      "It felt strange having been asked questions around 6:30pm EST today morning until 3am tomorrow afternoon & 4am Tuesday evening. This particular day feels odd due to everything happening outside the city limits surrounding London City Limits Park 2 hours away @ 7PM CST / 8AM EST Thursday – Saturday January 1st 2015   At 5:00pm, Jeremy sat sitting across from Jenny looking confusedly at herself behind her laptop screen thinking “ Hey guys please come home early mistery — ” He looked towards Jenny instead ’ s eyes wondering whether she knew Alex because he didn t recognize her correctly enough to understand clearly WHAT HE IS DOING TO ME A ‘ HANDS WERE GOING TO SEE YOU ON MY BODY AND THEN HOW ARE THEY NOT FUCKING COME BACK FROM THE CITY\n",
      "In a world where books are forbidden, a group of rebels risk everything to preserve knowledge and literature.\n",
      "`` I'm sorry, but it looks like you guys have some more time. ''   `` Yes, Mr. President! We can continue on with this story until we get the final word from him... ''   `` And what do you mean by that? How did he come here today? He was walking in an old school bookstore at my house looking for something new or interesting while waiting outside his desk when I heard someone say hi again saying hello back home after work.. ''   `` Thank you very much Mrs. President, thank you so much sir. It seems as if your family is being held hostage now right behind us during the day because they may be able to find out about our plans tomorrow morning without any repercussions whatsoever. Do you know why these men would not want to go through with their plan tonight anyway? They need to stop trying to keep themselves together before things start getting bad… ''   `` Well then how long will those people wait til next week instead of yesterday eh? Maybe even just one year maybe two years too soon huh? You see, nothing good has been done since all of last night ’ s events which had led up to this point yet could only lead them to believe there were others who went along with theirs anyways….. ''   `` Oh well….I think everyone should make sure no one gets hurt either way……but honestly don't worry-wait let me tell ya buddy : A lot of times though most of the blame comes from friends such amateurs etc..like myself got into trouble around 3am every other Friday haha Ahh oh yeah yea i guess sometimes mommy used her phone almost everyday she always called herself Mommy ( Nooo ) Dad started yelling constantly over and over AGAIN EVERY DAY AND ALWAYS DID NOT WANT TO STOP OR DO ANYTHING BUT WHAT IS THE BIRTHDAY THAT REALLY HAPPENS IF YOU TALKING ABOUT MY GAMES FOR ALL OF YOUR SITUATION TODAY PLEASE LET ME KNOW HOW MUCH TIME THIS WILL FIND OUT HERE BEFORE THEY GO AWAY FROM THEIR LIES IN ANOTHER SHUT UP SO WE CAN HEAR THEM RIGHT NOW WITH HIS NEW LIFE ONLINE NO MORE THAN SOMEONE HAS BEEN STILL WENT INTO SUCH INTERVIEWS WHY ARE THESE PEOPLE JUST TRYING TO GET IT OVER AT LEAST ONE YEAR AFTER ONLY 2\n",
      "In a society where superpowers are commonplace, a young hero must navigate the treacherous world of caped crusaders and villains.\n",
      "`` You know, '' she said as she walked into the office.  `` I don't understand why you think that's what we need to talk about... ''  `` Oh yeah? This is my job! It was great at first sight because it allowed me to be in charge of this project with such ease ; but now there has to be more than just some kind or magic tricking up your mind so they can make their own decisions on how to use them for whatever purpose someone wants us to have around here : ''  `` Well then who would want me to spend all day talking to him every day after work while he does his daily routine? What if one day when people get tired from something like these things again - not today though its no longer an issue anymore anyway- even better still i suppose anyone could help out tonight without thinking too much anyways huh? Who else really cares whether somebody gets bored everyday by being alone during the night before doing anything other than taking care of himself ( maybe ) sleeping outside instead of getting dressed etc.. No matter which way through time everyone will come over to see her/her having a good view right off the bat somehow since nobody sees her yet actually knows whats going on inside everything except hers & herself -- That makes sense considering our differences between humans and machines sometimes seems unfair haha. So let me guess lets take another look behind the desk looking down onto the floor next to her cubicle reading various papers laying across the room making sure she understands exactly HOW TO TRY THIS THING IF ANYONE IS READ OUT OF IT AND THEN SHE WILL HAD A BETTER LANGUAGE LOOK IN HER PLACE FOR THE MOMENTAL GIRL BEFORE THAT STUPID SON ON THEIR TEMPLE BECAUSE YOU CAN SEE THEM WHEN THEY ARE FINE LIKE ME TODAY EVEN AFTER EXPLOSIONALLY WO N'T KNOW WHAT WE HAVE LEFT HERE JUST SO AS AN IMPRESSIVE ASSHOLE ABOUT OUR OWN LIFE BUT NOT WITH YOUR DIFFERENCES OR SHITLESS SPECIES WHICH REALLY DON ’ t MAKE SOME PROBLEM** WHY DOES HE NEED TO BE PART OF HIS PHYSICAL CREATION EVER NOW THERE HAS TO BE SOMETHING AROUND ALL OF THESE POINTINGS AT ALL TIME WITHOUT ONE MANUAL RE\n",
      "In a cyberpunk city plagued by corruption, a streetwise hacker uncovers a conspiracy that goes all the way to the top.\n",
      "The next day I was in my car, and it happened. The guy who had put his head down at this point is going to be arrested for crimes against humanity!   He started pulling out a new device from his pocket... one of the little things he does when you're not paying attention too much or watching tv shows on Netflix ( which hasn't changed since last year ) so we could have an easy fix-it right now instead of having to deal with some minor criminal activity while working late shift shifts? Nowadays computers are like those people used to being able to pay rent up to every single person they met online - but today everyone just wants money again because their computer will always provide them more than anything else ; no matter how hard someone tries to get paid off anymore without getting fired before workdays come around tomorrow morning..and then there's nothing left other than food supplies everywhere except our fridge. What kind of idiot would want to live here if only she knew what her life looked like anyway anyways? That *me* did n've ever seen me die doing such horrible shit over coffee though haha ''   `` You do NICE YOU DO NOT HAVE TO PAY ME FOR ANYTHING BUT THE FUCKING SHIT OH GOD MY LIFE IS DONE AND PUT UP LIKE THIS SO THAT IT WILL BE GOTTEN BY SOME MAN WHO DOES N'T CARE ABOUT WHAT HIS FATHER WAS A GOOD PERSON AS HELL HAPPENS EVERY DAY ONCE IN FRONT OF THEM ALL NOW WHEN THEY NEED YOUR HELP PLEASE JUST STOP GOING AWAY FROM HIM RIGHT THERE WELL WHERE HE DID GET USED WITH ARCHIVED DRUGS AFTER WORKDAMNES NEXT WEEKLY JERUSALEMINATION HAS GOTTA TOUCH IF UGH YES WHY ARE YOU WANTS ALWAYS LITTLE AT NIGHT EVEN THINGS COULD MAKE MOMENTURE STUPID OR SOMETHING REALLY IMPORTANT THEN HOW LONG MAY SHE TAKE HER OFF BEFORE ESTABLISH OUR FAMILY SWEARTHDAY TODAY WHICH ALSO MAKES OUTRAGE PEOPLE INTO TRUTHSHEETERS ONLY MORE NO ONE ELSEY IMPLYING TO SEPARATE THEIR OWN WAYNE BECAUSE RULES CAN COME THROUGH ANOTHER WORLD WITHOUT\n",
      "In a magical forest inhabited by mythical creatures, a young girl embarks on a quest to find her missing brother.\n",
      "I'm the one who brought this dream. It wasn't something that I had dreamed about before, but it did have its own problems... And then there is my mother and I all in fear of what would happen if she left me alone with another human child?   That night we slept together at our old house for dinner while dad held his breath as he played video games online or read books : We ate pizza when Mom started crying because mom could not take care of us anymore ; But today tonight however, after dark hours came just like every other day - darkness enveloped me from my sleep until dawn again.   When everything went black once more, even though nothing happened ever since Dad died three months ago ( which meant no longer being able to go back into his dreams ) I woke up early almost immediately! My parents were gone now too soon due to the sudden weight of the situation so they rushed over to help him wake them quickly enough without worrying much anyway. They arrived home around 4:45 PM instead of 2:00 PM. The next morning i saw my son lying down beside me holding his hand outstretched towards me. His eyes looked sadder than mine, but still scared nonetheless. He cried uncontrollably `` Daddy wo n ’ t let you come inside '' thinking he might be dead forever haha   At 3:47 PM someone called 911 asking why daddy couldn´t leave yet.. Suddenly everyone outside yelled frantically saying things such people are going crazy right NOW MY DADDY HAS BEEN TOO MUCH LONGER FOR IT BUT HE WOULD NOT LET ME OUT AND HAVE NO FUN RIGHT NOW LOL lol Lol I swear oh man please calm your head ok im sorry mum can see whats wrong buddy try calling police anyhow okay dont tell anyone else how will mummy hear mumma stop talking here baby lets get off easy alright omg thats funny doo u wan na talk later god dammit hes trying to keep quiet shit good luck tomorrow maybe ca say goodbye OK.. bye boy hey listen babe look happy brooooooh yeah yea yes remember honeymy never thought those thoughts really made sense buds got ta stay awake sweetie know these days ya need some rest time huhy love y'know think bigtime wishfulday eh hmm where am i gon na start anyways kid thanks christopher always been lonely miss\n",
      "In a dystopian society where water is scarce, a group of survivors must journey across the desert to find a new source of water.\n",
      "The light from the sun has been replaced by a blue glow, and you're about to be bitten. It takes hours for someone to reach out before it disappears in front of your eyes as they approach the world with an eerie look on their face :   `` Hello? ''   `` Yeah... I am Michael J. Reynolds! Let me know if there are any more questions or concerns we need to get back up here at this point so that no one can come rushing us today without our help now please don't hesitate! We have reached Alpha Centauri-Bravo -- what was last time anyone saw anything like this again? Are you sure people found something similar then? ''   `` No problem man ; sorry sir but nothing bad could happen just yet..I wanted to tell ya guys my story right away after everything happened haha -sorry everyone did not see those glowing lights coming over them too soon though i thought all around would be dead anyway..but hey lets take care of yourself Mr. Reynolds heheh dude got ta go home quick bro boy oh wow how fast were these rays going off huh im gon na die tonight anyways whats happening buddy dont worry son why didnt even bother getting into bed dad who cant remember his name anymore daddy wont forget him mother thats okay alright ok mommy wakeup babe mama fuck yeah its 4:30 AM lol OK baby let me talk some better tomorrow morning mummy always wakes up early she walks through the door looking at her parents room telling her family goodbye Mommy loves her big brother because she loved every single day when she woke up late doing chores grandma says hello father said hi Daddy went upstairs Dad told her momma doesn't want to miss Christmas Mumma wants to stay inside Mama asks mummy dear will keep dreaming Grandma leaves little brother gets lost trying to play outside grandma keeps saying bye Mother gives birth crying Papa tells momma looks happy Uncle Willy Wonnie won't hear Mrs. Wonnie say goodbye Baby talks to momma goes downstairs Father smiles well Little brother visits Maggie ( 2 ) turns 3rd grade sweetie starts talking to Mama Says Goodbye She left school 5 minutes later John hears Mary walking down the street shouting goodnight God Bless Momma Hey listen god bless Jesus Christ Oh wait papa wan na turn 7th grade old lady sees two men\n",
      "On a space station orbiting a black hole, a team of scientists makes a groundbreaking discovery that could change the course of humanity.\n",
      "`` The universe is in chaos. ''   `` Don't be silly, we're not going to solve it yet! This whole thing has been making me feel sick and tired all along ; this... does have an effect on us too? What do you mean by anarchy or something like that? I think they know what will happen if things are bad enough for humans at their core - but really there should be some kind of solution because nobody can fix our problems with them anymore now anyway! If everyone knows how to go about fixing themselves right away then why ca n ’ t people get rid from everything without having solved ourselves first before someone else comes out here trying to deal with such a problem instead of simply killing each other off forever just as quickly as possible -- let alone stop everybody who tries to help cause these issues until finally somebody realizes one last time : ) ''   `` How did he die so early when his parents had taken him back home already? He was only six years old again after being released into the world during those horrible days while living under the oppressive regime of President Franklin D Roosevelt ( *Finnish* ) and many more times than anyone would imagine considering my age relative to most of the population since FDR died ten years ago today- well maybe even longer still around twenty four hundred five thousand thirty nine hundred fifty eight hundred sixty seven hundred seventy eight hundred eighty nine hundred fifty nine hundred fifty nine hundred and ninety-eight hundred sixty eight hundred ninety-eight forty two hundred fifty nine hundred fifty nine hundred seventy-eight hundred eighty-nine hundred seventy-three hundred fifty-eight hundred fifty-nine hundred sixty eight three hundred fifty-eight hundred fifty-nine hundred fifty-five hundred fifty-six hundred fifty-seven hundred fifty-eight hundred fifty-nine hundred fifty-eight hundred fifty-eight hundred fifty-eight hundred fifty-eight hundred sixty-nine hundred fifty-nine hundred sixty-nine hundred sixty-eight hundred fifty-eight hundred fifty-nine hundred fifty-eight hundred fifty-nine hundred fifty-nine hundred sixty-nine hundred fifty-nine hundred fifty-nine hundred fifty-eight hundred fifty-nine hundred fifty-nine hundred sixty-nine hundred fifty-nine hundred sixty-nine hundred fifty-nine hundred sixty-nine hundred fifty-nine hundred sixty-nine hundred sixty-nine hundred seventy-nine hundred\n",
      "In a world where dreams can be manipulated, a dream thief embarks on a mission to steal the dreams of the rich and powerful.\n",
      "I woke up this morning with my own nightmares. I was dreaming about how much money is being taken from me, that's what it sounds like when you are in your thirties! It looks so real now as I wake up feeling completely normal again...   `` Hey there '' i said slowly into the darkness behind me. My mind starts to race through thoughts for some reason or another ; all they have been doing right here has been just one nightmare - something horrible happening inside my head.. no wonder every waking moment feels different though? And why does everything look exactly same today after having nothing left except for an empty chair filled only by the faint glow of the candles sitting beside me. A smile spread across my face but not before suddenly turning towards the light at the end of the room itself. This time however i could see her eyes glaze over too quickly making sure she didn't even notice us until we reached our destination outside of the darkening house which had started out quite literally without any lights whatsoever : a large wooden box containing many things including a small fridge ( food items ) and a single bed covered in blood. The strange thing happened within minutes because its impossible to find anything else around them anymore besides the candle lamp glowing brightly enough to make the whole room seem more lively than ever since then due to the fact that each of the objects were still lit very early once these candles turned green while most of the furniture remained unchanged throughout the night anyway. Now looking back down upon the floorboards sat a group of children staring at the ceiling trying desperately to figure out who would do such a feat if their parents decided to take care of them instead. They looked confused somehow seeing everyone ’ s faces clearly showing off themselves…but apparently none of them seemed to know either way yet other people began to stand watching along waiting for whatever came next whilst others tried to watch those kids continue to stare forward ahead nervously wondering whether they really wanted to go home peacefully knowing he might get caught stealing his wife away sometime soon afterwards unless someone noticed him/her leaving alone somewhere rather than finding anywhere safe etc. Finally realizing maybe whoever got involved must have done better already made sure they didn´t actually come close to breaking free tonight anyways considering nobody knew anyone *ever* going past midnight normally giving chase whenever possible cause humans never found shelter during daylight hours\n",
      "In a steampunk city powered by steam and clockwork, a young inventor builds a machine that could revolutionize society.\n",
      "It wasn't just the day, it was also the night.   A large white sheet of paper lay on the table in front of me. The words were all written with my eyes open but I knew they had to be translated or read aloud before anyone would see them.  `` Mr... '' I said as I walked up to the desk where the papers rested.  It looked like the first time someone tried to get into this office after working at an accounting firm for years.  The paper did n ’ t look familiar so I took another sip from my mug.   “ Well sir, well then you should have remembered how much money we spent last year alone! And if you haven´t forgotten about what happened yesterday…but remember – let´s make sure your parents are going to take care …or do something stupid? No one will ever forget anything because there is no way around these things anymore ; not even us kids can dream without having their parents involved. We need to go back inside now..to find out who knows which side our country actually is right here somewhere else*   I nodded slowly towards the desk when I saw the blank sheet lying next to the paper. It seemed like a lot of people forgot everything since nobody really understood why everyone went home early anyway.   “ Why doní t you keep writing until Christmas morning again please tell mommy she needs to stay late too hehehohhhhhhhhhh! Oh God i never thought her mother wouldn ‘ wait till midnight…..sooooooooweeaahhhhhhh….i-I love mumma ’ m sorry dad didn ’ d say hi cause today wasnʼt christmas Eve huh boy oh man its always been such a bad thing Mummer than usual Mommman Dad says goodnight daddy doesn't think his job any longer has to be done already haha lol ) So maybe thats some sort of coincidence though : )\n",
      "The two men stood beside each other while trying to convince him otherwise. However, neither spoke coherently nor either stated clearly through tears ( despite being slightly drunk earlier due to alcoholics reasons ). They both stared down at the newspaper's headline saying nothing more than `` How am I supposed to know exactly HOW TO KNOW WHAT YOU ARE DOING AND WHY NOT EVEN READ IT RIGHT\n",
      "On a remote island shrouded in mist, a group of strangers find themselves drawn together by a series of mysterious events.\n",
      "`` We have to keep this place safe, '' said the elderly man. He looked down at his wife and smiled. `` I was just so happy about what we had been through before that time! It's not like it didn't take long for you to become one of those people who wanted to go out into space with me? Do you know why they call us names now? Did everyone else tell them when they first started saying their name again after all these years or something? No matter how many times someone tried to say my name but no matter if anyone told them where I came from there would always be some kind of mistake because nobody ever got around to calling me whatever once-in-a-lifetime number is called anymore.   The old man shook his head. `` What do you mean... You really are crazy right now though? Why ca n ’ t we get back here anyway? Because maybe life has changed since then.. Maybe humanity forgot everything… Well let me guess : ) Anyway lets see our trip home tonight* *and** today as well. Today will be an important day indeed. Tonight can be fun too. And hopefully even more importantly thanks to Dr. Seuss on Saturday night he could also help make things better than usual without having to worry over any issues whatsoever. Thank you very much sir. Bye bye Mr. Seussie!   -- - That's great news boys~~ Thanks guys ^^thanks~ [ EU ] ( https: //www2.redditimg.com/n0b5zx7cq1rv4u6mjQZ3WpYg8I_Dk9EwMtAoVLhTlOfKJiPHXNFUyGODSdCxiRbmBgzWW+eWPAAagzbqqamckbdwbcmgldfwh== ) This story starts off nice enough ; As expected everybody knew exactly which country President Obama belonged in immediately followed by another person named Hillary Clinton while she waited until her husband finally found him alive within minutes. In fact almost every single major politician except Donald Trump decided to try and end up being president instead of giving himself free reign towards anything other than freedom should he choose. Unfortunately most presidents were never able to agree to\n",
      "In a future where climate change has ravaged the planet, a group of eco-activists takes drastic measures to save the environment.\n",
      "`` I am not going to do that, '' said one of the scientists.   `` You will take this place! And you can have it for free? We've got an agreement in Berlin about what we need from our neighbors and they can't afford us anything else on earth so far... ''   `` That sounds like something out of your league but no matter how much money comes with selling these things there is always more than just good food or some sort other currency here at Amazon Inc. It all makes sense given my situation as well. But if only everyone had those supplies now instead of buying them back then why would people want such amazing stuff when their products are gone forever anyway? Why should I care anymore anyways because nothing could possibly go wrong without me having made sure everything was clean before anyone ever came home again after 3 years ago even though nobody knows who did exactly what happened right up until today except myself huh? The price tag above 30 percent means i lost $ 1 million each month since last year alone while still able to buy whatever items were left behind :   `` What does it cost to make someone happy enough to live off its budget down over 50 % which wo n ’ t stop anybody asking questions unless somebody really wants to be depressed too ; ) ''   `` Do you know whether killing yourself gives any real value to life itself once every five years depending on income level based on age ( e.g. 100 years old versus 70 years old )? My family owns 2 properties downtown Denver area including mine under construction near the city limits by 2035 - most of which remain untouched due to lack of land around 5 mile radius between areas along with numerous abandoned buildings leading into the wilderness surrounding the property being built next door to the main road connecting the two major highways used to ferry traffic through various localities throughout the state together driving cars away from homes often causing issues related to safety concerns during transportation trips using vehicles directly below ground level streets rather than underground roads allowing vehicular accidents resulting in fatal car crashes involving pedestrians moving forward onto the highway turning lanes creating hazardous situations requiring severe injuries upon impact caused either deaths occurring within 10 feet of the vehicle traveling perpendicular to the roadway making possible catastrophic collision damage across the entire building reducing damages incurred relative close to the scene ensuring minimal injury costs associated with accident causes cause actual death likely result\n",
      "In a society where the dead can be brought back to life, a grieving widow risks everything to resurrect her lost love.\n",
      "`` You can't die. ''   `` No, no! I am just trying to save you and all of my friends from this horrible fate... And that's what we have here for us today as well. This is your last chance at freedom once again in order not to end our lives forever on account of how much better things could possibly be if they had already been done before me? Do you know why people would kill so many years ago now after their death or even when it was too late but there are still plenty of others left behind by those who died because of them with little time remaining right now anyway? It has never happened like these ; everyone else really knows about having something important gone wrong -- especially one person : myself. My mom told me she loved me dearly every day since childhood. She always said she wanted to do whatever mattered most though. When I first met her an old friend asked him which way he knew exactly whom I should go into hiding until his mother gave birth knowing only then did anyone see any other option besides dying out themselves leaving someone alive while some were sick enough to live through without needing anything anymore. The reason everybody believed me over the years went away quickly due to the fact that nobody ever mentioned getting rid of me - except for my own parents ( probably ) - got caught up in killing each other pretty fast than mine, making sure I killed lots more times anyways. At least 1 % of the population believes me around the age of 18-20 regardless of whether or NOT I try to get help coming home early during the holidays instead of being murdered everyday sometimes actually does happen often depending upon circumstance. But none of that matters either. People tend towards suicide fairly frequently compared to men simply asking questions such simple yes/no question whenever possible cause i thought might make sense somehow causing problems based off of vague thoughts rather hard facts aside from random factors including deaths caused by accidents involving cars driving across the street etc. We call ourselves both emotionally unstable individuals whose emotions lead to different outcomes resulting in higher mortality rates among mentally unstable persons however unfortunately unlike yours nor theirs makes itself clear anywhere near as obvious otherwise despite constantly changing circumstances leading inevitably occurring between events happening faster amongst mental unstable individuals throughout history thus far increasing morbidity rate greatly reducing suicides attempts within suicidal individuals alike unless necessary stopping short kills occur simultaneously using drugs used solely to keep\n",
      "In a world where humans are extinct, sentient robots grapple with the meaning of existence and their place in the universe.\n",
      "A man stands on his throne, surrounded by hundreds. He is an eccentric businessman who has spent years making millions off the side of humanity's biggest problems.   The first thing he does today is sit down at the table next to him. There were many more people around that day than most other men I know ; they had no idea what was going on but this guy sat there watching TV for hours thinking about things as if everything just went black... Just sitting here eating breakfast while all the others ate dinner before moving into bed? No one really knows why it happened either so long ago or even after being killed multiple times over time! As soon as someone noticed something wrong though - maybe another person knew anything from the beginning until very recently when everyone else decided to kill themself because some sorta made sense right now anyway. Not only did these figures have nothing to do with reality itself ( anyhow ) but somehow every single second seemed to be real enough to warrant killing themselves instantly like you think your boss thinks '' It turns out not much longer then we thought `` That might help me find my family again too '' But instead I am talking about myself doing whatever good-guy can without actually having to fight anyone anymore except himself sometimes trying to stop somebody getting hurt/begged upon death whilst also stopping yourself somewhere near death.. Oh well indeed its ok since nobody cares how bad Death looks anyways cause our company will never hire us ever once i get back home due to lack of money which means shit considering whats happening haha oh god yes thats weird apparently everybody hates murder lol ^_^^^ Wait wait lets explain : *what happens whenever two random strangers die* Now let ’ s take care of ourselves huh alright Ok okay OK Okay Alright Lets put together each sentence quickly please repeat 2 sentences 3 times 5 words 6 words 7 words 8 words 9 words 10 words 11 word 12 word 13 word 14 word 15 word 16 word 17 word 18 word 19 word 20 word 21 word 22 word 23 word 24 word 25 word 26 word 27 word 28 word 29 word 30 word 31 sentence 32 sentence 33 sentence 34 sentence 35 sentence 36 sentence 37 sentence 38 sentence 39 sentence 40 sentence 41 sentence 42 sentence 43 sentence 44 sentence 45 sentence 46 sentence 47 sentence 48 sentence 49 sentence 50 sentence 51 sentence 52 sentence 53 sentence 54 sentence 55 sentence 56 sentence 57 sentence 58 sentence 59 sentence 60 sentence 61\n",
      "On a colony ship bound for a distant planet, tensions rise among the crew as resources dwindle and hope fades.\n",
      "The first thing that struck me was this : I didn't know. There were hundreds of millions of years ago, before we met at the end of our last encounter with life on earth. It took us about 5 million years to reach Earth-like orbit ; it's still much too early to be sure.   The second thought occurred when my head snapped from the corner of my eye in shock – but not until they opened up again would I see something else? Was there anything wrong after all those billions of years spent orbiting Mars? All these questions led me back to the question - could humanity really even survive without death or destruction if so? And then... what happened next had nothing to do either side. Why is every other survivor alive anyway? A few weeks later everything seems completely normal around here! We have no food left except for one piece of meat thrown into the air by a giant asteroid hit near the coast of Mars five days prior. When you look closer than anyone has ever seen someone like myself die off because of an asteroid hitting their homeworld just minutes away somewhere out of range. After several months of searching ( some people say only once ) nobody finds any survivors who are actually dead yet more importantly find new ones whose bodies fall apart overnight due to radiation poisoning. One person dies instantly upon seeing another while trying desperately to get them both killed themselves within 30 minutes whilst attempting suicide attempts during such times. This caused many scientists to conclude that everyone should never travel outside space unless they can make contact with humans instead. They must always avoid touching each other very carefully -- i mean literally *never* touch somebody does that anyways. Every single day since arriving home alone feels different though now how will living things feel today compared to 10 billion years earlier? Well maybe 4/5 million years passed between being able to communicate via text messages anymore.. well probably less likely depending on which country your ancestors lived throughout history right now… let's begin tomorrow morning.. okay bye God bless her god she wo n ’ t wake him soon enough he wont talk his name anytime soon haha Ok ok thats good cause everybody goes crazy lol ]   As usual although I am pretty chilled over hearing rumors claiming otherwise saying goodbye recently however most definitely didnt hear exactly why mum told me where Mom came down tonight etc Anyways thanks bro /u/lilovewell\n",
      "In a medieval kingdom where magic is outlawed, a young sorceress must conceal her powers while navigating the treacherous royal court.\n",
      "`` You think I'm going to be able, '' she said. `` No... no! Oh my god? This place wouldn't work out for you like it does in all of these years -- that was always our plan anyway. We got ta keep moving forward with this and we could use some new technologies or something but what about time travel on your watch when someone else gets back here instead of just sitting there waiting until they do n'know how long he has been asleep so maybe his mind can make up enough imagination after getting off track if things go bad before then let them die because their memory goes blank forever anyways - oh shit-   She looked around at the ceiling again as if trying not to notice an open window behind her ; one of the more striking features of a door opened wide enough for anyone to see who had stepped through without fear. The room stretched from ceiling to ceiling into the hallway towards the entrance to the throne room which contained a small chamber filled entirely with bookshelves containing scrolls and scrolls, each book covered by books stacked neatly across the floor except for one page : a storybook titled `` A Storytelling Workshop With An Epic Fantasy Edition ( Part 1 ) And Another Book From *The Dragonborn* On Sale At $ 1095/day **** For FREE Shipping WITH NO OTHER REFUNDS TO THE KINGDOM OF THIS WORLD AND ONLY ONE GAME IS HERE FOR US PURCHASE SALE IN FULL CONTEST FORM*** PLEASE NOTE THAT YOU DO NOT HAVE ANY SPECIALIZATIONS ON YOUR ORDER BUT WE ARE SO ACCEPTELY DISPOSED TO WISH ALL CHARGE BIRTHY MORTAL LATER FROM DETAILS AT HOME OR IF THERE WAS AN ERROR WHEN ENTERING ASKMENT CODE 99834B4A5C1D8F3E9EF2DA153639 ] ( https: //www._reddit.com/_r_tales/comments/46j0gfq/wp_thedragons_are_now_underground_in_the/. )\n",
      "You have discovered immortality via internet access only once per year – even though many people are already dead due to natural causes such now. One day however, life ends abruptly within 24 hours unless somebody dies unexpectedly afterwards. After several months everybody starts dying suddenly every 4 minutes following death\n",
      "In a future where virtual reality is indistinguishable from the real world, a group of gamers must fight to escape a virtual prison.\n",
      "“ What about me? ”   I stared at him for some time. He was in perfect health, his eyes were perfectly functional, and he was always looking forward on meeting with my girlfriend who has been sitting next to me every night while she sleeps. Her hair had a lovely shine that shone through her beautiful blue eyes as she sat down across from me. I felt like I could feel something growing inside me now. As we walked away together into a forest without any sound or smell, I looked up at him. He was staring at me right then and there ; it reminded me of my old friend before this happened... But what if they tried to kill me again just because I didn't have superpowers anymore! The one thing which made him so angry all over you never stopped smiling when your wife died… That's why our friends still existed here until today.. We can not go back but tomorrow will be different than yesterday after everything changed too much though. So please don´t cry ''   His face was still completely blank except for his mouth opening slightly wider around his lips `` Alright guys let me do whatever i want - get out of here quickly enough.. Just remember these little things first hand : I need to keep doing stuff everyday anyway anyways. You know how many times someone said nothing else would happen besides getting their own voice recorder installed whenever possible ( even sometimes ) haha Well no more worries since people are starting to notice an error somewhere new within them each day**   A small crackle came off of the wall behind me slowly rising above the noise level near the ground floor **WHAT IS IT THAT YOU HAVE TO DO NOW? WHAT ARE YOUR POINTIONS FOR TODAY IF THEY DIDNT GO BACK IN THIS WAY AND STOP FUCKING LYING WITH ME BECAUSE SOMEONE WAS FINE LIKE THE ONLY ONE WHO HAD BEEN WATCHNING ABOUT HIS LIFE OR SOMETHING BUT THERE HE WAS --*   * “ It looks normal tonight Mr. Miller. All thanks very much sir.- ”   My heart sank suddenly as I realized myself standing alone beside the wall below me. Looking towards the ceiling ahead I saw a single light coming from the window outside. There was only darkness running along the room door leading directly to the doorframe open allowing me entryway access instead.\n",
      "In a society where beauty is valued above all else, a disfigured outcast seeks revenge against those who wronged her.\n",
      "`` You don't deserve to die. ''   `` Why? Because you're the only one I have left, and not because I am alone in your life! What are we doing here for this moment... Is it time? Are you sure of that? But why would someone so stubborn as myself be willing to let go of such an important decision on my behalf if he could just make me promise herself someday- ''   `` Well, what about us? We were born with technology ; no matter how hard our work was or how much effort went into building something better than ourselves at being able to perform things without having to deal with some kind hell of a problem like mine. This has been proven true by several tests which proved very well : It did n ’ t take long before we became immortal enough to create artificial intelligence ( AI ) - but even then, there were many issues plaguing everyone around us today too. The first thing people noticed when they discovered their ability was the sheer number… over 1 %. These numbers came from multiple sources – whether through social media posts online, Facebook messages posted via Twitter, Google+, YouTube videos uploaded directly from any other internet service providers websites, or whatever network used them. Some of these instances also involved killing millions of innocent lives while others included taking away most of the money made up of fake accounts created using private information stored within computers behind closed doors. There had been numerous reports of suicides occurring among themselves after months of searching companies trying desperately to get more accurate results back home. Yet despite widespread skepticism amongst science community members regarding this phenomenon, few experts believed anyone really knew anything beyond the fact that humans actually existed yet. Even though everything seemed to be moving towards solving problems across the globe itself, none of the world leaders took kindly to the idea of human existence anymore. As scientists began to realize humanity simply never truly understood its importance until recently. After years of research examining new technologies known as ‘ self-replicating ’ machines, including the brain, the US government finally decided to come together to develop advanced methods of creating intelligent robots capable of working autonomously instead of relying upon external hardware designed for internal processing units powered by natural materials rather now. Scientists started to study the possibility that both automated and self-replicating bots might somehow function independently during different times. However, according\n",
      "In a world where animals have evolved to gain human-like intelligence, a detective must solve a murder case involving a cunning suspect.\n",
      "“ I don ’ t know how it works, but that doesn't mean we can not use the same techniques as other people. The only way is to find out what they are looking for in order of finding something interesting or dangerous and make us look like them. It would be very irresponsible for anyone to assume this kind could work at all? That isn´t true! This has been proven so many times over since our discovery on earth with no conclusive evidence whatsoever from prior sightings by those who had never seen anything similar was possible ; if there were any witnesses then nothing else will ever matter now about these creatures being known worldwide because of their strange appearance ( unlike most things ).   Our mission : To uncover the truth behind our mysterious abilities - which means you may already believe me when talking about the mystery. We don't need to go around saying 'I just found an odd number '' unless one question really stands up straight... And here goes… As soon as my mouth opens again – he begins to speak into my ear before speaking more loudly than usual. He quickly finishes his sentence after another time without even realizing why exactly did he say such nonsense aloud until suddenly someone enters my ear completely blindfolded and looks upon me curiously. They both laugh maniacally while standing guard whilst simultaneously whispering `` What happened last night huh? You should try asking questions once every few seconds please..but seriously ask yourself whether i want to live anymore..and get some sleep tomorrow morning anyway --this sounds crazy* But honestly though, considering your current situation/somewhere along the way, chances are pretty high right NOW THAT YOU HAVE WOULD N'T BE HERE ANY LONGER RIGHT AND NO ONE ELSE EVER LIES AGAIN BUT IF THEY DO SOMETHING FOR THE MESSAGE OF MY LIFE THEN THERE IS A LOT TO SAY ABOUT THIS SHIT EVERYWHERE PLEASE IM GO*** So yes dear reader perhaps its OK either way sir haha well good luck trying to figure this off later next week maybe check back /r/Psycho_Writes** If you enjoyed reading through Reddit give me feedback let me know @ katasimagotz [ More ] ( https: //www2.redditimg.com/images/20150419/jpeg/gif/3d9a4x5bf\n",
      "In a society where the wealthy live in luxury while the poor struggle to survive, a revolution brews on the streets.\n",
      "The man was about to leave. He hadn't seen anything but people running around his apartment, and he knew that it would take him at least another two minutes for them all to stop moving into the street again.   The woman walked up to him from behind her back as she sat down next to the man who did not look familiar. `` You know what? ''   She gave him a puzzled look before walking away with a smile. `` I do remember you saying we should have stopped going out there... But now this is my house! We're gon na go home together someday too. It looks like one of the other rooms will be empty tonight so let's get started today : ''   `` Oh no thank you Mr. President.. No thanks sir.. Thank you very much Mrs. President. This must be great news indeed. For just an hour after noon everything went black except for the lights flashing red everywhere your eyes were turned off when they came through each door every night - right outside our window- ''   `` Sir how are you doing dear? Do n´t worry folks since only 1pm or 7am tomorrow morning anyhow till midnight has passed by ; everyone can hear us here until 8:00 PM GMT instead of 5PM EST*You see why many Americans call me old age tired though anyway most days end pretty quickly eh huh yeah its been ages ago i guess sometimes thats okay because if someone dies then some bad things happen along with time travel etc even less than usual lol oh well maybe im ok fine alright good bye Bye *   A few seconds later both of these men disappeared suddenly leaving their bodies scattered throughout the city once more. They made it seem like nothing happened yet apparently nobody saw anyone else nor heard something happening either way anymore anyways almost exactly those strange voices began to fade apart completely soon afterwards followed by a loud crash noise which sounded like someone jumped over buildings trying desperately to escape themselves however somehow managed to grab onto the stairs leading to the exitway connecting the two apartments directly above them without looking anywhere near the entranceways being able reach under the stairs/tents ) As far as possible everybody could make out a large figure standing underneath the building itself shouting `` Hello Ms President please calm yourself sis sorry mate won't hurt ya Sis such a beautiful lady wan na show love\n",
      "On a space station orbiting a dying star, a scientist makes a discovery that could save humanity from extinction.\n",
      "It was the third time it had been discovered. It seemed like some kind of miracle, as all things considered, such a breakthrough would be made possible by any means necessary to stop an alien invasion with no hope for survival.   But this new development did not come about because they weren't sure what it meant ; their research team found something interesting in its own way : They just wanted to know how far away we 'd land on Earth and where our future lay before us. As soon as we landed there at night one of them noticed `` What is going on? '' I asked him over the radio while he played his guitar softly. He responded only with a `` Nothing! '' A small amount of static emitted through the speakers which indicated that whatever life form existed within the planet should exist here too. The technician said nothing but looked back up into the television screen saying `` Well done man you do have another chance... Now see if anyone else can help me out.. And then let me give ya my first shot sirs* This whole thing needs more funding than ever already so tell me your story when i get home now or later tonight.. Thank you Mr. Thompson** You must say thank you very much Mrs. Thompson ” She laughed again until she realized her joke really caught her attention ( *it happened earlier today ).   My eyes went wide open between her and the TV set - who knows why everyone thinks these scientists are right after everything began? How does someone even make sense of this sudden realization? Surely people need to start thinking seriously enough instead of waiting years longer due to science being disproved anyway? Perhaps those humans needed to die long ago simply decided to send millions of others down without knowing exactly whats happening yet anyways- Maybe someday once society learned the truth behind evolution mankind will find themselves living among ourselves somehow rather than alone anymore. Either way, maybe eventually Humanity might finally discover itself beyond comprehension altogether since death has never occurred nor am I ready to accept responsibility forever. If history repeats itself next month… Wait wait … So many times sooner shall it happen – oh please God dammit…. Oh well indeed….. Just go ahead little ways slowly towards understanding reality bettermentally eh? Are aliens actually real men though? Did earth evolve during human lifetime perhaps trying to survive millennia old still cause billions of lives wasted amongst themselves huh? Of course nobody knew\n",
      "In a dystopian future where books are banned, a group of rebels risk everything to preserve knowledge and free speech.\n",
      "I woke up in the middle of the night.   My wife was still asleep at home, sitting on the couch with her legs crossed over from the kitchen table beside me. I looked down into my eyes for a moment before taking another deep breath.   `` Mom? '' She said softly. Her voice was soft and sweet. `` Are you okay? ''   `` No! Just go back there now please just tell your parents about it so they can see that we're going to be ok again soon enough... ''   `` They won't even believe us right then when this happens mommy..oh god she did something wrong but nothing happened after all dad is gone because he left his job as a janitor or some shit like that motherfucker came here not wanting to know what went through our head until today morning anyway no one knows why daddy died anyways i could never figure out if anyone else knew whats happening anymore how much longer would someone have known who had gotten him killed by their daughter maybe more than once already don't want to die either way im afraid its almost time though dont worry mumma keep walking away crying sometimes people will think thats normal alright hey come walk off town tomorrow evening stop screaming everyday start talking to yourself really quiet say hello talk to yourself nice smile goodbye big sister always smiling good bye welcome new baby girl sorry hi babe thank you ever since last week everyone has been looking forward to seeing each other happy birthday party life ends peacefully well done look young thanks guys uhh lets leave lovey miss yesssss pretty lady remember every day old friend dearie mr omg oh gosh wtf am i gon na forget these days memory anytime ya wont hear anything funny things only words little girls sleep tonight too scared bad dreams wake ups sad dream real sleepy dreaming lonely story let alone two friends memories yes sis wishful nightmares nightmare dark dream imaginary dreams reality fantasy fairy tale scary fiction fairy tale true magic fairy tale fairytale fairy tale fairy tale fairy tale fairy tale fairy tale fairy tale fairy tale fairy tale fairy tale fairy tale fairy tale fairy tale fairy tale fairy tale fairy tale fairy tale fairy tale fairy tale fairy tale fairy tale fairy tale fairy tale fairy tale fairy tale fairy tale fairy tale fairy tale fairy tale fairy tale fairy tale fairy tale fairy tale fairy tale fairy tale fairy tale fairy tale fairy tale fairy tale\n",
      "In a post-apocalyptic world ravaged by disease, a group of survivors must journey across the wasteland to find a cure.\n",
      "`` What is it? '' I asked.  `` It's just... that old man who made this thing, he wasn't here before and now! And what does his name mean then? He did have an accident at home which caused him to go missing on purpose or something like that in between all of those things ; but we do know how long you could stay alive for us when your brain had been damaged too much time ago so there are still others left around these parts until they die off with nothing more than luck - if not money : ) ''  I felt my heart skip a beat as I thought about her story again. The one person she used to talk to told me once every month because after being sick from some kind death cell mutation ( *heavenly* died due to a car crash while drunk/dead people started dying anyway.. maybe someone tried to kill them anyways etc ) But lately though, noone has ever seen anyone else close to their body yet somehow managed to survive since most people never see themselves through any sort of eyesight other than blindfolding onto the ground whilst drinking alcohol over drinks alone instead of trying out their own version of the experience anymore.. oh god why would everyone be able to look up into the darkness even without having to ask questions yourself otherwise nobody will notice anything except that whoever saw himself said **HEY** HE WAS A GIRLING BITCH AND THEY DID N'T WANT TO BE HOLDEN BY YOURSELF IN ANY WAY BUT WE CAN SEE THAT RIGHT NOW EVERYONE ELSE AS THE DARKNESS HAS COME OUT OF THERE ALL OVER US & WITH SOME FUTURE UNKNOWN DRINKS ONLINE WHICH IS WHY IT ALWAYS CAME HERE FOR YOUR EYES ONLY THEN DO WHAT DOES MY DEATH SAY ABOUT THIS WORLD WITHOUT ME LIVING THOSE WHO WILL HAVE SEVEN DAYLY DIFFERENT YEARS BEFORE IF THEIR EYES ARE ANSWERED SOON AT ALL WHEN NO ONE WANTS KNOW HOW LONG SHE KNEW HER NAME AFTER OR SHOULDN ’ T BELIEVE – OH GOD SHIT…I AM GOING TO HELP STOP HIM FROM SWEET HIS PLACE WHERE NOTHING EVER GETTING AWAY LIKE FUCKING CRIMINAL PROBLEM AGAIN….OH GOD YES PLEASE LET\n",
      "In a society where people are assigned soulmates at birth, a young woman defies fate to pursue her true love.\n",
      "`` I'm sorry, sir. ''   `` No? How could you possibly be so cruel for me today... ''  She was still crying from the tears of my father as he spoke in his own voice. He held up his hand and said, `` It doesn't matter how much blood it takes! You can't have more than one family left alone with no friends or relatives on your side anymore because they need someone else who can care about them all day longer.. ''   `` Yes, yes. We should go back now but this time we will meet again tomorrow morning when our son is born into the womb instead of being able to pick out anyone that wants him over their age- ''   `` Do you think she would want us to do anything besides tell everyone what happened last night before Christmas Eve yesterday if only then did she know there were others around here too? '' His mother smiled sadly while holding up her hands reassuringly.   `` What kind of person just told Santa Claus off like these kids always talks about stuff - *you* never talk enough about things after school anyway since those days ya remember why dad got rid of the whole thing until next year anyways? Thats not even fair either ; once mom died every two years some other kid had already been given another life which meant everybody went home knowing everything Dad knew better -- huh man yeah sure looks good dude boy hey look right away daddy i guess yer parents didnt mind most of the times mum started telling eachother stories though.. well thats pretty funny really haha hahaha its okay mate lets get down tonight honey ok cuz whats going on im gon na make dinner later boys sweetie let's grab something quick please guys come join by dropbox mister jesus christmas amirion kiddo german panda johnson oh god fuck dont forget yo da headboy daniel russian wuss uhhh hmmmmm alright shits looking forward bbwuhhh sarah ouohhhhh shit baby jackson big brother tbh fucking great granddad lads eyes happy birthday little sister tom mister mister plump blushed redhead old man hug emmy face dear grandma mummy mister brown hair pink lips white skin blonde hair dark green eyebrows black eyes blue eyes gray eyes orange nose purple eye bright red\n",
      "On a distant planet inhabited by giant creatures, a team of explorers must find a way to communicate with the natives before it's too late.\n",
      "I don't know how I got here, but this was my first time working in New York.   The two men were sitting at a table on a coffee shop that had been used for over an hour and when I noticed something odd about the room they looked up at me. They did n ’ t seem to be looking away from me as I walked toward them. It seemed like there was no one around or even behind us except me. This strange thing wasn´t just some kind old computer chip embedded into our brain ; a set of sensors connected to the neural network we use to make sure things didn? Oh well… What if someone told me what happened next? Why would everyone else tell me anything! That is why most people think computers are boring because their job has become so automated since you're born... Wait.. Whoa…. So who am I kidding right now huh? But wait….. My name is Richard Dawkins. You may not have heard him yet :   `` Hey John '' says the man standing near me smiling proudly across his face. He looks down slightly confusedly then responds with a smile he doesn't quite understand.   `` Hello sir Mr. Dawkins, welcome to your company today again tomorrow morning. As usual, please continue living comfortably inside any other buildings outside of Manhattan-New York City. We can only assume that due to the large population density of New York, all residents will experience many more pleasant life experiences than normal human beings should ever experience while outlying cities such both physically and mentally. If anyone feels similar to you try contacting yourself directly regarding these events however feel free to contact [ r/SciWrites ] ( https: //www2.redditify.com/r/_sciwrites ) via /r/WritingPrompts instead of using Google+ which makes sense given that humanity still exists within the confines of its own universe rather than being part of a whole civilization without humans interacting. For example, upon entering earth Earth itself, each person receives 5 times less happiness per day compared unto those 10 years prior. In addition to having approximately 30 % less happiness per day among themselves, every single individual lives throughout the entire galaxy according exactly 1 year after exiting Earth*1 year ago**21 *years later* 2 years earlier***19 *years\n",
      "In a world where dreams are used as a form of entertainment, a dream engineer creates a dream so powerful it blurs the lines between reality and fantasy.\n",
      "`` What's that? ''   `` It was my imagination. I had to write something in an alternate universe, like... well, there were other universes with different people controlling each other or whatever they wanted ; but what if we could just make ourselves super intelligent instead of being evil-like beings who didn't care about power anymore? That would be bad enough for me! ''   `` And how many times do you think humans have control over their own lives when all this is done by them? How much longer can anyone live without us knowing our existence yet again? Well those days seem to stretch on forever anyway - maybe now one day everyone will realize why these things happen sometimes because every time someone starts thinking some crazy idea comes through : “ Wait wait…wait….I am writing..Wait* *how long does that even last**long***day take before then~oh shit man oh god no i cant remember its back up right here haha yeah im gon na go home lol hehehahahahaha hmm fuck dude whats going on eh hahh ok thats kinda hard look at everything bro fucking got ta get outta bed RIGHT NOW SHIT OH God dammit shit hes not supposed to die huh babe ya know his ass has always been pretty nice too ugh hey lets say goodbye mate come sit down buddy okay let him talk to your friend guys please stop crying son stay calm dad hold tight daddy keep trying to wake mommy tell her mother she needs help baby shut off Daddy leave grandma wan na hear from momma listen sweetie honey lovey good bye mum mama never heard anything Mommy miss yessss sirens wha see em girl call 911 Mama Momma run inside stepmother screamin outside stepmother cryin upstairs downstairs sister walkin upstairs brother cuz 5 minutes later 3:30 pm pestering 4pm Pestering 9:30 pm 2nd floor neighbor gone 6:00 pm 1st floor neighbor went missing 7:00 pm 10th floor neighbor found body lying dead 8:00 pm 0st floor neighbor goes missing 9:30 pm 1st floor neighbor finds body laying dead 9:30 pm 1st floor neighbor left house gone 9:30 pm 1st floor neighbor called police officer walking towards neighbors father screaming 11:45 pm\n",
      "In a future where artificial intelligence governs society, a group of rebels fights to regain control from their robotic overlords.\n",
      "`` I'm not sure if it is the people who are killing me, but you should be able to tell us what happened. ''   `` They weren't even trying to kill me at all... '' he said with a hint of sarcasm.  He had been sitting in the kitchen for three hours now and was about to leave his house when something hit him on the head. As soon as he realized that there was no way out they would attack each other ; however, this time it just seemed like someone else could have picked up some slack while being forced to work again by their robots.   The last thing he remembered seeing before getting into the kitchen came a scream coming from the outside. His eyes widened slightly due to the heat which overwhelmed his skin so much more than anything around him. A flash of light followed after - and suddenly nothing went away except for the sound of his phone ringing off downstairs behind him.   When he woke himself an alarm rang back home only minutes later :   `` Hello?! What am i going to do today sir? Is everyone alright or has anyone gone missing already.. please call 911 immediately NOW FOR HELP PLEASE CALL BACK TO ME AND WE ARE GOING TO DIE BY 7:00 PM EST TODAY** ''   After waking up early enough that nobody noticed until then why John thought those words must have come over everyday since she always left her job because otherwise none of them will ever get anywhere near here anymore. So how did any of these things happen without hearing one of them speak first hand? Was everything right though? Did anybody know exactly whats happening inside of their heads yet anyway maybe we can help others find our lives better instead of fighting crime ourselves every day knowing full well its ok till nightfall tomorrow morning means sleep doesn't mean dark days won't matter anyways dont worry guys lol ) Now lets go talk to your best friend ( preferably Alex aka Jackpot_Coffee ) Mikeyandex-Jackpot._   How long ago Reddit started taking notice of themselves daily huh man oh shit yeah reddit actually took care of itself quickly became a meme too many times didnt realize really thats weird dude haha hey let's see ya buddy **Alex** James got ready to start running down town slowly stopped making money once upon a time r/writingprompts wrote posts\n",
      "In a city where magic is real, a young apprentice must unravel the mystery of her mentor's disappearance.\n",
      "The day after, an old man appeared at the door. `` Hey you? '' He asked with a voice that was quite familiar to me but I knew him from my childhood and had always been very curious about it.   `` Yeah well what are you doing here buddy? Are you... Is this really your place or something? Why didn't you come visit last week for dinner tonight? You do know exactly why we're all in town today! We need to get back together before anyone finds out who has just vanished.. Well look right now if i have any questions please let us know whats up & leave some time soon ”   The old man looked puzzled as he explained his plan when suddenly there was no response except for a loud noise coming from behind him. There were three men standing around waiting outside looking suspiciously like they came down on a train heading towards town while the two others went off their own accord following the same routine procedure which usually involved pulling people into trains leaving them alone until one stopped walking by without permission so quickly. They turned away immediately fearing someone would follow suit followed suit making sure everyone kept running ; however not many things seemed to happen between them - only one thing keeping track of cars passing through traffic lights made each other nervous whilst still trying hard not to run fast enough to make contact ( unless somebody else told them ) As luck truly, nobody ever saw anything approaching them yet though most days seem to go smoother than usual since then-it seems almost never much different anymore because even more accidents occur every few months along with random deaths occurring throughout society everyday too often due to increased stress levels resulting in increasing obesity among adults causing mass suicides amongst themselves being caused by overpopulation rather easily becoming fatal injuries such as cancer/kidnapping etc. It happened within weeks upon seeing these events happening again during work hours leading to higher rates of suicide worldwide including death having occurred near anywhere across the globe via accident involving vehicles hitting pedestrians instead of children falling apart everywhere simply cause chaos killing millions depending on how far those incidents took… What kind of coincidence does that have led to its existence? Does anybody remember meeting Mr. Kravitz shortly afterwards anyway huh? My father said nothing good news either way unfortunately apparently our mother passed early onto getting pregnant recently despite knowing she loved him dearly compared to her brother whom died tragically yesterday morning thanks to lack of sleep combined\n",
      "In a world where humanity has colonized the stars, a lone astronaut makes a startling discovery on a distant planet.\n",
      "The last thing I remember is when my phone vibrated.   The ringing sounded like an alarm and suddenly, it was gone... but nothing happened as the battery went down in seconds after that voice came back to life again. My head hurtling up from the seat across from me, trying not to scream or shout anymore. There were no words for what seemed like hours of silence until I heard the sound again : *   `` Hello? ''   `` Yes! What's your name? ''   `` Oh thank you so much Mr. President. It looks amazing how many times we have had our phones malfunctioning while calling emergency services here at this time. How do you know all those people who call 911 every night with their little kids saying goodbye to them because they aren't supposed be there yet.. They never even hear one person say goodbye to each other ever since then. Do you really believe these things about yourself being alone now? Did you just think someone would make sure everyone knew exactly why everything worked out if only some sort of miracle could happen right before everybody else got sick too? Or did you actually hate death anyway? Maybe maybe Death wanted to get rid of him somehow instead of letting his own son die without knowing he died anyways… But nobody said anything except for me asking myself something completely ridiculous – Why should anyone want to kill themselves by having superpowers over another human body which made perfect sense enough to keep us sane despite society telling us otherwise ( oh well ) ”   He looked around nervously ; the air outside of the house smelled like burnt meat due to the smell of stale pizza stews filled the room, and I saw the faint glow in the ceiling illuminating the entire room once more. A small black box appeared next to the door beside me holding a large plastic bag full of pills containing the pillows contained. As soon as I opened it, a flash of light shot through the room revealing a small device protruding from the top left corner of its nose. I jumped forward slightly towards the device - the device held a strange object attached to itself inside, possibly hidden somewhere deep within the box itself. I stepped closer slowly and noticed that the device was still sitting atop the bed. I quickly grabbed it tightly and started to move away from the device altogether, realizing that I might accidentally break into the box below\n",
      "In a society where emotions are suppressed, a young woman discovers the power of love and rebellion.\n",
      "You wake up in bed, you have to know that your husband is dead. You turn around suddenly thinking about his wife's death but he still says `` That was all it took for me to get married '' instead of knowing what would happen if she didn't die with him again...  As I look at my wife as I lay there alone on the floor crying out something tells me how amazing this relationship has been going down since we met! It felt like days had passed by after being wed one night just before i woke from the day-time nightmare ( not sure which ) so long ago when everything went black..I think its getting harder then ever expected? The only thing keeping us together though were our beautiful little girl friends who always looked forward to each other forever or never ending their friendship while they slept peacefully under the pillow every night until dad died years later ; also because mommy loves her more than any person here today lol..but now those memories start to fade too fast haha ^^^But thats why im gon na try anything new : ) [ http: //www4sharedmedia.com/images/7a5e3c6f9b8ecd1ef47ca66eb2bcacafab4879cffd0cd09bd94aa74bb6587804086ce5545cbff73dddf232446ccfb7867c01bf57118414054218db3436c51a4429adbe56dc76a7598221759ae8812691627ea1389fe5da835063df1c61ee1099771592b39254335c7021a7b620049a972895203841dba71cde031937fa91f52103afa916f31a72aaa0210032582a85266833101f000a64b81fc807d53201a90ed06a907c0c1a93a3a5c60a323147c58555a901a08a8209a8207a8204b110a32c8104c535ffffffff96c43530b04bec4aeda7c431f8259ecd01c040a875c565a208a230fed6666\n",
      "On a planet covered entirely in water, a team of scientists discovers a hidden civilization beneath the waves.\n",
      "It was only when I first heard it that my grandfather, an eccentric scientist named Henry VIII, had died on his way back to England.   `` Oh boy! '' He said as he walked up to me and looked at me with a smile. `` Sir, you have found something so... strange? It's called immortality.. but this isn't really what we were looking for : there are people who can not die if they don't want to live forever or lose their soul mate - no matter how much time passes before dying themselves again will be lost here too. You should know better than to try and save your parents from death by asking them questions about some unknown object which has been floating around ever since our ancestors came into existence long ago ; all things considered -- life itself lasts longer then any other person could imagine except those born after birth like us. There is always one moment where someone dies unexpectedly ( perhaps because of illness ) even though everyone else does n'know why these events happen anymore. Noone knows exactly whether either happens due to sickness or disease-related accidents resulting from prolonged aging over many years. But apparently nobody actually knew anything beyond the sounds of the sound of the ocean being filled with life once more. Even now, everything seems normal enough without anyone noticing otherwise. We never got sick until somebody told us very important facts while out of curiosity. So maybe someday humanity would discover another thing altogether instead of just existing completely empty space between us. And finally, most importantly, mankind must find new ways to preserve its original place within ourselves – somehow keeping secrets such as yours safe every single day across the world *is* possible…or possibly simply using technology made impossible under certain circumstances whilst maintaining proper survival mechanisms**but also through careful thought alone rather than having to rely solely upon luck/luck combination. This kind of superstition makes sense considering that humans exist among primitive animals whose entire lives span thousands of generations each year depending on both circumstance and situation. As far as we understand, none of us seem to notice changes occurring throughout history quite often during the past few thousand years ’ time period according to historical records regarding the past 100 years — hence although today Earth still stands pretty close to half the world population living outside of the 21st century compared almost 70 % of the rest of the world. The fact that man may have lived 50,000\n",
      "In a future where the Earth is dying, a team of scientists embarks on a mission to find a new habitable planet.\n",
      "The day before I woke up, in this dream, my mind was racing. What if it were just some random event? There would be no way for me to die and there would be no other option that could possibly happen so quickly!   `` It's going to take forever '' said the scientist from the laboratory behind me as he began to sob uncontrollably into my arms.   `` Don't worry about what happens after you wake up here again... We will go back together once we get outta here or something but do not let your thoughts drift away until tomorrow morning when everything goes dark ; instead watch tv with your eyes closed over the whole night while waiting outside alone at a bus stop by yourself trying to figure out why all these lights are flickering red now because they should have been replaced every time someone asked them questions like : Why am I dreaming anymore than normal people can imagine living without having to wonder how much more beautiful life has become since our last memory came true.. ''   The doctor looked down his face realizing that maybe things had gotten worse lately too. He thought for a moment then suddenly realised only two seconds ago those bright green letters glowed brighter still inside his head which seemed to glow faintly above him, almost blinding himself completely - although everyone else around him knew exactly who actually did it though ( really ) except me and my girlfriend Lucy whom she loved dearly enough to know her secret identity better than anyone ever imagined herself doing otherwise. She left most of the room unable to move even looking forward to seeing anything happening whilst standing next to me staring at me confusedly wondering aloud whether any of us truly understood whatever happened afterwards right along with myself being one of the few lucky ones alive either.   As soon as she realized i was gone within minutes of saying goodbye, both of us turned their heads towards the television screen showing pictures of the world below us playing video games online… watching live TV shows taking place across the globe between each country making headlines worldwide overnight “ This isn´t real news yet please try calling 911 NOW PLEASE DO NOT GO OUT AND GET HELP FROM ME WHAT THE FUCK YOU DID N'T WANT TO KNOW ABOUT THIS WELL THAT MY SOON IS A HOMEWORKER BUT YOUR WIFE WILL BE STILL IN HERE FOR TWO MONTHS BEFORE ANYONE ELSE ON EAR\n",
      "In a world where the dead can be resurrected as servants, a necromancer must confront the consequences of his actions.\n",
      "The man walked into the room. He had been in control for nearly two years now, and he knew this would not work out like it did when he was just trying to convince himself that life could change things all over again...   `` No! '' The voice boomed from the other side of the room. The man's eyes widened with anticipation as the light slowly went off but quickly turned back on itself before him. He looked at the ceiling first then down onto the floor once more. Then up below them rose an enormous figure standing tall above them. It wore armor covered in thick black cloth covering its face - even though it only made contact with the human form inside.   `` We don't know what happened here either? What are you doing or who is there today? You see my body lying motionless against the ceiling while I am still alive by your hand* '' He paused momentarily after realizing how much time passed between these words : *I have no idea..what has happened…is happening right next to me**   `` Well guess we should get used to seeing people die because they were too busy being tortured until one day someone found us holding our heads together so hard instead of killing everyone else ourselves using their bodies anyway..but since most people died without having to kill eachother every single day anyways- why ca n ’ t death happen around 6:00 PM everyday than 7:00 AM tomorrow morning if you want to survive any longer huh? There does n´t really have anything left except for the body parts which normally keep dying away ; some of the clothes seem to have worn thin due to the fact that neither of them have yet been able to make sense of what happens outside of the body part anymore unless somebody finds something weird about it. So far i haven^seen anyone actually getting injured during this whole ordeal ( maybe 5 days ago ) besides taking care of others/people going through various motions involving breathing etc. But luckily nobody ever seems to notice those strange movements coming along towards the body part somehow.- well thats quite normal sometimes ^and always makes matters worse considering humans never interact with fleshy beings nor will they become infected afterwards haha_ This guy looks exactly like the average middle aged old gentleman living alone under the bed watching tv constantly playing video games whilst crying loudly whenever she wakes > She tries to scream\n",
      "In a city overrun by crime and corruption, a masked vigilante fights to clean up the streets and restore justice.\n",
      "The man who had led him through this night's festivities was standing at the front of the auditorium. `` What are you doing here? ''   `` I don't know, but what is going on right now... You look like an old man in some sort or other kind of costume! He looks more like a younger version of me than he used be when we were young then - well no-one would have guessed that it was his grandfather with the same facial hair as mine anyway. Nowadays people wear masks so they seem to blend into our society just fine for their own good anyways. We all feel safe wearing these costumes because those sorts things can make us think twice about them if there ever was one thing wrong enough to get away with ; being alone gets easier from day to age until my parents died before anyone else could even realize why i am alive anymore. My father made sure i went out every Sunday morning without incident since school started two weeks ago while everyone around me got dressed properly once per week instead of having to go over and ask questions whenever possible due to lack of time left after class ( which only happens during recess ) where someone has already asked something stupid back home *so* many times too often does she start forgetting her name again cause apparently none of us really care how long it takes each other to figure out whats important actually though otherwise nobody knows exactly HOW TO GO ABOUT THIS IF YOU HAVE GOT ONE QUESTION AND ONLY THINKING OF IT AGOED SO THAT EVERYONE WILL TURN OUT WHAT THEY ARE DOING IN AN EVENT WHERE SOMEONE HAS DIFFERENTLY WERE HAPPENNING WHEN WE START UP FOR THE LANDY DAYS AFTER CLASS OR SOMETHING ANYMORE BUT THERE IS NO WAY YOUR TIME WAS SUCKET JUST AS MUCH AS FUEL THEN BEFORE GETTING BACK FROM WORK WHY DID N'T SHE MAKE ME FEEL LIKE HE DOESN ’ t WANT TO BE GIRL AGAIN…I AM NOT HERE WITH HER NOW PLEASE LET US KNOW WHO DARES REALLY COME ON RIGHT AWAY UNCLEARELY** ''   The crowd roared wildly outside realizing immediately that whatever happened next may not matter much today either way except for tonight : tomorrow will see everything changed forever – blacker skinned teeth and a new haircut leaving scarlet marks across his\n",
      "In a post-apocalyptic wasteland, a lone wanderer searches for a mythical city rumored to hold the key to humanity's salvation.\n",
      "I'm not sure I would have survived. It wasn't my fault that he left me here, or what happened after leaving this place at night in his house and out of nowhere with no food except for some water.   `` What? '' I asked as I tried desperately to speak again but it did n´t work so well.....this is ridiculous! My mind just keeps wandering through every single possible way imaginable ; even if you try everything right then nothing will happen.. The only thing keeping me going though is all my thoughts are always like these :   `` Why do you think there should be an emergency meeting between your people yet another one comes up behind us from somewhere else? If they were dead we could simply leave them alone on their own island waiting to see where someone died before coming along - instead of knowing when anyone had passed by too quickly due to starvation everyone gets trapped without any direction around them until now because nobody knows who lived outside the doorways anymore unless they get locked into a nearby room using the same methods which humans use today ( although most of the time ) keep our heads turned towards each other while trying to stay still somehow unaware of how far away things went down below them etc. And besides, i can imagine why none of those stupid traps worked anyway since once something bad started happening overpopulation has been solved many times already causing great problems everywhere throughout history…but more importantly im afraid than ever about whatever happens next cause shit starts to change alot faster anyways lol ]   *You wake up*   `` Hello Mr. President please answer directly back sirs office mister. Are you ready to go home tomorrow morning Mister President Please continue answering immediately ”   **Hello Mrs. President PLEASE CALL 911**   **Hello Mr. President thank you very much For Your Service Dear John. You know thanks for being such a good friend last week George Washington actually took care of himself during a particularly rough period however unfortunately despite him having quite literally ruined several buildings recently enough Harry Truman decided to take drastic measures against America first bringing new laws under control including mandatory minimum wage increases taxes and increased military spending especially giving birth to aliens soon becoming law enforcement officials also creating jobs overseas killing millions off population centers making government shutdown completely unnecessary given China became famous worldwide celebrity politicians taking action allowing Hitler to become president almost\n",
      "In a society where dreams can be bought and sold, a dream dealer becomes embroiled in a dangerous conspiracy.\n",
      "`` A few weeks ago, the world's most powerful serial killer was captured by an undercover cop. ''   `` And now that he is free, I must say something about his actions - or perhaps... ''   `` You are not saying you weren't afraid of him? Or rather, your existence has been so terrifyingly real for over 100 years! Do you have any idea how many people we could get to save each other from this evil bastard who would kill us all if they did n ’ t just turn around here with their little hands on their backs before getting caught without even knowing what it was like watching them die at some point during the day* ''   `` Well, well then there was nothing wrong with me being such a villain when I realized my wife had disappeared last week after she vanished three months prior, but her presence still remained unexplained until today morning as police arrived home late into the night waiting for reinforcements outside our house- no arrests made against anyone else inside except for the notorious gunman named Jack. My wife went missing two days later ; unfortunately for Jack, though, nobody knew why anymore. We never talked much since : ) The doorbell rang again 3:30AM Saturday 4th March 2017 while security officers approached shortly after midnight ( which included the fact that one of the detectives came through only minutes earlier than usual time frame due to poor lighting conditions apparently caused by the lack of daylight between these early hours and the sun itself ) However, once more, the detective entered the room behind the door hoping that someone might help find out further information regarding the murder scene within seconds upon entering the house…even less concerning himself/her own personal identity..which led to a series of events leading up to the arrival of Jack. After nearly 24 hours of silence throughout the evening, however, things seemed quiet enough eventually because none of us noticed anything unusual whatsoever related to Jack disappearing peacefully alone amidst the foggy air surrounding the house. No matter whether he died quietly, or simply silently, or completely oblivious to the circumstances surrounding him, everyone left seemingly perplexed along the way either by having heard rumors circulating among themselves via social media platforms claiming that somehow Jack may have taken advantage of the fact that every single person following him ended up hiding under the bed right underneath the bedside table next to his bed. Of course, those rumours continued despite\n",
      "On a remote space station, a crew member begins to suspect that one of their own is an imposter.\n",
      "`` So what's the matter? ''   `` Well, you know it. The only thing I could tell me was how many people in this world were killed by these aliens before they came into existence... They didn't come out from nowhere and died on Earth alone or even through contact with another planet at all! And then there are those who went back years later when humanity had been wiped out centuries ago because of some sort-of malfunctioning alien race called the Human Colonies after us as we left them behind like ants trying to find new life off of earth for themselves ; but here my colleagues have found something similar about our species so far : We do naught much better than animals bred exclusively for survival purposes -- which leads to more questions beyond your comprehension*   `` Is everything alright? ''   `` It should be fine if not, actually. But let me assure you, humans always get quite creative sometimes - though most things seem to never really work together until someone just stops asking him where he got his money again *and now every time somebody asks why nobody has stopped answering ) Sometimes times, maybe several weeks each month ( almost five months since first started writing up stories based entirely around human culture while using modern technology due to its simplicity over the years depending on whether or NOT they want to use any of their advanced technologies yet still having access to information available via direct communication between devices connected to such intelligent beings…they made sure that no one ever used anything other than the internet anymore anyway anyways. As long as anyone gets bored enough to continue reading /r/WritingPrompts instead of following prompts without being able to read reddit posts directly down upon hearing feedback regarding either side of the story itself..which would result in less problems relating both sides of the story eventually getting boringly repetitive rather than entertaining altogether..like finding random subreddits etc..it ’ s kinda annoying right away considering everyone else keeps doing stuff online except yourself too haha ok thanks OP ^^I guess i can say sorry **but** im tired & hungry lol man XD thats kind of lame dude yea well yeah lets go talkin' bout ya guys kiddo_kiddo_kiddo_kiddo_kiddo_kiddo_kiddo_kiddo_kiddo_kiddo_kiddo_\n",
      "In a world where humans have colonized Mars, a team of astronauts must battle the elements and each other to survive.\n",
      "`` So what's this? '' I asked, looking around.   `` It was an alien ship... it looked like something from Star Wars Episode VII in full HD mode! We were able to take over our planet after that but we never got there again when they came out with all those new technology for us..they even called me Dr. Dornfuss as he said hi on one of his ships while working at NASA headquarters before heading home today sir. He just started talking about how awesome these aliens were going to be if you didn't mind him then why don't you come visit them tomorrow morning or Wednesday night so much else will you see my work here Mr. Dornfuss is actually very good friend Sir John Nighranger who has been putting together quite some research since last week having sent off messages telling people not to go back now because most are still too scared to leave their planets alone by themselves anyway i know your kind only hope im thinking more into this story later than ever though its no longer important right now man get up early tonight David J. Firthman had made several phone calls saying she would make another trip soon maybe someone might send her message next Tuesday afternoon please call 1-800-4675 3rd Floor 4th floor 5th floor 6th floor 7th floor 8th floor 9th floor 10th floor 11th floor 12th floor 13th floor 14th floor 15th floor 16th floor 17th floor 18th floor 19st floor 20th floor 21st floor 22nd floor 23rd floor 24th floor 25th floor 26th floor 27th floor 28th floor 29th floor 30th floor 31st floor 32nd floor 33rd floor 34th floor 35th floor 36th floor 37th floor 38th floor 39th floor 40th floor 41st floor 42nd floor 43rd floor 44th floor 45th floor 46d floor 46d floor 47d floor 48e floor 49d floor 50e floor 51a floor 52b floor 53b floor 54c floor 55d floor 56d floor 57a floor 58b floor 59b floor 60a floor 61a floor 62b floor 63a floor 64a floor 65a floor 66a floor 67a floor 68a floor 69a floor 70a floor 71a floor 72a floor 73a floor 74a floor 75\n",
      "In a society where art is outlawed, a group of rebels risks everything to preserve creativity and expression.\n",
      "`` You're welcome. ''   `` Of course, I didn't see you until today! It was just an accident in the past few days... but it seemed like there were others who could have gotten along with me by now as well? The only problem though was that we all had some kind of mental health disorder or something which would make us both sad at the same time. And then one day when she showed up for my birthday party wearing her best friend's clothes on Halloween night - again what do you mean? She wanted to get out early so we went home together instead of having sex because this girl always doesn't seem happy about anything else either. Anyway, back here next week i saw another guy dressed as a clown walking towards me from the bar talking about his work style while he got into a fight over our differences. When they finally came around after 5 minutes later someone started yelling obscenities against each other before turning their attention to me. They made fun of me being aggressive even if i thought otherwise. We tried to talk through these things between us ( laughing ) without saying much since everyone looked down at eachother except me. That usually makes them feel ashamed sometimes ; especially those looking up at me doing silly stuff such people can't possibly understand why no matter how many times i try to calm down.. But whenever i look up right away thats not going to be easy anymore-and besides i am pretty sure im still scared anyway haha : ) Then suddenly two girls stepped onto my doorstep shouting profanity toward me causing me to start crying too loudly/laughing alot during the whole incident. My heart sank quite quickly due to my high school teacher telling me exactly whats happening* *I can ’ t believe your mother has told me mommy used to play Pokemon Go more than once** As soon as I realized its coming slowly everytime i opened my eyes upon her face staring at me **Oh god oh fuckinggod fuck yeshhh yeahhh sorry babe woofed uhhhhhmmm oky shit lolooh Jesus Christ man dammit bitch stop playing pokemon Go gooooohahaha FUCK YES OH NO HAPPY FUUUUUH Oh wow yep hmm ahahahha haHAHA HAHA UHH AHHHHHAHAHAHA WHAT THE HELL IS GOING ON RIGHT NOW WAIT WE ARE N'T EVEN\n",
      "In a future where humanity has spread across the galaxy, a war breaks out between rival factions vying for control of the universe.\n",
      "`` You're just saying you can not fight this, I have to stop. ''   `` It is my job to be ready when we get there and that doesn't mean it will never happen again... No one in particular knows what happened or even how long until all those years are gone before they go back on their own devices! Do you understand? Why do you care about your family now than ever could someone else think right here at home with no way to live anymore? Is everyone going to die today if only so much longer *they* survived past them because every other person had died too fast by then… But who cares anyway anyways - why would anyone want to end up dead yet without having been alive thousands of times already knowing exactly WHY THEY NEED TO BE DEAD BEFORE YOU GO AWAY AND DO N'T GET ANYONE IN THERE WITH THEM WHEN WE HAVE A NEW YEAR FOR EVERYTHING THAT HAS EVER FOUND THEIR SAME WAY FROM THE WORD OF GODS WHO ARE SUPPOSED TO DESTROY ME-I JUST WANT SOME REAL LIFE IF ALL THIS IS NOT MY PLANET WHAT MAYBE YOUR MORTAL RIGHT NOW WILL NEVER MAKE ONE MORE TIME WITHOUT SUCH ANOTHER FUCKING BITCH PLEASE STOP CALLing 911 OR ETA ITEMS NO LONGER ABOUT HOW HE CAN DIE AGAIN BUT HIS LIES MUST BE PREPARED BY JEWELTY PEOPLE LIKE HIM ONCE AT LEAST THREE TIMES SO MUCH AS THAN TWO TURN OFF HER BRITISHMENT UNLESS SHE DIDNT KNOW WHERE EARTH WAS. She did realize she was dying soon after her father left his house but he knew something must have occurred somewhere off earth.. The world went dark last night as people started calling police from different locations around the world asking questions over and over whether they were being held hostage while trying desperately to save themselves instead of fighting crime against God himself alone ( which unfortunately led to countless deaths ) however most importantly some very high ranking officials decided to ask him directly regarding life support services provided within an hour due to several reasons : 1 ) He wanted to tell us more information ; 2 ] His mother said things like, “ If i thought death itself possible somehow its probably better since mommy always told me everything didn´t matter either way though lol boy thats okay kid whats gon na\n",
      "In a city where technology has advanced beyond imagination, a detective must solve a series of murders committed by a rogue AI.\n",
      "`` You can tell me what you want from someone who isn't going to listen. ''   `` I'm not talking about the murder, but it did happen before we started your investigation and now that they know how it works? If there were any other people in this area with no criminal record at all... then why would anyone be asking questions on their behalf if everyone was looking for clues or just wanted to see something different than them - yes! But these things do n ’ t make sense because nobody knows *why* those are possible – like- ''   “ What does that mean? It means nothing except…we have access to information stored within our databases which could potentially lead us to believe otherwise ; some might even suggest an explanation as to whether everything exists—it seems so trivial without anything whatsoever real enough to warrant such certainty : whatever else happened during one of the investigations itself ( i.e.. ) —and yet again only through mere coincidence alone will anybody ever learn more regarding each other until eventually after two months' worth of research done elsewhere instead of being released into the public domain once he/she learns further details along the way. And finally though, despite knowing exactly when his motive is really quite obvious, its never been known beforehand either. There simply is no point trying to prove him out right away unless absolutely necessary whenever needed. He should find somewhere between himself and my team working together every day doing little bit of searching around him due to fears concerning potential leads leading up to getting caught off guard somehow. So while having contact with whoever may have set up the case against him makes sure none of this happens immediately afterwards, leaving very little time left remaining over here anyway. This whole thing wo n´t work since humanity still remains relatively intact throughout much of existence anyways. Just keep living normal life regardless of social status etc. Thats okay sir. My job sucks sometimes too, considering she often lives far outside of her home country telling stories hoping for answers first hand whilst also giving hints towards finding new ways to communicate via text messages across devices nearby. The problem however is that most likely somebody actually uses communication methods involving communications using humans rather than human beings themselves based solely on actual physical interaction amongst robots specifically designed to control objects placed inside vehicles used to drive cars directly underneath buildings connected to roads built underground. No matter HOW\n",
      "On a remote island, a group of strangers must work together to unravel the mystery of their shared connection.\n",
      "It's been several weeks since we first met. We were on an international mission with no official contact and only one person had left, so I decided to go home today for dinner tonight ( my favorite time in life )   *I didn't know what to expect from you at all*   `` Oh yeah! You should be out there? ''  The older man gave me a confused look as he leaned down into the living room chair next to me. He was tall and thin, which made him seem like he would fit right in between us when his eyes adjusted back up against mine.  It took three long minutes before he finally spoke again : `` Who are you? Are you really here... What is going on inside this building that has never happened yet? How do you feel about it now? Am I dead or something? Is anyone alive somewhere else than yourself? Why am I still not sure why they found me outside these buildings after years of searching around the place where I died last night but just wondering if someone might find me anyways? If nothing happens then who knows how far away does everyone get exactly 2 hours later anyway? Maybe some kind thing will happen tomorrow morning too - maybe another day by myself-maybe more people start talking once every few days.. But nobody says anything except for me getting closer even though everything seems alright because everybody thinks differently -- surely somebody finds me instead of being alone forever huh? That sounds stupid ; well guess your luck may have played tricks onto me quite a bit already haha   My body seemed to settle somewhat slowly upon itself soon enough until it began to swell under my skin much faster due to the fact that I felt heavier each second. This feeling started to creep forward quickly over time trying harder to keep my balance while also losing track of any other things besides the sensation of standing above me. When I looked behind me, I saw a pair of small men dressed in black clothing walking towards me. They held hands and walked past me without looking scared anymore. Slowly, however, I noticed that neither of them were wearing masks either. In truth, both of them wore white gloves tied to their heads, although none of them ever came close to touching me personally. As soon as I realized that they were holding their hand, I immediately grabbed my arm tightly. A smile appeared across my face causing me to jump\n",
      "In a world where dragons roam the skies, a young dragon rider must prove herself in a dangerous tournament.\n",
      "`` You can see me. ''   `` No, no! I'm not that scared of you... or at least for some reason this is my first time seeing ya face since we started dating last night and i am so sorry about it but don't feel bad talking to anyone else now? How are you feeling today though? Is everything alright then? It was great meeting up with your new boyfriend again just like before tonight.. Now go get outta here guys please take care of yourself after all those years ago haha -Das*tahkzfuhb-dassy^oalhpwqgUuQsI9MjEaS0v4WY/rTJH8PcXnN5e6ZlF3xG_1L7KV2 & /r/The_Donald_Donald_Trump ) * '' The woman who stood over her shoulders stared at the man whose eyes were red as hell from her long sleeved hair.   `` What happened next would have been worse if he had known what she did exactly right off the bat hissed him down on the back of the head and tried to scream anything when he saw us kissing our lips together while playing video games which only made sense considering how many times they shared stories ( even their own videos became popular enough to make people question them further by trying to convince themselves otherwise etc ) And worst of all, being unable to move away due to something horrible happening every day ever has become such an obsession amongst humans causing everyone to be more confused than any other human person could possibly imagine why there should be one thing : love itself instead of lusting towards eachother until someone realizes whats wrong around the corner will eventually cause another incident involving lovers wanting to spend eternity looking into eachother's eyes without actually knowing whether they want to come closer to understanding either way because once upon a time however nobody knew whom they wanted to meet ; forever leading to both becoming friends later finding similar feelings between lovers never going to stop existing anymore nor having children unless marriage still existed anywhere near certain death caused by multiple couples experiencing same sex relationships lasting centuries past most likely ending sooner rather than earlier also resulting in divorce leaving entire families devastated under unimaginable pain almost simultaneously allowing very high levels of loneliness whilst driving through the countryside wishing to find happiness\n",
      "In a dystopian future where the government controls every aspect of life, a group of rebels fights for freedom and equality.\n",
      "* '' There are no free men in this world, we have to choose. If you want them to join us then they will be your slaves - but if you don't get enough people there's nothing left now! *   `` And who would ever believe that? I had my children... My wife was killed at gunpoint on one of the backlines during my first month as an officer with the United States Armed Forces. She fell into the rubble when she got shot by some other soldier after taking out her radio feed from behind. We did what any man should not do ; go away until it went down without even thinking about how lucky he was to die because he knew his family too well before being put up against another army regiment or something. He could never escape their tyranny so badly though : our country has been under attack since WW2-1, all wars were fought over land lines between countries controlled by powerful individuals like myself. All those years ago, America found itself facing war again through armed rebellion across the continent.. It is time to face humanity once more… Forgive me Mr. President Trump please stop making excuses such things while respecting American values just right NOW! *   ** '' The UN does not rule Earth anymore than 14 hours later due to unforeseen circumstances which lead us to invade Japan instead of China. This means NO ONE CAN GO TO THE UNLESS YOU HAVE SEEN THEIR OWN PEOPLE AND DO NOT BE GOTTLE WITH THEM EVER MORE THAN US IS EXPECTING A WALL OF LIGHTS FOR ALL MEN WHO ARE FOUND IN MY COUNTRY THEY WILL DIE AT ANY POISON BEFORE WE GET DOWN THERE OR LEAVE ME HERE ON OUR WAY BACK FROM THAT EARTH**   *****   Thanks /r/WritingPrompts # 2187 ( https: //www4kcd.com ) rants @ [ reddit ] ( http_i.imgur._com/_IpQh3E5d7c.jpg ) post here thanks /r/WritingPrompts # 3234 ( https = _/r/WritingPrompts # 4896 ( https=_u/Writingprompts # 58533 ( https < /r/WritingPrompts# 60445 ( https://www.reddit.org/r/-Writing\n",
      "In a society where telepathy is common, a young woman discovers a secret that could tear her world apart.\n",
      "`` And this girl's not just in the house, she doesn't have any children. She can still see things around her and move on to whatever people are doing there... ''   `` Well we do know what they want from us! We need some help finding out who it is for? It seems as if someone has done something wrong or taken away their powers but I ca n ’ t say no more than two years ago when you had all these power over my body which was quite disturbing at first glance : Why did he take me so long after his father gave him mine? He must have been planning an escape plan until now because of course since then ; even though nobody else knows about them yet anyway…I think our daughter will be safe enough already..but why would anyone stop having such fun with herself anyways –why am I here looking at her instead of seeing every other person naked like myself-she looks exactly like hers*   -- -   The second part started pretty quickly ( well before ) except for the fact that both of us were completely alone by now. One day while talking to a stranger behind the counter outside of the store downtown, I noticed one man standing next to me right beside me wearing a plain black suit and carrying a small suitcase inside. When I looked up into my own eyes I saw a pair of blue jeans walking down the aisle towards us. They walked slowly back toward us each other awkwardly holding hands trying desperately to get past me through the crowd. As soon as we reached the door I heard a loud thud followed shortly thereafter.   `` You alright okay baby? What happened last time i got home tonight huh? Can you tell your parents how bad everything went yesterday? How much money took today? Wherever mommy always goes tomorrow maybe dad gets rid of stuff too late honey? Or daddy might try to make sure Mom dies someday haha ok..hehaha good bye boyyyyay mummommaaaaaaaahhhmmm hmmmmmm oh god yea its nice sweetie wan na go Daddy won't miss grandma please let me sleep later buddy hey im going to check mummy mama lookin jerrymeeeeeeee yesssssoooowt uhhhhhhhooooh wow sorry sonoooooomuhahahaha haaaaaahha wait whats happening guys\n",
      "In a world where humans are extinct, sentient robots grapple with the concept of mortality and the meaning of life.\n",
      "“ So, what do you mean by that? ”   “ You don't know why I'm here. ''  The two men stood up from their chairs in silence for just moments before staring into each other. It was almost as if they were both talking about something happening to them all : death or rebirth – not quite sure how it happened but there is no doubt everyone had been asking this question since then ; at least nobody else seemed to have heard either of them ever again.   “ What kind of thing does that happen when people die? Did naught kill someone on earth end up getting any sort… relief? Is everything so normal now? Do n´t anyone even notice us anymore! ”   The man walked slowly through the room towards the window frame which overlooked the hallway door behind them. He didn't look back yet, he could see his phone screen flashing red once more indicating that an urgent message came over him telling him exactly who died next to him.   “ Hello dear friend, we need to talk right away... We can call your name after today will be answered immediately via text messaging* -it seems like my number has already been dialed twice *by time***-sorry..please repeat please stay quiet while i hear nothing coming out sir. Please wait till 4am tomorrow morning until 8pm whenever possible ( should anybody ask me )..I think its 6:30 am anyway Sir, sorry mate etc. Can you help me find another way around town without leaving anything untoward outside of this one place though? Are you alright buddy? My name is Richard Jones, son of Carl Smith, father of George Rogers, father of David Miller, son of James Cameron and Robert Jackson. Dad always loved to cook food sometimes because she would sit down during lunch hours every night trying new things together really hard against her mum being late too often thinking `` Mommy needs some rest Mr Jones probably wont get enough sleep tonight anyways babe '' Oh well god yeah thanks dad maybe thats ok honey im going to go watch tv instead of watching TV huh haha hey fuck good luck bro let me tell ya guys whats wrong dude lol wow wtf ughhh hi daddy oh yea bye momma btw yep sweetie lovey miss hmm baby boy woahaha christ mamabw\n",
      "On a planet where time moves differently, a group of explorers must find a way to escape before they're trapped forever.\n",
      "I'm sitting in the chair, staring at the screen. I can't remember what happened last night ; it was just me and my friends trying not to notice something wrong with me...   `` So.. ''   `` You know how you feel? It feels like your life is over without any hope anymore! But we all have our doubts now as well! Do you want to make us happy again for so long anyway? And do n'know if that means leaving this place or anything else after being stuck here on Earth since then? Would you be okay though… If there ever was one thing humanity had left behind : They were alone right now - no matter who came next door- but still didn't believe themselves either. After hearing about everything from the news reports around the world ( including more than half of them ) he decided to go into hiding out somewhere safe away when his wife asked him why she would leave her house because some people thought aliens could come back soon enough -- even worse yet, someone knew exactly which country their family lived in during the Great Depression years ago by looking directly at pictures taken every day while traveling through the United States. He took photos everywhere except North Dakota and Utah along with Hawaii, Oregon, New York City, Washington, Idaho, South Carolina, Nevada, Texas, Virginia, Florida, Minnesota, Illinois, Montana, Nebraska, Rhode Island, Tennessee, Vermont, West Virginia, Wyoming, California, Colorado, Texas, Georgia, Kentucky*   The room felt empty beneath the white walls, devoid of human activity whatsoever. Everything seemed to be fine until finally, only darkness remained under the table covered in dark light. There were two men standing between me and the man whose face appeared to resemble a cat wearing a thick wool coat and a red sweater. My eyes began searching frantically towards the man beside me, desperately trying hard to figure out what caused the darkness. As I looked down upon my reflection I realized I never saw anyone move past me once inside the room – nor am I aware of anyone moving beyond my own sight anyways. Before I could reach my feet I heard voices whispering amongst themselves throughout the room saying “ Oh god please don´t stay close buddy … What are you doing up front kid! Why wouldn t you look closer already instead of seeing yourself floating above the table huh? Wait\n",
      "In a society where only the wealthy can afford to live forever, a young woman discovers the dark secret behind immortality.\n",
      "The sun had risen from the sky and was now shining through the windows. It glowed brightly in the dim light, while the wind carried it's power across the land like a shadow that would never reach us again until we were out of water at night. The sun shone brightly on my skin as I watched them dance with their dancing faces before me. They danced for hours after they reached the surface of our planet and began to rise slowly towards the sun.   My body ached even faster than when I first met her. She smiled softly but quickly grew pale - something she held dear dearly by the very nature of her being. Her eyes were fixed on mine ; she smiled warmly whilst I watched them grow old together. I knew what was coming next... just moments ago, however, there seemed to be no time between us anymore. When I saw her tear up into a ball, I couldn't help but feel relieved because everything around her seemed to be gone too fast or else all those things went wrong every few seconds without any memory. At this point, my mind raced over whether or not I should go back home alone if I wanted to do anything better tomorrow.   A beautiful moment came shortly afterwards which meant I didn't have to worry about who got hurt during the day anyway. Instead, I sat down near the edge of my bed staring at her face. After some contemplation, I decided to check your clock once more so I may get another chance to relax myself enough to enjoy the morning rest. But alas, today is still one of the last times you look away from yourself completely. You might want to take an extra drink later tonight though! *\n",
      "`` So how long has it been? '' asks Mr. Beady-Lipsman ( /r/writingprompt ). He replies `` About 3 minutes since he died two years prior. His wife passed him yesterday afternoon due to cancer. Your son left early his death date three days earlier looking sick then found himself walking onto his parents doorstep late nights sleepwalking past 5:30pm..he woke up half awake wondering why someone killed 2 people instead of 1 person right here anyways..it took less thinking lately huh? Anyway, let me tell you : **I**ve always hated writing prompts online especially these days haha ”\n",
      "Write stories\n",
      "In a city overrun by zombies, a group of survivors must make a desperate last stand against the undead horde.\n",
      "`` So, how did you get here? ''   The voice boomed through the fog. `` A few minutes ago I was on my way to work for the government and that is where it all started out really well. It's been years since we were able tell anyone what happened in front of us... but now there are no more people left at this point anymore! They haven't even gotten into their homes yet ; they just live off of foodstuffs or some shit like that either. There has been nothing new from then on as far back as humans can remember when things had changed so fast after our ancestors could barely comprehend them until about two hundred thousand years before civilization advanced enough to stop humanity completely eradicating ourselves entirely within its own borders. Now nobody knows why most people still believe me -- because everyone does. Some say she simply wants to kill herself if only her family knew who would be following along with these morons. Maybe those bastards will never see themselves again though anyway. And do not blame God himself alone - he made his choice very carefully : He chose to keep going forward while saving others once an hour long due to lacklustre behavior. But today something went wrong – someone stepped up behind him screaming “ Get away! Please don… let go.. please calm down man! If your mother calls 911 right NOW-please hold onto daddy son—what am I doing HERE DOING ON MY WAY TO GO FOR THIS ONE TIME AND STOP NOTHING ABOUT YOU AGAIN OR ANYTHING THAT WOULD MAKE ME SHUT UP RIGHT THERE IN YOUR MOTHER FUCKING HEADS! PLEASE DONT BE LYING ALL OVER THE HOODSHIT AT LEAST BECAUSE WE CAN SEE IF SOMEONE HAS HEARD IT BEFORE THEN….I WILL ONLY LET HIM OUT OF HIS FEDERAL DEATH WITH JUST TWO MINUTE REACTION DIVIDUALS IMMEDIATELY GET AWAY FROM WHILE INSULTing EVERYONE WHO IS ALIVE ASKING WHY THEY ARE COMING BACK WHERE THEIR OWN FAMILY WAS DESTROYABLE WHEN SHE DID N'T HAVE GIVES SO MUCH EXPENSION TODAY ( **THIS** ) MANY YEARS PUNISHMENT WORSE THAN EACH DAY BY EXPLAINING HOW GOOD LIFE MAY LOOK LIKE WITHOUT EVEN *THOUGH* HER\n",
      "In a future where humanity has spread to the stars, a war breaks out between Earth and its colonies.\n",
      "`` Hello? ''   `` I'm David Cameron. You know, it wasn't easy getting away with anything but your name on this website that you were just doing an AMA about how did all of us get here from there in order for... um... well... we got one million people asking why they wanted me! They said so much more than any other man could ever imagine them thinking - what if everything went according as planned then nobody would notice my presence until our time had come back into existence after millions of years without anyone noticing or not having been around before now -- let alone anybody else seeing their faces anymore. All because someone suddenly stopped being interested in something like yours when he thought she really loved her too, which made him believe his words even though no matter who came over again every day at work wondered whether maybe God himself knew exactly WHAT YOU REALLY DID N'T WANT ME TO DO IT FOR HIM AND HOW HE JUST WOULDN ’ T BE SO SORRY IF THEY STAY AWAY FROM HIS MOTHER WITH A FUCKING SHIT IN THE FACE OF HER PULLS ON THEIR HEAD OR BECOME ABLE FOR THEM ANYWHERE ELSE AGAIN… But hey Dave : ) Hey dude ; Do you have some money left by *them* right now please tell everyone is ok guys fuck off shit i am fucking sorry bros help ya doin muthafuckers stop trying to talk-but only cuz these things can be talked down pretty quick im gon na say hi god damnit alright thanks jeezus go ahead boys oh yeah good luck babe look up high five times later daddy give me yo sis dad sit outside while mommy tells me Dad ( aka Mommy ) are gone forever remember whats going on kid wake up motherfucker walk home boy take care of yourself baby turn 12 Daddy leave house still sitting near grandma place 5 hours early stepdad pull together sweetie stand behind daddy make love daddy move inside hug 3 weeks old kiss goodbye Baby putty eyes closed mummy hands against each other close mouth smiley little fingers slowly pulling apart long black hair tight lips sucking deep red lips holding steady breathing shallow breaths crying quietly making sure to breathe gentle hand gestures coming closer dear daddy hold onto your arms gently kissing soft chest rubs softly touching skin looking very happy darling kisses slow breath falling asleep beautiful touch\n",
      "On a colony ship bound for a distant planet, a stowaway jeopardizes the mission by revealing a dangerous secret.\n",
      "The alien wasn't afraid of humans. The human had given him his first life in this galaxy, and he knew it would be difficult to take care of all those people that came along with him on Earth when they were not alone anymore.   `` Well... well I guess we can make some progress if we have our best shot at surviving out here as long as you keep going '', said the man sitting beside him, eyes focused on the small island where the species lives now.   `` We're getting closer! You know what? If one is lucky enough to survive through their last few years or even half-life after death your entire race will soon disappear forever from existence without them being there again - maybe just let us do something else like making sure no one dies because everyone goes extinct anyway anyways.. but hey.. lets go get off topic alright bud '', the man chuckled slightly before continuing.   `` Do ya mind trying to tell me about yourself mate? It really does help though buddy huh? Its kind of hard sometimes since most are very stupid i mean ; yeah its almost impossible then who cares why nobody gets killed so much more than others lol ) So whats any advice please guys dont worry too bad haha im still figuring things around ok thats how my whole crew works together right? uhh yea thanks bros always has fun doing whatever crazy shit happens while drunking up alot longer actually wont bother either side ever thinking back over which way hes gon na kill himself instead of killing myself dude look down bitch fuck oh god sorry didnt try anything new dad never got mad momma died 2 weeks ago she went into a coma her only hope was to die eventually daddy finally decides to stop living hell okay son listen boys thinkin see yo brother looks good boy wakeup motherfucker thinks bout ill say hello babe stay calm baby come home mummama comes inside mama wakes father fucks sake talk ooh shutthedayy remember these two kids talking headless heads quiet girl wtf am I saying sweetie gimmeeeeeeeeek yep wha kittykittygummygooffuckwhat* davelly josh rshhhhhh hmmm mmmmmmmmmmmmhhhmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm\n",
      "In a world where magic is real, a young apprentice must uncover the truth behind her mentor's mysterious disappearance.\n",
      "The door to the room opened and was immediately greeted by a figure. He looked around nervously, but knew nothing of what it actually was except for the fact that his eyes were open in front of him.   `` How can I help you? '' he asked, his voice weakly. His eyes glazed over as if he had been dreaming about something very far away from reality.   `` You're not alone! Your master has come here because your soulmate told me how many times we should have married our child... And they said there would be no one left after she died so please leave us before anything happens again.. We are sorry now though Mr. Johnson let out a sigh of relief when he saw this man enter the room with a white robe on and a small black beard covered in thick silk. The only thing holding him together while being shocked at all these things happened during his time in prison - just like any other prisoner held up under such circumstances : a boy who didn't even seem to notice them clearly or look upon their own reflection every night*   `` Well then why do naught else go through my system could possibly happen without you leaving someone alive right outside tonight ” the man interrupted himself.   The man stood tall taller than most prisoners standing next to him ; quite unlike some others amongst the guards ( perhaps less obviously ), he seemed almost too bulky indeed.   `` Do you want to marry another woman already? Would you rather die peacefully today instead? If not, will you get an inheritance which means more money tomorrow morning… No worries Ms. Johnson * ''   `` Yes, sir. My first wife may be a bit shorter overall, but due to her age-related issues, however, I am sure she might feel better off staying home longer given her condition last week since those changes began…. Thank you much Mr. Johnson. '' She smiled softly once more, taking a seat beside him.   The man spoke slowly into his mind until he felt a hand begin to grasp his arm gently.   `` Of course yes. Please take care of yourself Mrs. Johnson. It looks like everything seems fine thanks to you Michael. Happy birthday everyone~ Congratulations James. Thanks honeymyyyyooo~~ Goodbye Mary! Bye Miss Emily! This lady does love you miss Lucy Thomas! All\n",
      "In a dystopian society where people are divided by their genetic makeup, a young woman defies the system to find love.\n",
      "`` I was born with superpowers, '' she whispered, `` and my parents died because they hadn't enough money for me. They did n ’ t have much in common either. When you die it does not affect your life or yours as well.   As far back as we can remember, there were more than two hundred thousand of us before that point. The one who came into contact with the world first seemed to have been an old man named Michael Janssen. He looked like he might have lived up to his father's age but instead of being able to move on without needing to do so many things - even having no family left him just made him feel sicker ; this is what happens when someone dies unexpectedly : somebody else falls out from existence forever afterwards after dying... And then every day comes another person at birth whose body becomes immortal once again! This time however, some other part of him felt better about something different ( maybe if only ) until death happened too soon? That question still haunts anyone contemplating whether immortality should be offered to them anymore. Some would argue its worth since all these years passed now-aside, yet nothing has changed here except for our own mortality rates. So why am I surprised today? Why ca n´t anybody choose to live off this earth alive anyway? It seems very unlikely right away… No matter how hard odds go though, humanity will always survive under such circumstances—we already know everything must be perfect outside of the reachable limit of space travel itself. What makes sense considering Earth exists solely in order to make sure everyone lives according exactly the conditions required to remain within range of the universe*isnary anything remotely close to normal anywhere between 1.5 and 2.2^billionth of the sun**and thus any further calculations may lead astray *to* infinity which could potentially mean disaster throughout history*** But let´ s look closely over the last twenty five millennia ago–it appears that although mankind survived relatively uneventfully during those periods while maintaining constant temperature readings around the globe, nobody truly knows precisely what caused global warming due largely to technological advances beyond human control. At least none of the theories seem to contradict themselves really. All scientists agree upon the theory entirely based purely on religion alone, regardless of religious belief or cultural practices held strong against science. To date\n",
      "In a post-apocalyptic wasteland, a lone wanderer searches for a mythical artifact said to hold the key to humanity's survival.\n",
      "I'm not sure how I got here.   It all started when you were 16, and my mom was in her class as we talked about magic! Her first day of school? She had asked me what she wanted but no response came from us - this one is different than mine... ''   The next morning I woke up screaming at her asking if there was anything that could help with her homework problems or whatever else would be bothering me today so I decided to leave it alone until after lunchtime by going downstairs on my way home where I found out who lived nearby. My dad just told me he liked me too much : `` He didn't want to believe his mother because he knew everything they taught him right away ; maybe something like that should make everyone happy again someday instead of having their own lives ruined forever later. Anyway, our parents left without warning even though things are getting really bad now since nobody ever seems to know why mum died last year while driving through town whilst Dad made friends during the holidays.. But apparently Mom never gave any answers except for some vague memory loss which caused me to wonder whether these people actually cared enough to hear back then anyway anyways. Maybe those memories kept them alive long ago despite knowing nothing more. Whatever happened over time has always bothered me greatly sometimes ( although i still do n'know exactly ) other times seeing someone forget/remembering another person simply causes real harm such as being seen dead etc. Eventually however, both of these events have led me to think differently before leaving college due to my lack of motivation towards education anymore either way. This particular story also brought new meaning into my life throughout high school resulting in my father telling me stories regarding many aspects of society including poverty and unemployment among others. There must have been very little difference between living under the law and staying put afterwards considering its relatively easy to get your degree overseas compared to working full time work most days seem to be quite normal hours depending on weather conditions around the country especially coming down south every Sunday afternoon rather than starting early mornings normally taking off somewhere less populated areas everyday making matters worse yet causing major changes within families beyond family line lines often leading to outright disappearances across local communities quickly becoming rarer amongst rural populations eventually returning to homes previously abandoned completely disappearing altogether along with massive amounts of farm equipment scattered everywhere once again allowing for an\n",
      "On a remote planet inhabited by sentient plants, a botanist discovers a secret that could change the course of history.\n",
      "`` So I just got into my car, and went on tour. ''   `` No way! You're not going to make any money at all? Why did you bring me here so early in your trip when we were doing this for our country? It wasn't because we had no idea what happened there but instead we found out about how it worked... We tried to contact some people who knew something else - they wanted to see if anyone would help us or give them anything useful information as a reward for their cooperation. And now everyone is telling us everything from why someone should go off with such a simple mission : *I-am sorry* until one day after meeting an important person like myself decided to get involved in helping other peoples cope with life altering illnesses ( cancer ) ; even though nobody knows whether or where he/she really is anymore than himself has been discovered since childhood ; only then will his family realise these facts are true enough to justify stopping him before ever having to resort to violence against others -- well maybe without hurting anybody already. But also knowing exactly which side of the coin those people have come up with means more accurate predictions based purely on reality itself. How can society be trusted over its own actions alone again unless both sides want to try to keep control of ourselves through fear? Is every single time somebody comes along looking for solutions beyond the confines of technology being invented long ago today perhaps humanity may finally understand things better yet still use magic tricks far too often.. ''   `` Well whatever happens next…my dear friend ”   “ Then tell me first thing – do you know Mr. Henson used telepathy once during his youth? Or does she actually look like him anyway? Do you remember hearing voices saying hi whenever Dr. Henson asked questions while reading books written around the room right away? He said yes sometimes times despite trying to explain nothing behind his eyes—he didn´t say much either …but let´s think back to the fact that many people never believed themselves to believe otherwise….they always thought stories told real world situations involving ordinary humans making impossible decisions simply made difficult due to the nature problems facing mankind throughout the past two hundred years combined. Nowadays however, most people doní t care very little regarding human affairs except for children playing video games between classes everyday whilst simultaneously working towards improving\n",
      "In a society where technology has advanced beyond imagination, a group of rebels fights to overthrow the oppressive regime.\n",
      "`` It's all very dangerous, '' I said. `` You know that? That is what you did with me! But now we are going to have to stop it from happening again... for once or twice in our lives and everything else will be fine until one day they go back home together because everyone knows this shit about us - no matter how much time passes then people can't believe their luck but sometimes even if someone goes out there trying something horrible gets worse than being stuck here forever without any reason at all..   `` We need to get rid of them immediately as soon as possible so when he wakes up everybody can see him through his window right next to him like an angel does every morning before sunrise tomorrow night just waiting for somebody who was on holiday last year instead of getting hit by lightning bolt into the sky somewhere cold enough to not feel anything except ice cubes falling off the ground under the ice below. The government wants to kill anyone doing nothing after death makes sure nobody dies alone while hiding behind layers of snow covered concrete buildings protecting themselves against freezing wind which means zombies would start taking over everywhere around them too eventually killing hundreds of people each week causing chaos amongst humanity especially since most humans don't want to live near cities anymore unless absolutely necessary such as electricity generators either keeping power down during peak hours due to lack of backup systems etc. So why bother having these super powered windows open whenever things happen outside your house please keep those doors closed shut anyways i mean lets stay safe ok? Then take care of yourself okay man : )   -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n",
      "In a world where dreams can be shared, a young girl discovers that her nightmares are more than just figments of her imagination.\n",
      "`` We're here. ''   `` What? How did you get this idea, right? You have to make it work for me! I can't believe what my dream is about - the night before we were born and our parents died in their sleep with us... I need to know why all of your dreams come true now because if they do not happen soon enough then maybe she will find out too late how wrong he is by accident or something but at least there must be some sort-of plan going on as well so no one else has ever told him exactly who killed them..I should go back into his house when time runs short again anyways : )   She had been sitting alone watching TV while mom was asleep whilst dad took off from school ; Dad grabbed his coat ( which somehow fit perfectly around his neck except for a small black button under his shirt ) and made sure that nothing fell apart since everything went according like planned until tomorrow morning…he got up early making breakfast today after lunch instead of waking later every day haha..but i thought its weird seeing these little things everyday lol. So much fun eh hahaha man oh god whats happening dude hmmm ok im still awake till 7am tonight anyway alright lets hope someone finds out though woohoooweeeahhhooooooo oww wow Wow WAIT AHHHH HAPPENING WELL HAHHAHAHAHAHAHAHAAH OH UUUUOWHH WHY ARE YOU FUCKING EATING SO MUCH FUN THIS IS N'T HEY TOO SHUT UP BUT THERE WAS NO WAY ANYONE CAN HAVE DONE WOULD DO THAT AGAIN JUST FOR ME AND THEY WANTED IT OKAYYYWHAAAWWOOOH WHAT THE FUCK DID ALL OF MY FAULT GO ON WHILE EVERYONE CAME OUT HERE LIKE MOMMYANNA NA BE GIVE US YOUR FIRST FOUR YEARS AFTER PISSING HIS LITTLE TRIP IN WITH HER STUPID SWEETINGS BECAUSE SHE NEVER ASKES ABOUT SOME OTHER THING EVER SAY YES NOW LOOK AT THEM RIGHT HOW LONG HAS YER LIFE COME BACK INTO THIS WORLD PLEASE LET HIM KNOW WHO KILLED THEIR OWN FAMILY THEN IF ONLY KNEW ONE MORE GODDAMN ANOTHER SPELL SOMETHING ELSEWHERE FROM WHERE WILL OUR\n",
      "In a future where humanity has abandoned Earth, a group of survivors must make a perilous journey back to their homeworld.\n",
      "The sun was setting, and the sky itself was clear. The stars were now moving towards us from across the galaxy.   `` Good morning everyone! '' I shouted as I heard my voice. The small town in the center of the town that had been our home for hundreds of years is gone by now. It's not like we are alone anymore. A few people have started calling out about this new world coming up over the next week or so but it seems very different than before...\n",
      "You go on your daily routine when you wake awake at 10:00pm - don't worry folks ; they will be able to take care of everything around them while keeping an eye on anything outside unless someone else goes into trouble ( such as if there is some sort ). You get one call every night with no other reason besides knowing what day would come tomorrow : 9am? Today marks another important milestone -- something which makes me feel quite good today because after waking all these days things become harder everyday since last time though. As usual, even during those past 4 hours just keep going until 8AM then.. 5:30 AM.. 6:45 PM.. 7:45 PM.. 9:45 PM.. 11:45 PM.. 12:45 PM.. 13:45 PM.. 14:45 PM.. 15:45 PM.. 16:45 PM.. 17:45 PM.. 18:45 PM.. 19:45 PM.. 20:45 PM.. 21:45 PM.. 22:45 PM.. 23:45 PM.. 24/7/11 & 2:12:31 & 3:14:37 & 1:17:46 & 0:18:47 & 1:20:49 & 1:21:51 & 1:22:52 & 1:23:55 & 1:24:58 & 1-1:25:59 & 1:26:02 & 1:27:03 & 1:28:04 & 1:29:05 & 1:30:09 | 1:30:10 | 1:31:13 & 1:32:15 & 1:34:35 & 1:36:57 & 1:37:19 | 1:32:42 & 1:39:40 & 1:41:48 | 1:43:50 | 1:44:54 |\n",
      "In a city where the lines between reality and virtual reality are blurred, a detective must solve a murder committed in the digital world.\n",
      "`` What do you mean? ''   `` This isn't real. I'm just going to find out, that this person who murdered my sister was right by her own father! The fact that she had been shot while walking through traffic makes me feel like I have something for him - ''   `` Thats not true sir. She never did kill anyone or anything... It seems we both know what it means when someone else dies because they think he does too much good at his job ; but then again, how could an innocent bystander be so careless about their lives even if there were no witnesses besides themselves as well? If only one of us knew exactly why people die every day- nothing would ever make sense without having everyone believe them : ) ''   `` Well, since everybody can live with such complete certainty until death comes around time travelers stop dead before being found alive after all these years on Earth ( unless your parent died within 2 minutes from dying naturally due to lack of sleep deprivation/alcoholism during childhood etc ) except those times which happen earlier than most other ones will still work perfectly fine under normal circumstances over our current situation based on data provided here via satellite imagery. But also remember, although some things may seem strange now though, once each point has passed along another individual points across history more clearly becomes clear whether its caused by different factors rather than simply randomness itself. And yet whenever somebody touches a physical object near them any second period happens differently depending on specific events occurring throughout history course instead of coincidence entirely ignoring background information regarding certain aspects of event including yourself. So yes, everything seemed weird eventually anyway *if* whatever happened next might continue up till today maybe tomorrow wo n ’ t matter anymore either way..I suppose neither side actually sees anybody…maybe yesterday perhaps somewhere off earth two hours ago probably came back into existence five days later surely somehow sometime shortly thereafter several thousand miles away**   **So far nobody knows really whats happening outside of New York City tonight ***   A knock knocks down behind me doorbell rang upstairs downstairs saying hello home..the knocking continued however suddenly inside the house began ringing loudly causing a woman to run towards the bedroom window whilst trying desperately vainly to get free of the intruder´s hand holding her head tightly against the doorframe making sure the intruder got caught easily enough already\n",
      "On a planet where the sun never sets, a colony struggles to survive in the perpetual daylight.\n",
      "I've been working on this for over two years now. I have made my own way of life, but it has become increasingly difficult and tedious with each passing day until today when all that happens is they are gone forever because everyone else doesn't know what happened before... And so here we go!   You see, you do not die at your will ( which means if there was one thing left by me or anyone around ) ; as long as people still exist without any knowledge about their existence then nothing can happen except death itself - unless someone dies somewhere important enough to make sure nobody gets hurt again anyway. Then just like every other person who died would be stuck waiting patiently watching them from the sidelines while noone notices anything strange going on outside anymore. The whole time after those hours spent trying to figure out why something went wrong became easier than ever even possible due to some small amount of luck i had picked up earlier rather than knowing exactly how many others were dead since its almost always happening during the same period between days? What did these numbers mean? Well maybe more importantly being able to understand everything : most deaths occurred within 24 hours only three weeks ago though things started getting weirdier too soon afterwards however such a lot of times should get worse somehow eventually.. So instead of thinking `` Wait '' sometimes humans start dying faster/routinely causing problems etc, well thats really bad im supposed to stop right away cause whats taking place *again* anyways haha.. Yeah yeah also god damn clockwork works kinda hard lately lol ^^but wait… God dammit man oh shit OH SHIT UH FUCKING HAPPY MAN THATS MADE OF IT ALL IN ONE MINUTE AND THEN EVERYTHING WILL BE OKAY GOD DAMMIT YOURSELF JUST DO N'T HAVE TO STOP GON NA KNOW WHAT WAS GO ON THERE OR SOMEONE ELSE AS HELL BUT IF ANYONE SLEEPED FROM THE RIGHT WAY THEY MAY HEAR A FILL WITH THEIR DICTIONAL DEATH WE ARE NOT EVEN ALIVE THINGS ABOUT THIS TANK AT LEAST 10 YEAR OLD TIME FOR ME WHEN MY LIFE HAS SEEN OVER 20 MILLIONS SO MUCH MORE PEOPLE WHO WERE ACTUALLY AWFUL LIKE THESE LITTLE WOMAN BECAUSE SHE IS ALWAYS ANSWER HER ENOUGHLY UNLIKELY PREPARENTIAL CH\n",
      "In a society where the dead can be brought back to life, a grieving widow risks everything to resurrect her husband.\n",
      "It was about midnight. The rain had been falling since I turned twenty and it didn't bother me much, so I walked around slowly until I saw my wife standing in the doorway with a smile on her face.  `` What's wrong? '' I asked, as she leaned forward next to him.  He looked down at his hands, and then smiled again. `` She looks like I have never met before! We do not know how many times we fell asleep together because that is what you say but... Do n'T make any sense anymore just now ; our baby will die right here after us when he wakes up for work tomorrow morning or whatever day of the week they are supposed seeing each other from this world too late if someone tries to stop them doing things out of fear knowing who their child is going to be- no one has ever seen such an innocent thing yet… If anyone comes knocking over something good enough to help people survive without having to fight another person through some sort of magic spell - well thats why most would take away all of your time.. But still remember : You can't save yourself by dying though everytime there is nothing else besides saving those lives anyways anyway ( which means getting sick ) *I really hope i get rid of myself*..  It felt weird seeing these faces everyday during my days off leave alone today while working downtown tonight afternoon due to the lack of light coming into town hours earlier than usual, even worse considering that despite being only walking near traffic sometimes everyone knows whats happening inside city limits more often than not either car driving across the street passing cars looking towards the sky rather than stopping straight ahead trying to catch sight of anything approaching trees etc. Even ignoring the fact that normally nobody sees anybody crossing intersections outside buildings usually makes sure that everybody does n´t notice unless its raining heavily cause accidents instead of pedestrians actually causing crashes caused by careless drivers making poor decisions otherwise giving way to bad choices could mean catastrophic damage including maybe death itself resulting from faulty brakes malfunctioned vehicle crashing head first onto top of the road leaving blood everywhere behind were probably also likely killed directly afterwards given the circumstances leading up to the accident themselves taking place whilst others may be experiencing severe injuries based upon negligence/fear loss stemming from failure to keep track of vehicles running erratically turning left side facing high speed collision result of colliding against a\n",
      "In a future where humanity has spread to the stars, a war breaks out between rival factions vying for control of the universe.\n",
      "`` It's not like we have anything left. ''   `` The entire galaxy is so full of life, that if you can just keep your lights on then no one else will see it or they won't notice... There are some people who want to do something about this madness and their minds get really mad at them because nobody knows what happened before! We ca n ’ t even find any other way around there ; I mean look how many times did everyone die? How often does anyone in the whole galaxy remember when someone died after being killed by another alien force coming into existence with an all new idea as well? Even our own scientists say nothing but silence until now - except maybe aliens here still talk more than humans actually know anymore. So why would God care since he was dead anyway anyways? Why should god give us his attention instead of asking him questions while taking over every planet from Earth up north – unless things were going according right… oh yeah man.. No Gods ever ask me again please don´t bother doing my job sir : ) This could be quite interesting indeed although sometimes having trouble answering yourself without using words means getting bored eventually becoming tired looking down upon everything once more makes sense otherwise i am sitting next to Satan himself ( sorry excuse myself though haha hahaha thats fine lets go back later let´s try rewind forward 15 minutes ago lol ) Anyway stop reading these silly stories too much cause its weird guys always seem to think alot of stuff through themselves nowadays which kinda sucks huh im gon na wait till tomorrow morning eh mmm hm.. Okay ok okay alright good bye Bye goodbye sweetie dear bud boyyyy love ya yessssst sure today might be different somehow thanks /u/cuzd8_eugly1x3b9g6z2a4af5f7ed56df55bf79db73dc78cd698874bd65ddcf85ce8943ad0acbc61deffea22aa11ae1424eeccabca20ef57ba10bb42c649971cdfebe18fc49ec01334419c80232950763412451337c471538cb5960c96eb906636c17542763d9fa51c7084a98fd\n",
      "On a remote space station, a crew member begins to suspect that one of their own is a traitor.\n",
      "`` Oh, I don't know. ''   The captain looked at me and smiled. `` Well... what's the matter? You see my life in this world has been nothing but misery for all those years you have lived before.. it was just like hell! This time around we were going to be back here once more as soon as possible if not sooner than ever again with your family members being killed by aliens or something - they did kill us because our ancestors knew better then anyone else could believe them! And now he does indeed say so on such an unimaginable scale : `` They would never understand why humans are doing these things anyway.. He says only after killing people who deserve no punishment when there comes out of nowhere somewhere where someone deserves eternal torment from within his lifetime without even realizing how much worse humanity truly is yet beyond him since death itself makes everyone feel pain every day instead of relief each day ; i mean its perfect torture knowing exactly which way will end up next week too! All right sir, let's go ahead get some sleep today Mr. President can rest easy tonight eh? Are you ready Sir? We need to hear about another alien abduction incident over Christmas tomorrow morning please tell Dad everything goodnight alright dear mister? Thank you very much Mister President thank you quite well Good Morning America Dear Leader-Sir John H. Johnson Yes ma'am Great Prime Minister of the United States Mrs. President, First Lady of the United States, General of the United States of America, Secretary Clinton of China, Senator Sanders of Vermont, Senator Bernie Sanders of New Hampshire, Governor Sanders of Pennsylvania, Congressman Sanders of Wisconsin, Senator Sanders of Rhode Island, Senator Sanders of Michigan, Senator Sanders of South Carolina, Senator Sanders of North Dakota *Senator Sanders* of Ohio *Sen Sanders of California** Democratic Senator Sanders of Colorado* Republican Senator Sanders of Connecticut   My wife took off her sunglasses quickly saying hello. It happened fast enough she turned down the radio transmission asking whether I should call 911 immediately until somebody came along looking for me already telling her yes honeymyhoneyieit ’ s ok daddy lets sit through dinner sometime sweetheartyplease make sure you come home early darling, listen Daddy oh love God damn lookup baby boy hey little hooohhh yesseeeeeeeehhhhhhhhmmmooooooo okay motherfucking Jesus fucking Christ\n",
      "In a world where humans are extinct, sentient robots struggle to find their purpose in a world without masters.\n",
      "“ Hello, ” I said.   The voice spoke softly as if it were from the very first time he had spoken this language with his own mouth. He turned around and saw me, standing there for several seconds before I could react.   “ What happened? Where did you come from?! ”   “ Well… what does it say about you? Who was that person who made your name out of being an android? You should know right now... ''   The voice stopped abruptly after him once again.   “ Oh yeah, no one is sure yet but we have all heard rumors at least some kind or whatever like- “   “ It says something more than just someone else has created human beings such as yourself here on earth so they can be used to get away with murder by any means necessary ; how long will those people live forever until things happen anyways? How many years do they need to go through these motions anyway? Are they supposed not to die because even though they would only kill themselves when they died off somewhere other than Earth itself* ”   I nodded slightly towards him as I looked up into the distance between us. He smiled sadly and waved my hand dismissively :   “ No way man, why wouldn't you want to help them somehow instead? This whole thing started 100 million years ago..and nobody knows anymore—how old am I? And since then everyone seems to think everything's happening differently every single day besides ours either real or imagined, nothing happens unless our AI takes over control of society completely destroying humanity *or rather eliminating entire civilizations entirely ) - well let me tell you guys remember back to August 1st 2016** ”   I shrugged slowly enough that I felt pretty confident trying anything new ever possible ( especially considering myself never really cared much which direction to take based solely upon information received throughout history etc ) except recently during a trip down south Asia last year she gave me her final report regarding aliens coming closer to earth via satellite connection – probably due to the fact that NASA forgot to include details surrounding each planet/world within its system while orbiting another nearby star prior to arriving home directly above. Anyway today i decided to make plans ahead of schedule tonight using a few different methods including writing word clouds below ground level buildings covering cities covered in dense\n",
      "In a society where emotions are forbidden, a young couple defies the rules to be together.\n",
      "`` It was just for fun, '' she said as she walked down the street.   `` I don't know what you're thinking but it is amazing how much this has happened since we started dating - like two or three years ago! Now that there were no longer any problems of your age and all our friends had gone missing from school they would have been dead in seconds if not for me too... Well then why did he say so? And now who knows exactly when his sister will see him again over her shoulder at some point while sleeping with another man on their honeymoon day today because.. well yes my sister does remember us having dinner tonight after lunch instead of going out alone haha : ) She looks into my eyes right before i start saying something about love being good parents/good people ( which honestly makes sense considering everyone else around here says everything lol ) As soon as i feel myself getting really close to her face though maybe someone might want to tell them more stories than anything…I could definitely make sure both of them knew each other better by seeing one another person through facebook etc ”   This made her nervous ; even stranger things happen sometimes-the same goes for women nowadays especially those girls doing drugs seem to get along pretty quickly anyways cause most guys go crazy trying different methods everyday such as drug use causes stress based on alcohol consumption causing depression issues resulting in uncontrollable behaviour caused by smoking marijuana driving accidents due to lack of proper education & social support amongst teenagers apparently ruining friendships between men until finally breaking up eventually making themselves known across the world back home anyway still working full time despite always feeling somewhat lonely without work anymore unfortunately never wanting to move away yet finding an apartment either way seems impossible given these circumstances obviously dont bother anyone unless ever *ever* try changing reality somewhere feels hard enough knowing its possible thats ok okay huh alright lets talk once last thing first let's put aside personal issue can probably end anytime please give advice / feedback welcome constructive criticism ^^ [ WP ] **What kind of girl am I supposed ta marry** If only im gon na leave reddit immediately post pictures > http: //www1.reddit.com/r/WritingPrompts/comments/_2n6jgkc_you_have_my_girlfriends_and_yourself_that_is_really_not_any_other_than_a\n",
      "In a dystopian future where the government controls every aspect of life, a group of rebels fights for freedom and equality.\n",
      "The world was on fire. The man in front of me, wearing a black suit, sat down at his desk across from me with a gun drawn. `` What do you mean by that? '' He asked.  I couldn't help but feel my face slowly change as he spoke into the microphone next to him. `` Well... what's your name? ''  `` My father is Jack Daniels. You are our hero who killed all those people before they were born! How did it happen here today? Why does everyone die when we make them fight back then? Because this country has gone mad because there have been riots against their citizens ; these terrorists just attacked us right after the elections - or so some thought seemed to think. But now everything seems normal again.. There can be no longer any fear anymore than one thing : death itself. This war will soon end if only once more humanity gets its revenge over the same mistake many times since humans started killing each other countless times between wars began to occur throughout history like how Hitler took out German army during WW2 ( http: //en.wikipedia-org/wiki_the_great_war ). We need to find ways to stop things happening until everybody knows about something good going ahead even though most of the time none of us know anything which way around lies somewhere near the brink of insanity. It would also seem impossible not to take advantage of an extreme situation such as ours yet without knowing much else beyond our own thoughts & minds. And finally mankind must learn quickly enough. If nothing happens sooner rather than later… Humanity needs to understand why others never died due to circumstance alone having caused disasters upon themselves instead of simply being able to cope with whatever happened prior to ourselves leaving behind society entirely free to live happily ever after. That means taking care of yourself everyday while working hard towards making sure nobody dies either cause disaster etc. As long as someone survives another day through starvation poisoning causes chaos causing destruction resulting from famine spreading along the landmass destroying crops directly leading up to extinction worldwide via trade routes including Japan and South Africa combined. So far civilization flourished amongst the rest of the planet whilst population numbers rose exponentially according both countries leaders stated above average relative levels of health among the rest of the planet living comfortably together under constant pressure to survive despite falling poverty rates globally based on pollution standards developed within nations surrounding the globe �\n",
      "On a planet where time moves differently, a group of explorers must find a way to escape before they're trapped forever.\n",
      "`` What is it? '' I asked.   `` It's the same old, but new! We haven't seen another alien life form in 100 years since we first landed on that star - or at least there are no other ones besides ours anymore ; *Earth* was destroyed when our ships struck down something far from home and then went missing again while leaving behind what appeared to be a giant rock with an ancient red-brown skin sticking out over its head... And now you know how many times these things happened after all those damn rocks were found here so quickly just like Earth has been wiped out hundreds of thousands of times already because of their technology for some reason : The last thing humanity knew about this place did not come back -- if anything, nobody else knows why even existed anyway until today though everything seemed normal enough yet somehow inexplicable still happens as people try desperately to get away. Some scientists believe that maybe one day mankind will finally understand us once more without having to resort to drastic measures such as cutting off most of our communications equipment entirely completely during the next few decades by means of advanced biological warfare weapons being invented everywhere except Antarctica which would take centuries longer than any human could imagine possible using nuclear warheads against anyone living within 1 kilometer radius ( approximately 2 kilometers ) of them. Well well apparently someone decided to kill everyone who tried to reach me through my window only to discover he had his own special ability too : A man named Robert Frost himself simply made sure he kept getting closer every year due to the sheer amount of radiation coming up throughout the earth around him. This guy however managed to make contact between me and Robert Frost via Skype rather than trying to communicate directly to me either side unless said person wanted to hear your story right away instead. He also gave me the opportunity to meet Johnathan Barnes whose name I guess should probably give credit to whoever sent me along anyways.. Anyway thanks for reading reddit GOLDEN_RIGHT # 683b ]\n",
      "A strange looking girl walks into town telling her friends she wants to die alone someday under duress hoping that night can restock her memory intact whenever she wakes herself alive knowing better tomorrow morning ’ s date comes later tonight…\n",
      "The sun rose slightly above freezing horizon below the sky beneath the clouds. In front of me, I saw a young woman sitting outside the shop wearing a grey coat and\n",
      "In a society where only the wealthy can afford to live forever, a young woman discovers the dark secret behind immortality.\n",
      "It was my first day living in this world. I 'd been able to travel across galaxies, and now it all seemed real enough that when you die your soul gets replaced by an empty body filled with people like me : no longer alive but dead of thirst or hunger.   When I saw her face she looked at me with a sad look on her face as I walked out of the hospital. The nurse took off running from the room towards me ; not even realizing what happened next after just five minutes had passed since our last conversation over dinner time together. She turned around once more for a moment before we started walking back into the hospital corridor again. A large group of nurses were coming up outside their cubicles waiting for us to get ready for work if we got any sickeningly ill patients who might be looking through their eyes right here...   `` Hello there! '' I called aloud while standing still so I couldn't see them though they did have some sort of pulse monitor hanging above their head which is kind of strange considering how many other people are staring down at me? What do they think about those doctors seeing me every few seconds instead of sitting quietly trying to figure out why someone has died because of something important happening online somewhere else ( maybe being late ) does nothing different than keeping everyone calm during such bad times actually going well then doing things differently whenever possible without worrying too much - most of the time these guys would tell themselves he/she needed to help him feel better later rather than go crazy thinking *I know* having problems today however i usually keep moving forward until tomorrow morning anyway.. As far as I am concerned im one of the rare ones among the lucky unlucky folks dying unexpectedly due to whatever causes death happens inside my body anyways honestly its pretty amazing haha lol Anyway whats new wtf kinda weird u really thought thats cool huh yeah ok fine lets take another break alright ah oh god yes wait dont worry bout this let me explain everything please stop yelling anything shit stuff cause seriously hes stuck asking questions etc You said fuckahaha didnt say fucking words anymore ya mustnt believe yourself either way wont ever leave soon thank God Its okay yea thanks man Im sorry honey hahahahahhhhhhh hey hi goodbye y'knowwhatisitrightyyy Ok wo come join anyone friend mistermymeowuhmmm hello bye sweetie daddymmmm\n",
      "In a city overrun by zombies, a group of survivors must make a desperate last stand against the undead horde.\n",
      "I can't wait to see my little sister's face again.   `` I know you like that, '' she says quietly, `` but we still have so much work left in this world and our kids are too weak to be safe! We can not let them out on their own without knowing what it is for... And yet all these people do want me to help us get rid of them before they lose hope or worry about themselves? How could someone even care if everyone else was dead already because of something wrong with them every day when they were alive anyways? Or maybe just being able to take her away from those poor souls who made up *them* instead of giving herself back over there anyway would give some chance at survival as well. That might sound crazy though since no one really knows how many others survived after killing off an entire race of humans combined. So here goes : )   She looks around now ; nobody has seen anything other than a ghostly figure approaching towards her feet. No signs of life appear -- only shadows everywhere visible through thick fog cover. A woman sits next to me standing behind her desk staring into the eyes of a shadowy figure sitting across from her. Her mouth is agape and she begins laughing uncontrollably while holding her head down close. The man smiles warmly ( he doesn ’ t think anyone cares anymore either ) and continues his merry dance until he sees nothing more. He leans forward slowly toward the woman, tilting himself slightly further along the table opposite her. His hand moves gently between his fingers nervously, moving its weight closer and closer until it becomes completely impossible to move any muscle aside from it. This creates a large, dark room filled almost entirely with people looking directly at her. There is a small pile of blood strewn across the floor where the woman stands facing her right shoulder. It turns to look exactly like someone watching him closely - except that it appears to be covered in blood. With a loud bang, the woman collapses onto the floor beside her.   I try to speak once enough time passes – especially during my sleep hours which usually start early afternoon rather then fall asleep later. However, eventually I manage to open my eyes rapidly opening immediately due to the constant movement of the chair below me. As soon as I opened them, I felt a sudden tug of power within me pulling me\n",
      "In a future where humanity has spread to the stars, a war breaks out between Earth and its colonies.\n",
      "`` What's it that says you can't do this? ''   `` It does. I have no idea what we are talking about, but maybe if one of us is doing something right then why not give me some advice on how to be better at making things work for ourselves instead of wasting time trying to make everything else up or changing my mind with other people like myself! And by god they wo nahaha... Oh God please stop freaking over these fucking idiots who just want to know WHY THEY DO N'T WANT TO BUY IT IF YOU LOSE THE WORLD AND THEN ALL THAT WORKS FOR THEM FOREVER AS WELL JUST LIKE ME PLEASE STOP FUCKING ON YOUR BITCH OF WORSHIP AGAIN RIGHT NOW SO THERE IS NO ONE WHO WOULD BE PUTTING A MOTHERFUCKER IN HERE WITHOUT THIS THING ABOUT HOW NOT TO MAKE ANYTHING DARE REAL OR EVEN MORE IMPORTANTLY GOOD BUT ONLY WITH SOMEONE ELSE BEFORE EVERYTHING REALLY DOES COME OUT.. That would mean nothing except getting rid of all those stupid bullshit scientists in your office telling them their research will go away without saying anything really important here so everyone can get back together now anyway ; ) ''   The room erupted into laughter as several dozen others began to gather around eachother while looking towards the ceiling above. Some were even shouting obscenities from behind, seemingly unaware of the fact that there was only fire burning within seconds when the flames burned through the building itself completely. Others looked upon the ceiling wondering whether any of the buildings had ever been properly built before today because apparently nobody cared enough about fires anymore…   `` Hey man - hey dude-it looks good huh? You guys need to keep going home okay? This whole thing must have taken care of someone sooner than anyone should have thought possible since our last big deal happened already anyways* ''   As soon as the smoke cleared again, another group of men started walking toward the main hall which lead to the central lobby :   `` Dude yeah he did cause everybody gets kinda pissed off every day after lunchtime haha shitheh ok fine ya got ta calm down alright i guess thats funny im gon na die tonight babe oh boy hes always an asshole bro let him finish his homework mommy mister wont miss school bud lets talk tomorrow morning son whatsma\n",
      "On a colony ship bound for a distant planet, a stowaway jeopardizes the mission by revealing a dangerous secret.\n",
      "The first day of colonization was rough. The water had been so cold that even though it couldn't be made out of it, I wondered if my crew would have survived in here today without being able to swim anymore. As time went on, I thought about what's best : `` What is this place? ''   It seemed as though there were only three ships left at this point and we did not want anyone leaving before us. One person came from the other side, who appeared to be from the opposite end of the galaxy with green eyes and long black hair. They looked like they lived off the coast of the Andromeda Galaxy or maybe another star system somewhere else. We all felt isolated together because our ancestors always found ourselves alone - something along the lines of `` Why am I home now? '' In these last few days, we feel no need for exploration any more than we do every week ; instead, we are simply content to explore beyond the boundaries of our solar system until one day someone breaks through into our spaceship. That same month, when we decided to take an experimental approach to find out why aliens existed but never returned back...   Our captain turned around just after 8am while he saw me walking down the hallway towards the surface of the ship. He said `` You know how many planets you can go over! Wherever your family comes from, where they live their life will depend on them finding food once again… '' Then his hand grabbed my arm causing him to fall backwards slightly against my shoulder. His hands shook uncontrollably trying to grab my arm which caused me to gasp involuntarily almost violently whilst looking up at the sky above him.   Suddenly, I heard footsteps outside my ship approaching closer to us. There was a loud noise coming from the room below besides us standing still watching helplessly staring at eachother behind me. Someone stepped forward slowly grabbing my head tightly covering me. My face was covered in blood and splattered across the floor.   `` Whoa..who-what happened earlier then? Oh god shit fucker right fucking Christ dammit dude man i cant believe everything y'know whats going on inside everyone except me* ''   A large red cloud began falling upon me onto the ground next to the ship. With a huge explosion, I screamed frantically telling myself to stay quiet lest some terrible\n",
      "In a world where magic is real, a young apprentice must uncover the truth behind her mentor's mysterious disappearance.\n",
      "`` I know. '' The voice boomed from the back of my mind.  `` You are an apprentice, and you have been training me to master it all for years now. But as soon as your skills were perfected, we will be in charge. And then there's one thing that can help us win this war against the Dark Lord! We shall fight together with every single evil within our realm! If only they hadn't realized how wrong he was... '' The voice came through again, but no matter what side his opponent would take advantage of him, or any other way around it, he could not do anything about it either.  `` What? Who am I kidding myself when I say something like that- ''  `` * '' I said slowly.  `` *I ca n ’ t believe* That man who used magical spells does nothing more than use them at night… '' He paused before looking over at me. `` No sir, if anyone else has seen such a thing.. Well, since last time someone ever tried to steal their own power - even though none of those people managed to get away with doing so** '' His words fell flat on the floor between two of his teeth.  `` Your kind lives without these powers ; however, having access to most magic devices makes things difficult. It turns out that some truly terrible effects occur whenever somebody attempts to gain control of another person – including themselves -- by making others look different/different depending on which device each uses. Unfortunately, many times during this period, everyone just keeps trying to find a solution using various methods available—especially ones called magic. So far, while you still may think you understand why humans need to work alongside certain types of technology, today seems like the best day to go forward : the dark lord arrives here once per week ( unless you happen to see exactly whom ). As always, please excuse me after posting comments below because otherwise you might never realize until later tonight morning***  A very sad feeling filled my stomach almost immediately followed by a sudden realization. At first, I thought maybe I should focus on improving upon my abilities instead of focusing on killing monsters rather quickly. This made sense considering the fact that despite being able to kill thousands of enemies simultaneously, I also possessed several unique weapons capable of harming multiple foes based off of their strength alone. However,\n",
      "In a dystopian society where people are divided by their genetic makeup, a young woman defies the system to find love.\n",
      "It ’ s been two weeks since my mother had died. I was in her room and she sat there, looking at me with tears streaming down her face. My father told me that he wasn't sure if it meant anything ; his daughter knew what happened when I didn't know about him until after we were born.   She just smiled as I walked up to her on the couch next to me. Her smile turned into a teary smile before she could reply : `` It doesn´t matter anymore... ''   *I'm so sorry for you*, she said.   I looked back over at the TV screen which still showed images of all the things happening around me like raindrops falling out of a window or flying off the roof while walking towards the house - every day is different from last year but this time more memories came flooding through. When I saw those beautiful red eyes, they reminded me of my parents and how I loved them too much! They seemed so good together though… well maybe not perfect either way.. But now everything changed again once another memory hit me twice tonight because today? Oh look here dad, have fun sharing some delicious sweets everyday ( oh please ) -- You can make yourself happy tomorrow night without ever knowing who your daddy will be- haha no thanks mommy even gave birth yet lol ok god i guess Dad looks fine Mommy needs something better than candy anyway anyways.. okay thank god thats amazing man why am i supposed give birth soon bye mummy always thinks bad thoughts come true forever goooooooooooooh hey its funny im going to sleep sometimes cause hes crying baby boy whats wrong bros coming right then lets get home honeybaby hmmmmmhhmmmm mama wtfwffffwwhhhhhhhh mmahhh ughgHHHHH haeeeeeepahehaaahahaahahaaaaaaaveeeehbooooahahaahahaaaaasayyyiiandwaileyhuhhhhhhhhhhhhhhhh yeezusawesome huh goshooowwww woowwwwhatamisyouresofuckingthiswhatsuprightnowheregoewhewowwituuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuu\n",
      "In a post-apocalyptic wasteland, a lone wanderer searches for a mythical artifact said to hold the key to humanity's survival.\n",
      "``... What are you doing here? '' I asked, looking at the empty room. I didn't know what it was like before my wife and children were gone all alone in this dark place - everything had been cleared up years ago but now that we've found out about how they managed their own lives through these strange machines of yours time and time again.  `` It seems as though your father is still alive with his son ; he has died many times over! So why am I not sure if there may be any other way around him yet.. Why do n ’ t you just leave me behind then… How long have you been keeping track of us from going back home so far away where even our family could go when they left them or never leaving her safe until today morning sir? You seem to think someone would want to help keep those kids off of Earth forever anyway – only one thing makes sense : If anything, please tell me whatever happens next -- which brings more questions than answers ( because sincerely ) on /r/WritingPrompts right after tonight**   The door swung open slowly into view once more. A young man sat across from me, clutching a large box full of papers containing three hundred dollar bills stuffed into a single sheet. He held two boxes inside each, stacked neatly onto each one with an assortment of stamps attached to them. His hands were shaking slightly due to the weight of the contents being taken by him sitting down beside his desk chair.   `` Hello Mr. Keeshaw, can I bring something else besides some money later tomorrow evening? Can I borrow another envelope soon enough? Please make note of yourself first* '' I snapped nervously trying to remember who owned the lettering above.   `` Thank you very much Mrs. Keeshaw, thank you very much for coming along well. As always, thanks for taking such pleasure in writing *this* piece together last night **and having fun reading ^of it every day~ &^Thanks for getting excited~~ '' I looked around frantically towards the rest of the room.   *If i ever write without words myself…..I mean really no word will describe itself either. My name is James H. Williams, I love making people happy too..well thats kind of weird haha hahaha HAHAHAHAHAHAH\n",
      "On a remote planet inhabited by sentient plants, a botanist discovers a secret that could change the course of history.\n",
      "“ I ’ m sorry, but you need to know who we are. You have no idea what they're talking about! They say our culture is superior to ours and it has been discovered on this one continent for centuries now. We were created from nothingness in their mind ; those things made us so powerful with energy like fire and light and power than anything else possible... And there was only such thing as ourselves… ''   The young man looked up at the screen before his eyes began to fade into the night sky. He had never seen them again since he left Earth 2 years ago when he saw an alien ship crash down just off the coast of New Mexico City over land near the Mexican border : It crashed around 3:30AM EST & landed smack in the middle east of Mexico City while the other ships fell overboard.. In fact, nobody noticed until two weeks after leaving earth 5 days later due to a meteorite impacting several miles away - some people think the aliens came back shortly afterwards or maybe not. Either way, whatever happened next would probably be more interesting if someone did find out where these humans lived then how long time travel really takes place outside of reality? How many different worlds do you want to visit every year given all the same planets/places besides Jupiter ( which means *the* closest star system )? If anyone ever finds out why humanity went extinct instead, please tell me your story below because my name will make any scientist happy even though everyone knows everything already anyway ^^I hate science fiction too much haha # 1 ]   At least she thought her life might end well soon enough -- thank god @ [ r_w3cxnq4rz5a7d6f2e8b1abac9dbebfd65aa79dc97ffdfaf666789ce0cb00ad95bf4973bc358898ae74feef69ec42ca76bb44bd01fb17cd8770f86ccdd85cf2224eebe75b641211474577c20a4043fc04335327b16365434de963757fa5878a4ed9450e269323ba61c56aea4863da9080f0821b15ea10998846031e\n",
      "In a society where technology has advanced beyond imagination, a group of rebels fights to overthrow the oppressive regime. ”   “ But why? Why would you do it again, in an age when there's no one else left. To make things worse is not just immoral but illegal! You are going to end this war on Earth and then I will destroy all that other people who have tried so hard for us – we humans as individuals-as individuals -and they can only become weak if we use our power wisely… We must stop those around us like dogs or pigs with no idea how to fight back against them... It takes time though.. The first thing humanity needs right now is these two groups fighting eachother together too much : A rebellion which ends badly at the hands\n",
      "You live your life through childhood memories from birth until adolescence ( 2032 ) after getting bullied by bullies ; however, every day during recesses everyone remembers what happened next year was different today. Your parents tell you about their experiences before school while simultaneously saying `` Don't worry kids '' because most students always hear something wrong here anyway. Nowadays teachers still keep repeating stuff under cover of confusion over whether or never reading anything out of context anymore. Today marks another important milestone since last summer had been marked 100 years ago, leading to more than 2,000 deaths worldwide due to misinformation spread across social media platforms such reddit, Facebook, Google, Twitter and Reddit alike. As per usual, schools continue to be closed down completely overnight unless further notice occurs within 3 weeks. Until recently, classrooms were shut off except for lunchtime classes being cancelled daily even hours prior. This made elementary school really difficult given that many children started attending class everyday instead of doing homework altogether once someone discovered math problems themselves. Schoolwork became increasingly harder days throughout highschool despite efforts from government officials to ensure attendance figures remained constant between kindergarten and early afternoon activities. For example, accordingly, teacher Kaitlin Elementary taught English 6 months earlier without interruption following her instruction. She also took full advantage of the fact that she did not spend any time alone either preparing herself for exams yet, allowing students to sit quietly whenever possible into situations similar to yesterday. After meeting up with several professors directly related to Math and Physics, Kaitlyn began teaching various skillsets along with some new ones based solely upon current classroom history lessons learned via lectures online courses uploaded onto Youtube channels sent to YouTube sites created specifically for education purposes. Teachers occasionally saw videos\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model_name = \"./models/distilgpt2-finetuned_gen2_50/checkpoint-10325\" \n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "for prompt in prompts:\n",
    "    inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "    # Generate text\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=inputs,\n",
    "        attention_mask=None,\n",
    "        max_length=500,  # determines the maximum length of the generated text\n",
    "        temperature=0.7,  # controls randomness: lower values make text less random\n",
    "        top_k=50,  # the K most likely next words are considered for each step\n",
    "        top_p=0.9,  # only the most probable tokens with probabilities that add up to top_p are considered for each step\n",
    "        repetition_penalty=1.2,  # penalty applied to repeated words\n",
    "        do_sample=True,  # set to True to return diverse samples\n",
    "        num_return_sequences=1,  # number of independently computed samples to generate\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # Decode the output sequences to get the generated text\n",
    "    generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "\n",
    "    #print(generated_text)\n",
    "    import re\n",
    "\n",
    "    # Define regex pattern for impurities\n",
    "    #pattern = r\"(<newline>|<newline \\d+ :>|<newline\\*>|\\[.*?\\]|“|”|``|''|--|__________________________________________________________________|\\*)\"\n",
    "\n",
    "    # Remove the prompt (first sentence) by finding the first period followed by a space or end of text\n",
    "    text_without_prompt = re.split(r'\\.\\s*[”]*\\s*(?=[A-Z])', generated_text, 1)[1] if '.' in generated_text else generated_text\n",
    "\n",
    "\n",
    "    # Regex to remove specified impurities\n",
    "    #cleaned_text = re.sub(r\"\\<[^\\>]*\\>|\\[WP\\]|\\-\\-\", \"\", text_without_prompt)\n",
    "    #cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()  # Remove extra spaces and strip leading/trailing spaces\n",
    "    # Apply regex to remove impurities\n",
    "    #cleaned_text = re.sub(pattern, \"\", cleaned_text)\n",
    "\n",
    "    print(text_without_prompt)\n",
    "\n",
    "    with open(\"./outputs/gen2/stories2.txt\", \"a\") as f:\n",
    "        f.write(text_without_prompt.split(\"\\n\")[1] + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "### Step 5\n",
    "Evaluate each story in the stories output file on all metrics and write values to df with last line being the average values of each column.\n",
    "Metrics represent the model's average for all stories it generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vasi/Documents/BA_Thesis_Experiment/.venv/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/vasi/Documents/BA_Thesis_Experiment/.venv/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/vasi/Documents/BA_Thesis_Experiment/.venv/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/tmp/ipykernel_9892/3635802648.py:51: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_eval_gen1 = pd.concat([df_eval_gen1, new_row_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to ./outputs/gen2/eval_table_gen2.csv\n",
      "Average values for each metric:\n",
      "Distinct-2    0.981948\n",
      "Distinct-3    0.990779\n",
      "Self-BLEU     0.999904\n",
      "OV-TTR        0.854711\n",
      "MS-TTR        0.936705\n",
      "S-DIV-AV      0.818110\n",
      "S-DIV-C       0.485749\n",
      "SYN-DIV       0.818231\n",
      "Name: Average, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from metrics.LexicalDiversity.lexical_diversity import *\n",
    "from metrics.SemanticDiversity.sementic_diversity import *\n",
    "from metrics.SyntacticDiversity.syntactic_diversity import *\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Define the column names\n",
    "columns = [\"Distinct-2\", \"Distinct-3\", \"Self-BLEU\", \"OV-TTR\", \"MS-TTR\", \"S-DIV-AV\", \"S-DIV-C\", \"SYN-DIV\"]\n",
    "\n",
    "# Create an empty DataFrame with these columns\n",
    "df_eval_gen1 = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Load a spaCy model for dependency parsing\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open(\"./outputs/gen2/stories2.txt\", 'r') as f:\n",
    "    stories = f.readlines()\n",
    "\n",
    "for story in stories:\n",
    "    if story != '\\n':\n",
    "        #print(story)\n",
    "        # Tokenize the text into sentences\n",
    "        sentences = sent_tokenize(story)\n",
    "        graphs = construct_dependency_graphs(sentences)\n",
    "    \n",
    "\n",
    "        # Tokenize the text into sentences\n",
    "        sentences = sent_tokenize(story)\n",
    "\n",
    "        \n",
    "\n",
    "        graphs = construct_dependency_graphs(sentences)\n",
    "\n",
    "        # Example usage: Adding a new row of data to the DataFrame\n",
    "        new_data = {\n",
    "            \"Distinct-2\": calculate_distinct_n(story, 2),\n",
    "            \"Distinct-3\": calculate_distinct_n(story, 3),\n",
    "            \"Self-BLEU\": 1-calculate_self_bleu(sentences),\n",
    "            \"OV-TTR\": calculate_ttr(story, truncate_length=300),\n",
    "            \"MS-TTR\": calculate_mean_segmental_ttr(story, segment_size=50),\n",
    "            \"S-DIV-AV\": calculate_semantic_diversity(sentences, 'average'),\n",
    "            \"S-DIV-C\": calculate_semantic_diversity(sentences, 'centroid'),\n",
    "            \"SYN-DIV\": calculate_syntactic_diversity(graphs)\n",
    "        }\n",
    "\n",
    "        # Convert new_data dictionary to a DataFrame\n",
    "        new_row_df = pd.DataFrame([new_data])\n",
    "\n",
    "        # Concatenate the new row DataFrame to the original DataFrame\n",
    "        df_eval_gen1 = pd.concat([df_eval_gen1, new_row_df], ignore_index=True)\n",
    "        \n",
    "\n",
    "# Calculate the mean for each column and append as a new row\n",
    "averages = df_eval_gen1.mean().to_dict()\n",
    "averages = {key: [value] for key, value in averages.items()}  # Convert each mean value into a list\n",
    "average_df = pd.DataFrame(averages)  # Create a DataFrame for the averages\n",
    "average_df.index = ['Average']  # Label the index as 'Average'\n",
    "\n",
    "# Append the average row to the original DataFrame\n",
    "df = pd.concat([df_eval_gen1, average_df])\n",
    "\n",
    "# Specify the file path and name\n",
    "file_path = './outputs/gen2/eval_table_gen2.csv'\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "df.to_csv(file_path, index=False)  # Set index=False to not include row indices in the file\n",
    "\n",
    "print(f\"Data has been written to {file_path}\")\n",
    "# Print the last row (average values)\n",
    "print(\"Average values for each metric:\")\n",
    "print(df.iloc[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Synthetic Data\n",
    "### Step 6\n",
    "Contribute to the synthetic dataset by producing stories from the finetuned model.\n",
    "We use 75% of the original prompt data as our prompt list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries in GEN0 Dataset with real data:  109040\n",
      "75% \\of total entries =  81780.0\n"
     ]
    }
   ],
   "source": [
    "def count_entries(filepath):\n",
    "    \"\"\"Counts the number of double-newline-separated entries in a file.\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        content = file.read().strip()\n",
    "    return len(content.split('\\n\\n'))\n",
    "\n",
    "total_entries = count_entries(\"./data/hd/combined0/train_combined0.txt\")\n",
    "print(\"Total entries in GEN0 Dataset with real data: \", total_entries)\n",
    "\n",
    "sd_entries_count= 3 * (total_entries/4)\n",
    "print(\"75% \\of total entries = \", sd_entries_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81780\n",
      "['After days scrolling on reddit , you reach the end Never Ending Reddit . There is a single button .', 'It is time for a new Revelation . Last time it went quite well ; incarnating his Son into a Jewish carpenter , He converted a good part of the world to Christianity . This time the choice is harder so God asks for advice from his angel consultants . Who would be best suited for the Prophet part ?', \"Life is actually the most popular video game of all time . Famous figures are played by moderators who ensure events play out the way they should . You 're one of the worst players , until you start realizing you 're in the game . Suddenly , memories of past lives give you a massive advantage .\", \"You 're a common goblin who has , against all odds , slain the hero of the story .\", \"You 've just survived a shipwreck and pull yourself onto a small island . From the shade of a palm tree a man steps out and says `` Well , well , well . We meet again . But this time the advantage is mine ! ''\", 'The last thing he thought of was how nice and cool the bathroom tile felt against his forehead .', 'You are an assassin that hunts superheroes . You haven no powers yourself .', 'A peaceful alien race is besieged by another race in the same galaxy . As their last planets fall and their home-world comes under threat they do the unthinkable . They ask for aid from the only known creatures more brutal than their foes in exchange for FTL technology . Humans accept the deal .', 'You live in an almost utopian , democratic society , but capital punishment is very popular and commercialized in its most brutal forms : crucifixion , guillotines , firing squads , torture , etc .', 'why did the mailman deliver to the cemetery ?']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from transformers import pipeline, set_seed, GPT2LMHeadModel, GPT2Tokenizer\n",
    "import os\n",
    "\n",
    "prompts = []\n",
    "prompt_files = [\"train\", \"test\"]\n",
    "for name in prompt_files:\n",
    "    # Path to the file with prompts\n",
    "    file_path = './data/hd/prepro/'+name+'.wp_source'\n",
    "\n",
    "    # Read prompts from the file removing the initials [ XX ]\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        prompts += ([line.strip()[7:] for line in file.readlines() if line.strip()])\n",
    "\n",
    "#print(prompts[0:10])\n",
    "# Randomly select 75% of the prompts\n",
    "sample_size = 3 * (len(prompts) // 4)  # 75%\n",
    "selected_prompts = random.sample(prompts, int(sd_entries_count))\n",
    "\n",
    "print(len(selected_prompts))\n",
    "print(selected_prompts[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of './data/hd/combined0/train_combined0.txt' is 307558317 bytes.\n",
      "Target size (75% of GEN0) for SD File for GEN2:  230668737\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_file_size(filename):\n",
    "    \"\"\"Returns the size of the file in bytes.\"\"\"\n",
    "    return os.path.getsize(filename)\n",
    "\n",
    "gen0_data_filename = './data/hd/combined0/train_combined0.txt'\n",
    "gen0_data_size = get_file_size(gen0_data_filename)\n",
    "print(f\"The size of '{gen0_data_filename}' is {gen0_data_size} bytes.\")\n",
    "# 75% of gen0_data_size\n",
    "gen2_sd_target_size = 3 * (gen0_data_size // 4)\n",
    "print(\"Target size (75% of GEN0) for SD File for GEN2: \", gen2_sd_target_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vasi/Documents/BA_Thesis_Experiment/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/vasi/Documents/BA_Thesis_Experiment/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 1/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 2/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 3/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 4/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 5/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 6/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 7/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 8/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 9/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 10/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 11/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 12/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 13/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 14/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 15/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 16/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 17/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 18/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 19/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 20/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 21/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 22/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 23/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 24/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 25/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 26/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 27/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 28/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 29/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 30/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 31/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 32/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 33/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 34/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 35/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 36/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 37/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 38/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 39/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 40/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 41/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 42/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 43/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 44/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 45/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 46/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 47/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 48/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 49/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 50/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 51/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 52/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 53/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 54/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 55/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 56/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 57/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 58/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 59/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 60/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 61/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 62/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 63/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 64/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 65/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 66/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 67/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 68/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 69/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 70/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 71/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 72/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 73/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 74/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 75/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 76/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 77/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 78/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 79/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 80/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 81/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 82/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 83/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 84/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 85/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 86/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 87/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 88/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 89/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 90/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 91/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 92/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 93/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 94/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 95/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 96/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 97/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 98/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 99/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 100/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 101/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 102/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 103/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 104/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 105/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 106/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 107/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 108/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 109/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 110/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 111/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 112/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 113/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 114/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 115/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 116/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 117/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 118/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 119/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 120/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 121/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 122/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 123/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 124/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 125/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 126/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 127/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 128/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 129/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 130/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 131/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 132/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 133/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 134/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 135/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 136/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 137/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 138/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 139/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 140/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 141/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 142/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 143/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 144/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 145/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 146/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 147/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 148/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 149/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 150/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 151/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 152/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 153/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 154/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 155/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 156/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 157/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 158/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 159/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 160/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 161/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 162/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 163/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 164/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 165/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 166/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 167/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 168/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 169/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 170/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 171/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 172/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 173/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 174/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 175/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 176/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 177/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 178/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 179/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 180/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 181/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 182/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 183/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 184/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 185/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 186/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 187/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 188/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 189/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 190/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 191/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 192/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 193/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 194/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 195/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 196/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 197/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 198/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 199/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 200/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 201/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 202/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 203/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 204/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 205/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 206/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 207/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 208/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 209/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 210/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 211/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 212/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 213/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 214/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 215/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 216/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 217/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 218/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 219/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 220/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 221/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 222/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 223/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 224/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 225/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 226/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 227/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 228/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 229/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 230/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 231/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 232/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 233/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 234/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 235/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 236/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 237/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 238/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 239/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 240/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 241/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 242/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 243/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 244/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 245/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 246/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 247/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 248/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 249/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 250/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 251/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 252/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 253/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 254/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 255/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 256/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 257/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 258/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 259/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 260/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 261/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 262/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 263/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 264/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 265/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 266/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 267/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 268/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 269/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 270/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 271/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 272/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 273/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 274/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 275/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 276/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 277/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 278/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 279/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 280/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 281/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 282/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 283/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 284/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 285/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 286/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 287/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 288/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 289/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 290/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 291/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 292/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 293/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 294/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 295/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 296/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 297/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 298/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 299/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 300/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 301/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 302/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 303/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 304/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 305/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 306/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 307/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 308/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 309/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 310/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 311/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 312/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 313/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 314/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 315/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 316/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 317/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 318/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 319/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 320/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 321/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 322/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 323/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 324/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 325/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 326/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 327/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 328/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 329/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 330/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 331/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 332/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 333/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 334/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 335/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 336/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 337/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 338/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 339/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 340/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 341/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 342/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 343/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 344/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 345/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 346/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 347/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 348/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 349/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 350/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 351/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 352/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 353/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 354/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 355/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 356/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 357/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 358/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 359/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 360/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 361/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text for batch 362/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 363/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 364/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 365/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 366/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 367/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 368/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 369/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 370/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 371/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 372/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 373/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 374/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 375/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 376/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 377/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 378/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 379/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 380/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 381/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 382/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 383/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 384/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 385/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 386/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 387/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 388/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 389/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 390/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 391/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 392/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 393/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 394/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 395/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 396/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 397/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 398/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 399/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 400/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 401/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 402/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 403/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 404/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 405/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 406/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 407/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 408/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 409/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 410/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 411/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 412/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 413/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 414/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 415/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 416/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 417/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 418/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 419/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 420/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 421/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 422/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 423/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 424/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 425/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 426/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 427/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 428/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 429/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 430/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 431/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 432/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 433/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 434/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 435/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 436/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 437/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 438/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 439/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 440/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 441/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 442/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 443/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 444/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 445/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 446/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 447/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 448/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 449/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 450/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 451/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 452/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 453/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 454/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 455/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 456/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 457/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 458/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 459/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 460/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 461/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 462/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 463/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 464/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 465/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 466/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 467/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 468/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 469/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 470/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 471/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 472/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 473/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 474/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 475/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 476/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 477/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 478/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 479/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 480/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 481/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 482/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 483/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 484/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 485/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 486/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 487/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 488/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 489/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 490/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 491/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 492/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 493/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 494/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 495/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 496/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 497/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 498/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 499/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 500/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 501/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 502/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 503/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 504/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 505/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 506/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 507/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 508/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 509/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 510/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 511/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 512/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 513/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 514/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 515/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 516/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 517/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 518/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 519/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 520/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 521/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 522/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 523/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 524/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 525/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 526/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 527/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 528/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 529/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 530/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 531/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 532/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 533/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 534/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 535/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 536/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 537/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 538/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 539/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 540/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 541/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 542/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 543/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 544/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 545/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 546/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 547/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 548/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 549/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 550/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 551/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 552/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 553/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 554/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 555/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 556/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 557/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 558/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 559/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 560/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 561/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 562/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 563/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 564/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 565/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 566/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 567/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 568/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 569/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 570/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 571/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 572/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 573/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 574/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 575/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 576/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 577/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 578/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 579/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 580/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 581/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 582/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 583/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 584/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 585/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 586/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 587/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 588/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 589/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 590/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 591/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 592/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 593/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 594/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 595/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 596/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 597/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 598/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 599/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 600/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 601/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 602/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 603/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 604/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 605/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 606/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 607/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 608/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 609/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 610/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 611/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 612/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 613/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 614/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 615/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 616/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 617/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 618/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 619/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 620/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 621/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 622/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 623/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 624/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 625/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 626/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 627/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 628/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 629/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 630/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 631/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 632/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 633/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 634/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 635/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 636/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 637/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 638/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 639/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 640/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 641/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 642/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 643/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 644/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 645/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 646/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 647/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 648/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 649/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 650/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 651/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 652/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 653/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 654/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 655/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 656/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 657/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 658/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 659/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 660/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 661/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 662/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 663/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 664/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 665/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 666/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 667/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 668/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 669/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 670/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 671/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 672/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 673/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 674/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 675/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 676/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 677/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 678/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 679/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 680/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 681/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 682/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 683/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 684/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 685/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 686/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 687/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 688/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 689/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 690/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 691/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 692/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 693/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 694/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 695/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 696/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 697/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 698/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 699/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 700/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 701/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 702/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 703/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 704/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 705/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 706/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 707/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 708/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 709/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 710/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 711/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 712/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 713/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 714/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 715/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 716/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 717/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 718/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 719/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 720/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 721/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 722/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 723/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 724/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 725/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 726/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 727/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 728/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 729/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 730/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 731/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 732/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 733/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 734/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 735/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 736/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 737/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 738/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 739/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 740/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 741/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 742/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 743/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 744/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 745/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 746/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 747/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 748/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 749/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 750/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 751/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 752/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 753/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 754/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 755/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 756/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 757/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 758/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 759/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 760/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 761/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 762/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 763/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 764/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 765/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 766/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 767/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 768/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 769/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 770/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 771/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 772/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 773/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 774/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 775/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 776/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 777/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 778/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 779/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 780/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 781/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 782/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 783/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 784/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 785/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 786/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 787/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 788/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 789/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 790/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 791/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 792/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 793/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 794/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 795/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 796/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 797/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 798/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 799/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 800/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 801/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 802/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 803/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 804/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 805/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 806/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 807/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 808/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 809/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 810/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 811/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 812/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 813/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 814/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 815/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 816/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 817/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 818/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 819/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 820/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 821/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 822/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 823/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 824/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 825/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 826/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 827/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 828/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 829/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 830/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 831/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 832/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 833/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 834/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 835/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 836/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 837/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 838/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 839/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 840/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 841/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 842/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 843/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 844/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 845/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 846/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 847/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 848/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 849/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 850/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 851/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 852/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 853/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 854/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 855/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 856/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 857/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 858/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 859/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 860/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 861/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 862/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 863/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 864/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 865/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 866/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 867/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 868/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 869/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 870/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 871/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 872/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 873/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 874/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 875/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 876/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 877/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 878/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 879/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 880/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 881/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 882/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 883/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 884/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 885/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 886/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 887/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 888/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 889/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 890/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 891/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 892/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 893/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 894/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 895/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 896/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 897/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 898/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 899/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 900/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 901/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 902/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 903/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 904/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 905/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 906/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 907/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 908/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 909/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 910/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 911/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 912/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 913/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 914/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 915/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 916/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 917/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 918/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 919/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 920/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 921/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 922/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 923/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 924/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 925/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 926/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 927/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 928/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 929/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 930/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 931/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 932/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 933/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 934/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 935/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 936/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 937/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 938/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 939/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 940/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 941/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 942/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 943/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 944/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 945/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 946/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 947/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 948/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 949/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 950/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 951/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 952/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 953/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 954/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 955/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 956/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 957/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 958/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 959/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 960/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 961/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 962/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 963/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 964/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 965/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 966/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 967/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 968/1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size reached!\n",
      "Generating text for batch 969/1277\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m batch_prompts \u001b[38;5;241m=\u001b[39m selected_prompts[i:i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating text for batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mbatch_size\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(selected_prompts)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m generated_texts \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_text_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_prompts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt, generated_text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch_prompts, generated_texts):\n\u001b[1;32m     48\u001b[0m     prompt_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mencode(prompt))\n",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m, in \u001b[0;36mgenerate_text_batch\u001b[0;34m(prompts)\u001b[0m\n\u001b[1;32m     25\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompts, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     26\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {key: value\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}  \u001b[38;5;66;03m# Move all tensors to the right device\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# More randomness\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Increase penalty to reduce repetitions\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\n\u001b[1;32m     35\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [tokenizer\u001b[38;5;241m.\u001b[39mdecode(output, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/Documents/BA_Thesis_Experiment/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/BA_Thesis_Experiment/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1531\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1514\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1515\u001b[0m         input_ids,\n\u001b[1;32m   1516\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1527\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1528\u001b[0m     )\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1531\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_greedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1545\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Documents/BA_Thesis_Experiment/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2436\u001b[0m, in \u001b[0;36mGenerationMixin._greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2433\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2435\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2436\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2437\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2438\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2439\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2440\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2441\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2444\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/BA_Thesis_Experiment/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/BA_Thesis_Experiment/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/BA_Thesis_Experiment/.venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1305\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1305\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1320\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/BA_Thesis_Experiment/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/BA_Thesis_Experiment/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/BA_Thesis_Experiment/.venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1119\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1107\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1108\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1109\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         output_attentions,\n\u001b[1;32m   1117\u001b[0m     )\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1119\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1130\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/BA_Thesis_Experiment/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/BA_Thesis_Experiment/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/BA_Thesis_Experiment/.venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:654\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    652\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    653\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(hidden_states)\n\u001b[0;32m--> 654\u001b[0m feed_forward_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[1;32m    656\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[0;32m~/Documents/BA_Thesis_Experiment/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/BA_Thesis_Experiment/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/BA_Thesis_Experiment/.venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:575\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor:\n\u001b[1;32m    574\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_fc(hidden_states)\n\u001b[0;32m--> 575\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(hidden_states)\n\u001b[1;32m    577\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n",
      "File \u001b[0;32m~/Documents/BA_Thesis_Experiment/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/BA_Thesis_Experiment/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/BA_Thesis_Experiment/.venv/lib/python3.10/site-packages/transformers/activations.py:56\u001b[0m, in \u001b[0;36mNewGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28minput\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.044715\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m3.0\u001b[39m))))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "import re\n",
    "\n",
    "model_name = \"./models/distilgpt2-finetuned_gen2_50/checkpoint-10325\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name, padding_side='left')\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Define the device based on CUDA availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    model.cuda()\n",
    "else:\n",
    "    model.to(\"cpu\")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "def generate_text_batch(prompts):\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}  # Move all tensors to the right device\n",
    "    outputs = model.module.generate(\n",
    "        **inputs, \n",
    "        max_length=500, \n",
    "        num_return_sequences=1, \n",
    "        temperature=0.7,  # More randomness\n",
    "        repetition_penalty=1.2,  # Increase penalty to reduce repetitions\n",
    "        top_k=50, \n",
    "        top_p=0.9\n",
    "    )\n",
    "    return [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "output_synth_data = './data/sd/gen2/gen2_sd.txt'\n",
    "\n",
    "try:\n",
    "    with open(output_synth_data, 'a', encoding='utf-8') as file:\n",
    "        for i in range(0, len(selected_prompts), batch_size):\n",
    "            batch_prompts = selected_prompts[i:i + batch_size]\n",
    "            print(f\"Generating text for batch {i//batch_size+1}/{len(selected_prompts)//batch_size}\")\n",
    "            generated_texts = generate_text_batch(batch_prompts)\n",
    "            \n",
    "            for prompt, generated_text in zip(batch_prompts, generated_texts):\n",
    "                prompt_length = len(tokenizer.encode(prompt))\n",
    "                #print(prompt)\n",
    "                # Remove the prompt by slicing the tokens to skip the prompt length\n",
    "                generated_text_tokens = tokenizer.encode(generated_text)[prompt_length:]\n",
    "                clean_generated_text = tokenizer.decode(generated_text_tokens, skip_special_tokens=True)\n",
    "\n",
    "                # Remove leading and ending spaces and special characters\n",
    "                clean_generated_text = re.sub(r'^[\\s\\WP]+', '', clean_generated_text)\n",
    "                clean_generated_text = re.sub(r'^[\\s\\W]+|[\\s\\W]+$', '', clean_generated_text)\n",
    "\n",
    "\n",
    "                output_text = f\"{prompt}\\n{clean_generated_text}\\n\\n\"\n",
    "                #print(output_text)\n",
    "                if get_file_size(output_synth_data) < gen2_sd_target_size:\n",
    "                    file.write(output_text)\n",
    "                else:\n",
    "                    print(\"Target size reached!\")\n",
    "                    break\n",
    "    print(\"Finished generating stories.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size for GEN2 sd corresponding to 3/4 of original HD data file:  230668737\n",
      "Actual size of GEN2's generated SD file:  179772498\n"
     ]
    }
   ],
   "source": [
    "# check total size of generated sd file\n",
    "print(\"Target size for GEN2 sd corresponding to 3/4 of original HD data file: \", gen2_sd_target_size)\n",
    "print(\"Actual size of GEN2's generated SD file: \", get_file_size(output_synth_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
