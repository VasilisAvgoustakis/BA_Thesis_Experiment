{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self BLEU\n",
    "A high Self-BLEU score indicates that the generated texts are very similar to each other, suggesting low diversity, while a lower Self-BLEU score suggests higher diversity, therefore we report 1 - self Bleu which reverses this for a more intuitive reading. Thus the higher the reporte score the higher the diversity. \n",
    "\n",
    "The weights parameter in sentence_bleu is set to give equal importance to 1-gram, 2-gram, 3-gram, and 4-gram matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def calculate_self_bleu(texts):\n",
    "    \"\"\"\n",
    "    Calculate the Self-BLEU score for a set of texts.\n",
    "    \n",
    "    Parameters:\n",
    "    - texts (list of str): The set of generated texts to be evaluated.\n",
    "    \n",
    "    Returns:\n",
    "    - float: The average Self-BLEU score of the texts.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for i, candidate in enumerate(texts):\n",
    "        # Consider all other texts as references for the current candidate text\n",
    "        references = [texts[j].split() for j in range(len(texts)) if i != j]\n",
    "        candidate_tokens = candidate.split()\n",
    "        # Calculate the BLEU score for this text against all others\n",
    "        score = sentence_bleu(references, candidate_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "        scores.append(score)\n",
    "    \n",
    "    # Calculate the average score across all texts\n",
    "    average_score = sum(scores) / len(scores) if scores else 0\n",
    "    return average_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-BLEU score 1: 0.9426469781691708\n",
      "Self-BLEU score 2: 0.9874474642014096\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example text \n",
    "#PROMPT: Generate a Story about love.\n",
    "#gpt3.5\n",
    "file_path1 = \"/Users/Vas/Documents/Coding_Projects/BA_Experiment_Tests/Metrics/sample1.txt\"\n",
    "#gpt4\n",
    "file_path2 = \"/Users/Vas/Documents/Coding_Projects/BA_Experiment_Tests/Metrics/sample2.txt\"\n",
    "\n",
    "with open(file_path1, 'r', encoding=\"utf-8\") as file:\n",
    "    text1 = file.read()\n",
    "with open(file_path2, 'r', encoding=\"utf-8\") as file:\n",
    "    text2 = file.read()\n",
    "\n",
    "generated_text1 = text1.split('.')\n",
    "generated_text2 = text2.split('.')\n",
    "self_bleu_score1 = calculate_self_bleu(generated_text1)\n",
    "self_bleu_score2 = calculate_self_bleu(generated_text2)\n",
    "print(f'Self-BLEU score 1: {1-self_bleu_score1}')\n",
    "print(f'Self-BLEU score 2: {1-self_bleu_score2}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
